name: Daily Data Update

on:
  schedule:
    # 19:00 UTC = 03:00 Asia/Shanghai (next day). GitHub cron is always UTC.
    - cron: "0 19 * * *"
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: daily-data-update
  cancel-in-progress: false

jobs:
  update:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install crawler deps
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r crawler/requirements.txt

      - name: Run discovery + news
        env:
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
          PERPLEXITY_MODEL: ${{ secrets.PERPLEXITY_MODEL || 'sonar' }}
          ZHIPU_API_KEY: ${{ secrets.ZHIPU_API_KEY }}
          GLM_MODEL: ${{ secrets.GLM_MODEL || 'glm-4.7' }}
          GLM_SEARCH_ENGINE: ${{ secrets.GLM_SEARCH_ENGINE || 'search_pro' }}
          USE_GLM_FOR_CN: ${{ secrets.ZHIPU_API_KEY && 'true' || 'false' }}
        working-directory: crawler
        run: |
          set -euo pipefail
          export CONTENT_YEAR="$(date -u +%Y)"

          python tools/auto_discover.py --region all || true
          # Sanity prune weekly dark horses (avoid headline-like items).
          python tools/prune_dark_horses.py || true
          python tools/auto_publish.py || true

          # Quality / enrichment pipeline (best-effort, don't fail the whole run)
          python tools/backfill_source_urls.py || true
          python tools/resolve_websites.py --input data/products_featured.json --aggressive || true
          python tools/validate_websites.py || true
          python tools/cleanup_unknowns_and_duplicates.py || true
          python tools/fix_logos.py --input data/products_featured.json || true

          python main.py --news-only || true
          # Merge China-native RSS news into blogs_news.json so CN + US/global coexist.
          python tools/cn_news_only.py || true
          python tools/rss_to_products.py --input data/blogs_news.json --sources youtube,x,xhs,wechat --enrich-featured || true

      - name: Verify JSON data sources (LLM-assisted)
        continue-on-error: true
        env:
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
          PERPLEXITY_MODEL: ${{ secrets.PERPLEXITY_MODEL || 'sonar' }}
          ZHIPU_API_KEY: ${{ secrets.ZHIPU_API_KEY }}
          GLM_MODEL: ${{ secrets.GLM_MODEL || 'glm-4.7' }}
          GLM_SEARCH_ENGINE: ${{ secrets.GLM_SEARCH_ENGINE || 'search_pro' }}
          USE_GLM_FOR_CN: ${{ secrets.ZHIPU_API_KEY && 'true' || 'false' }}
        run: |
          set -euo pipefail
          python crawler/tools/verify_data_sources.py \
            --mode llm \
            --check-network logos \
            --skip-backend-snapshot-check \
            --report-json /tmp/data_report.json \
            --report-md /tmp/data_report.md

      - name: Publish verification summary
        if: always()
        run: |
          if [ -f /tmp/data_report.md ]; then
            cat /tmp/data_report.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "No verification report markdown found." >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload verification report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: weeklyai-data-report-${{ github.run_number }}
          path: |
            /tmp/data_report.json
            /tmp/data_report.md

      - name: Sync snapshot for backend deployment
        run: |
          set -euo pipefail
          mkdir -p backend/data/dark_horses

          cp -f crawler/data/products_featured.json backend/data/products_featured.json
          cp -f crawler/data/blogs_news.json backend/data/blogs_news.json
          cp -f crawler/data/industry_leaders.json backend/data/industry_leaders.json
          cp -f crawler/data/last_updated.json backend/data/last_updated.json

          # Keep curated weekly files in sync.
          rsync -a crawler/data/dark_horses/ backend/data/dark_horses/

      - name: Commit and push (if changed)
        run: |
          set -euo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add \
            crawler/data/products_featured.json \
            crawler/data/blogs_news.json \
            crawler/data/last_updated.json \
            crawler/data/dark_horses \
            crawler/data/rising_stars \
            backend/data/products_featured.json \
            backend/data/blogs_news.json \
            backend/data/industry_leaders.json \
            backend/data/last_updated.json \
            backend/data/dark_horses

          if git diff --cached --quiet; then
            echo "No data changes to commit."
            exit 0
          fi

          git commit -m "chore(data): daily update"

          # Some best-effort steps may touch additional tracked files (e.g. candidates cache).
          # Keep the working tree clean so `git pull --rebase` won't fail.
          if [ -n "$(git status --porcelain)" ]; then
            echo "Working tree dirty after commit; stashing leftovers before rebase/push."
            git stash push -u -m "ci-autostash-before-push" || true
          fi

          pushed="false"
          for attempt in 1 2 3; do
            echo "Push attempt ${attempt}/3"
            if git pull --rebase origin main && git push; then
              pushed="true"
              break
            fi
            git rebase --abort || true
            sleep $((attempt * 5))
          done

          if [ "$pushed" != "true" ]; then
            echo "‚ùå Push failed after retries (non-fast-forward or rebase conflict)." | tee -a "$GITHUB_STEP_SUMMARY"
            exit 1
          fi
