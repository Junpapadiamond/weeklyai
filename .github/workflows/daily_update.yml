name: Daily Data Update

on:
  schedule:
    # 08:00 UTC ~= 03:00 US/Eastern (winter). GitHub cron is always UTC.
    - cron: "0 8 * * *"
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: daily-data-update
  cancel-in-progress: false

jobs:
  update:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install crawler deps
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r crawler/requirements.txt

      - name: Run discovery + news
        env:
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
          PERPLEXITY_MODEL: ${{ secrets.PERPLEXITY_MODEL }}
          ZHIPU_API_KEY: ${{ secrets.ZHIPU_API_KEY }}
          GLM_MODEL: ${{ secrets.GLM_MODEL }}
          GLM_SEARCH_ENGINE: ${{ secrets.GLM_SEARCH_ENGINE }}
          USE_GLM_FOR_CN: ${{ secrets.ZHIPU_API_KEY && 'true' || 'false' }}
        run: |
          set -euo pipefail
          export CONTENT_YEAR="$(date -u +%Y)"

          python crawler/tools/auto_discover.py --region all
          python crawler/tools/auto_publish.py

          # Quality / enrichment pipeline (best-effort, don't fail the whole run)
          python crawler/tools/backfill_source_urls.py || true
          python crawler/tools/resolve_websites.py --input crawler/data/products_featured.json --aggressive || true
          python crawler/tools/validate_websites.py || true
          python crawler/tools/cleanup_unknowns_and_duplicates.py || true
          python crawler/tools/fix_logos.py --input crawler/data/products_featured.json || true

          python crawler/main.py --news-only
          python crawler/tools/rss_to_products.py --input crawler/data/blogs_news.json --sources youtube,x,xhs,wechat --enrich-featured || true

      - name: Sync snapshot for backend deployment
        run: |
          set -euo pipefail
          mkdir -p backend/data/dark_horses

          cp -f crawler/data/products_featured.json backend/data/products_featured.json
          cp -f crawler/data/blogs_news.json backend/data/blogs_news.json
          cp -f crawler/data/industry_leaders.json backend/data/industry_leaders.json
          cp -f crawler/data/last_updated.json backend/data/last_updated.json

          # Keep curated weekly files in sync.
          rsync -a crawler/data/dark_horses/ backend/data/dark_horses/

      - name: Commit and push (if changed)
        run: |
          set -euo pipefail
          if git status --porcelain | grep -q .; then
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

            git add \
              crawler/data/products_featured.json \
              crawler/data/blogs_news.json \
              crawler/data/last_updated.json \
              crawler/data/dark_horses \
              crawler/data/rising_stars \
              backend/data/products_featured.json \
              backend/data/blogs_news.json \
              backend/data/industry_leaders.json \
              backend/data/last_updated.json \
              backend/data/dark_horses

            git commit -m "chore(data): daily update"
            git push
          else
            echo "No data changes to commit."
          fi

