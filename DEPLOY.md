# WeeklyAI éƒ¨ç½²æŒ‡å—

## ğŸ³ Docker éƒ¨ç½² (æ¨è)

### å‰ææ¡ä»¶

- Docker 20.10+
- Docker Compose 2.0+
- Git

### å¿«é€Ÿå¼€å§‹

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/your-username/WeeklyAI.git
cd WeeklyAI

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥ API Keys

# 3. åˆå§‹åŒ–æ•°æ® (é¦–æ¬¡éƒ¨ç½²)
mkdir -p crawler/data
cp -r crawler/data/* /var/lib/weeklyai/data/  # æˆ–ä½¿ç”¨ Docker volume

# 4. å¯åŠ¨æœåŠ¡
docker-compose up -d

# 5. æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

### æœåŠ¡ç«¯å£

| æœåŠ¡ | ç«¯å£ | è¯´æ˜ |
|------|------|------|
| Frontend | 3000 | å‰ç«¯é¡µé¢ |
| Backend | 5000 | API æœåŠ¡ |
| Crawler | - | å®šæ—¶ä»»åŠ¡ (æ— ç«¯å£) |

### å¸¸ç”¨å‘½ä»¤

```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f frontend
docker-compose logs -f backend
docker-compose logs -f crawler

# æ‰‹åŠ¨è¿è¡Œçˆ¬è™«
docker-compose run --rm crawler run --region all --type mixed

# åªè¿è¡Œä¸€æ¬¡çˆ¬è™« (ä¸å¯åŠ¨ cron)
docker-compose run --rm crawler once --region us

# é‡å¯æœåŠ¡
docker-compose restart

# æ›´æ–°é•œåƒå¹¶é‡å¯
docker-compose pull
docker-compose up -d

# åœæ­¢å¹¶æ¸…ç†
docker-compose down
docker-compose down -v  # åŒæ—¶åˆ é™¤æ•°æ®å·
```

---

## ğŸš€ GitHub Actions CI/CD

### é…ç½® Secrets

åœ¨ GitHub ä»“åº“çš„ Settings â†’ Secrets and variables â†’ Actions ä¸­æ·»åŠ :

| Secret | è¯´æ˜ |
|--------|------|
| `PERPLEXITY_API_KEY` | Perplexity API Key |
| `SERVER_HOST` | æœåŠ¡å™¨åœ°å€ (å¯é€‰) |
| `SERVER_USER` | SSH ç”¨æˆ·å (å¯é€‰) |
| `SERVER_SSH_KEY` | SSH ç§é’¥ (å¯é€‰) |

### å·¥ä½œæµç¨‹

1. **Push åˆ° main**: è‡ªåŠ¨æ„å»ºå¹¶æ¨é€é•œåƒåˆ° GitHub Container Registry
2. **å®šæ—¶ä»»åŠ¡**: æ¯å¤© UTC 19:00 (åŒ—äº¬æ—¶é—´ 03:00) è‡ªåŠ¨è¿è¡Œçˆ¬è™«
3. **æ‰‹åŠ¨è§¦å‘**: å¯åœ¨ Actions é¡µé¢æ‰‹åŠ¨è¿è¡Œçˆ¬è™«æˆ–éƒ¨ç½²

### æ‰‹åŠ¨è§¦å‘çˆ¬è™«

1. è¿›å…¥ GitHub ä»“åº“ â†’ Actions
2. é€‰æ‹© "CI/CD Pipeline"
3. ç‚¹å‡» "Run workflow"
4. å‹¾é€‰ "Run crawler to update data"
5. ç‚¹å‡» "Run workflow"

---

## â˜ï¸ äº‘å¹³å°éƒ¨ç½²

### Railway

```bash
# å®‰è£… Railway CLI
npm install -g @railway/cli

# ç™»å½•å¹¶éƒ¨ç½²
railway login
railway init
railway up
```

### Render

1. è¿æ¥ GitHub ä»“åº“
2. åˆ›å»º 3 ä¸ª Web Service (frontend, backend, crawler)
3. é…ç½®ç¯å¢ƒå˜é‡
4. è‡ªåŠ¨éƒ¨ç½²

### Fly.io

```bash
# å®‰è£… flyctl
curl -L https://fly.io/install.sh | sh

# éƒ¨ç½²
fly launch
fly deploy
```

---

## ğŸ”§ æœåŠ¡å™¨éƒ¨ç½² (VPS)

### ä½¿ç”¨ Docker Compose

```bash
# SSH åˆ°æœåŠ¡å™¨
ssh user@your-server.com

# å®‰è£… Docker (Ubuntu/Debian)
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# å…‹éš†å¹¶éƒ¨ç½²
git clone https://github.com/your-username/WeeklyAI.git /opt/weeklyai
cd /opt/weeklyai
cp env.example .env
nano .env  # ç¼–è¾‘ç¯å¢ƒå˜é‡

# å¯åŠ¨
docker-compose up -d
```

### Nginx åå‘ä»£ç†

```nginx
server {
    listen 80;
    server_name weeklyai.example.com;

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }

    location /api/ {
        proxy_pass http://localhost:5000/api/;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### SSL è¯ä¹¦ (Let's Encrypt)

```bash
# å®‰è£… certbot
sudo apt install certbot python3-certbot-nginx

# è·å–è¯ä¹¦
sudo certbot --nginx -d weeklyai.example.com
```

---

## ğŸ“Š æ•°æ®ç®¡ç†

### æ•°æ®å¤‡ä»½

```bash
# å¤‡ä»½æ•°æ®å·
docker run --rm -v weeklyai_weeklyai-data:/data -v $(pwd):/backup alpine tar czf /backup/data-backup.tar.gz -C /data .

# æ¢å¤æ•°æ®
docker run --rm -v weeklyai_weeklyai-data:/data -v $(pwd):/backup alpine tar xzf /backup/data-backup.tar.gz -C /data
```

### æ•°æ®æ›´æ–°æµç¨‹

1. çˆ¬è™«è¿è¡Œ â†’ æ•°æ®å†™å…¥ `/data/products_featured.json`
2. åç«¯ API è¯»å– â†’ è¿”å›æœ€æ–°æ•°æ®
3. å‰ç«¯è¯·æ±‚ API â†’ å±•ç¤ºæœ€æ–°äº§å“

æ— éœ€é‡å¯æœåŠ¡ï¼Œæ•°æ®è‡ªåŠ¨æ›´æ–°ï¼

---

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: å‰ç«¯æ˜¾ç¤º"åç«¯æ— æ³•è®¿é—®"**
```bash
# æ£€æŸ¥åç«¯æ˜¯å¦è¿è¡Œ
docker-compose ps backend
docker-compose logs backend
```

**Q: çˆ¬è™«æ²¡æœ‰æ›´æ–°æ•°æ®**
```bash
# æŸ¥çœ‹çˆ¬è™«æ—¥å¿—
docker-compose logs crawler
cat crawler/logs/cron.log
```

**Q: API Key æ— æ•ˆ**
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
docker-compose exec crawler env | grep API_KEY
```

### å¥åº·æ£€æŸ¥

```bash
# æ£€æŸ¥æ‰€æœ‰æœåŠ¡å¥åº·çŠ¶æ€
docker-compose ps

# æµ‹è¯• API
curl http://localhost:5000/api/v1/products/weekly-top?limit=1

# æµ‹è¯•å‰ç«¯
curl http://localhost:3000
```

---

## ğŸ“ˆ ç›‘æ§ (å¯é€‰)

### æ·»åŠ  Prometheus + Grafana

```yaml
# åœ¨ docker-compose.yml ä¸­æ·»åŠ 
services:
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
  
  grafana:
    image: grafana/grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

---

*æœ€åæ›´æ–°: 2026-01-20*
