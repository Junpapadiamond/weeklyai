# WeeklyAI Docker Compose
# 使用方法:
#   启动所有服务: docker-compose up -d
#   查看日志: docker-compose logs -f
#   手动运行爬虫: docker-compose run --rm crawler run --region all
#   停止服务: docker-compose down

version: '3.8'

services:
  # 前端服务 (Express + EJS)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: weeklyai-frontend
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - API_BASE_URL=http://backend:5000/api/v1
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - weeklyai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/"]
      interval: 30s
      timeout: 3s
      retries: 3

  # 后端服务 (Flask API)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: weeklyai-backend
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - DATA_PATH=/data
    volumes:
      - weeklyai-data:/data
    networks:
      - weeklyai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/v1/products/weekly-top?limit=1"]
      interval: 30s
      timeout: 3s
      retries: 3

  # 爬虫服务 (定时任务)
  crawler:
    build:
      context: ./crawler
      dockerfile: Dockerfile
    container_name: weeklyai-crawler
    restart: unless-stopped
    environment:
      - PERPLEXITY_API_KEY=${PERPLEXITY_API_KEY}
      - ZHIPU_API_KEY=${ZHIPU_API_KEY}
      - DATA_PATH=/data
      - TZ=Asia/Shanghai
    volumes:
      - weeklyai-data:/data
      - ./crawler/logs:/app/logs
    networks:
      - weeklyai-network

# 共享数据卷
volumes:
  weeklyai-data:
    driver: local

# 网络
networks:
  weeklyai-network:
    driver: bridge
