#!/usr/bin/env python3
"""
æ‰¹é‡ä¿®å¤å’Œè¡¥å……äº§å“ Logo

ç­–ç•¥ï¼š
1. Clearbit Logo API (æœ€ä½³è´¨é‡)
2. Google Favicon API (å¤‡é€‰)
3. å®˜ç½‘ favicon æå–
"""

import json
import os
import re
import sys
import time
import requests
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®
TIMEOUT = 5
MAX_WORKERS = 10

# Logo æ¥æºä¼˜å…ˆçº§
LOGO_SOURCES = {
    "clearbit": "https://logo.clearbit.com/{domain}",
    "google_favicon": "https://www.google.com/s2/favicons?domain={domain}&sz=128",
    "duckduckgo": "https://icons.duckduckgo.com/ip3/{domain}.ico",
}


def extract_domain(url: str) -> str:
    """ä» URL æå–åŸŸå"""
    if not url:
        return ""
    
    try:
        if not url.startswith(("http://", "https://")):
            url = f"https://{url}"
        
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        domain = re.sub(r'^www\.', '', domain)
        return domain
    except Exception:
        return ""


def check_url_exists(url: str, timeout: int = TIMEOUT) -> bool:
    """æ£€æŸ¥ URL æ˜¯å¦å¯è®¿é—®"""
    try:
        response = requests.head(
            url,
            timeout=timeout,
            allow_redirects=True,
            headers={"User-Agent": "Mozilla/5.0"}
        )
        return response.status_code == 200
    except Exception:
        return False


def get_logo_url(domain: str) -> tuple:
    """
    è·å–äº§å“ logo URL
    
    è¿”å›: (logo_url, source)
    """
    if not domain:
        return None, None
    
    # 1. å°è¯• Clearbit (æœ€ä½³è´¨é‡)
    clearbit_url = f"https://logo.clearbit.com/{domain}"
    if check_url_exists(clearbit_url):
        return clearbit_url, "clearbit"
    
    # 2. å°è¯• Google Favicon
    google_url = f"https://www.google.com/s2/favicons?domain={domain}&sz=128"
    # Google favicon é€šå¸¸éƒ½å­˜åœ¨ï¼Œä½†å¯èƒ½æ˜¯é»˜è®¤å›¾æ ‡
    # æˆ‘ä»¬å…ˆå°è¯•ï¼Œåé¢å¯ä»¥äººå·¥ç­›é€‰
    if check_url_exists(google_url):
        return google_url, "google"
    
    # 3. å°è¯• DuckDuckGo
    ddg_url = f"https://icons.duckduckgo.com/ip3/{domain}.ico"
    if check_url_exists(ddg_url):
        return ddg_url, "duckduckgo"
    
    return None, None


def process_product(product: dict) -> dict:
    """å¤„ç†å•ä¸ªäº§å“ï¼Œå°è¯•è·å– logo"""
    name = product.get('name', 'Unknown')
    website = product.get('website', '')
    current_logo = product.get('logo', '')
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤
    needs_fix = False
    
    if not current_logo:
        needs_fix = True
    elif not current_logo.startswith('http'):
        needs_fix = True
    elif 'google.com/s2/favicons' in current_logo and 'sz=128' not in current_logo:
        # å‡çº§ä½åˆ†è¾¨ç‡ favicon
        needs_fix = True
    
    if not needs_fix:
        return product
    
    # æå–åŸŸå
    domain = extract_domain(website)
    if not domain:
        print(f"  âš ï¸  {name}: æ— æ³•æå–åŸŸå")
        return product
    
    # è·å– logo
    logo_url, source = get_logo_url(domain)
    
    if logo_url:
        product['logo'] = logo_url
        product['logo_source'] = source
        print(f"  âœ… {name}: {source} ({domain})")
    else:
        print(f"  âŒ {name}: æ— æ³•è·å– logo ({domain})")
    
    return product


def fix_logos(input_path: str, output_path: str = None, dry_run: bool = False):
    """
    æ‰¹é‡ä¿®å¤äº§å“ logo
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤è¦†ç›–è¾“å…¥)
        dry_run: é¢„è§ˆæ¨¡å¼
    """
    if not output_path:
        output_path = input_path
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        products = json.load(f)
    
    print(f"   å…± {len(products)} ä¸ªäº§å“")
    
    # ç»Ÿè®¡
    stats = {
        "total": len(products),
        "no_logo": 0,
        "invalid_logo": 0,
        "fixed": 0,
        "failed": 0,
    }
    
    # æ‰¾å‡ºéœ€è¦ä¿®å¤çš„äº§å“
    to_fix = []
    for p in products:
        logo = p.get('logo', '')
        if not logo:
            stats["no_logo"] += 1
            to_fix.append(p)
        elif not logo.startswith('http'):
            stats["invalid_logo"] += 1
            to_fix.append(p)
    
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"   æ—  logo: {stats['no_logo']}")
    print(f"   æ— æ•ˆ logo: {stats['invalid_logo']}")
    print(f"   éœ€è¦ä¿®å¤: {len(to_fix)}")
    
    if dry_run:
        print("\nğŸ” é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¼šä¿®æ”¹æ–‡ä»¶")
        print("\néœ€è¦ä¿®å¤çš„äº§å“:")
        for p in to_fix[:20]:
            print(f"  - {p.get('name')}: {p.get('website', 'no url')}")
        if len(to_fix) > 20:
            print(f"  ... è¿˜æœ‰ {len(to_fix) - 20} ä¸ª")
        return
    
    print(f"\nğŸ”§ å¼€å§‹ä¿®å¤ {len(to_fix)} ä¸ªäº§å“çš„ logo...")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    fixed_products = {p.get('name'): p for p in products}
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_product, p): p for p in to_fix}
        
        for future in as_completed(futures):
            try:
                result = future.result()
                name = result.get('name')
                if result.get('logo') and result['logo'].startswith('http'):
                    fixed_products[name] = result
                    stats["fixed"] += 1
                else:
                    stats["failed"] += 1
            except Exception as e:
                print(f"  âŒ Error: {e}")
                stats["failed"] += 1
    
    # æ›´æ–°äº§å“åˆ—è¡¨
    updated_products = list(fixed_products.values())
    
    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(updated_products, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š ä¿®å¤ç»Ÿè®¡:")
    print(f"   æˆåŠŸä¿®å¤: {stats['fixed']}")
    print(f"   ä¿®å¤å¤±è´¥: {stats['failed']}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æ‰¹é‡ä¿®å¤äº§å“ Logo")
    parser.add_argument(
        "--input", "-i",
        default="data/products_featured.json",
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output", "-o",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤è¦†ç›–è¾“å…¥)"
    )
    parser.add_argument(
        "--dry-run", "-d",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¿®æ”¹æ–‡ä»¶"
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šæ–‡ä»¶è·¯å¾„
    script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    input_path = os.path.join(script_dir, args.input)
    output_path = os.path.join(script_dir, args.output) if args.output else input_path
    
    fix_logos(input_path, output_path, args.dry_run)


if __name__ == "__main__":
    main()
