#!/usr/bin/env python3
"""
RSS æ–°é—»èšåˆæ¨¡å—

åŠŸèƒ½:
- èšåˆå¤šä¸ª RSS æºçš„ AI/ç§‘æŠ€æ–°é—»
- æ”¯æŒ YouTube é¢‘é“è®¢é˜…
- è‡ªåŠ¨è¯†åˆ«åŒ…å«æ–°äº§å“ä¿¡æ¯çš„æ–‡ç« 
- è¾“å‡ºåˆ° blogs_news.json

ä½¿ç”¨:
    python tools/rss_feeds.py              # æŠ“å–æ‰€æœ‰æº
    python tools/rss_feeds.py --youtube    # åªæŠ“å– YouTube
    python tools/rss_feeds.py --tech       # åªæŠ“å–ç§‘æŠ€åª’ä½“
"""

import json
import os
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

try:
    import feedparser
    HAS_FEEDPARSER = True
except ImportError:
    HAS_FEEDPARSER = False
    print("âš ï¸ feedparser æœªå®‰è£…ï¼Œè¿è¡Œ: pip install feedparser")

# ============================================
# RSS æºé…ç½®
# ============================================

RSS_FEEDS = {
    # ä¸»æµç§‘æŠ€åª’ä½“
    "tech_media": [
        {
            "name": "TechCrunch AI",
            "url": "https://techcrunch.com/category/artificial-intelligence/feed/",
            "category": "funding",
            "language": "en",
        },
        {
            "name": "VentureBeat AI",
            "url": "https://venturebeat.com/category/ai/feed/",
            "category": "industry",
            "language": "en",
        },
        {
            "name": "The Verge AI",
            "url": "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml",
            "category": "product",
            "language": "en",
        },
        {
            "name": "Ars Technica",
            "url": "https://feeds.arstechnica.com/arstechnica/technology-lab",
            "category": "tech",
            "language": "en",
        },
        {
            "name": "Wired AI",
            "url": "https://www.wired.com/feed/tag/ai/latest/rss",
            "category": "industry",
            "language": "en",
        },
    ],
    
    # ç¤¾åŒº/è®ºå›
    "community": [
        {
            "name": "Hacker News AI",
            "url": "https://hnrss.org/newest?q=AI+startup",
            "category": "community",
            "language": "en",
        },
        {
            "name": "Reddit r/artificial",
            "url": "https://www.reddit.com/r/artificial/.rss",
            "category": "community",
            "language": "en",
        },
        {
            "name": "Reddit r/MachineLearning",
            "url": "https://www.reddit.com/r/MachineLearning/.rss",
            "category": "research",
            "language": "en",
        },
        {
            "name": "Product Hunt AI",
            "url": "https://www.producthunt.com/topics/artificial-intelligence/feed",
            "category": "product",
            "language": "en",
        },
    ],
    
    # ä¸­æ–‡åª’ä½“
    "chinese": [
        {
            "name": "36æ°ª",
            "url": "https://36kr.com/feed",
            "category": "funding",
            "language": "zh",
        },
        {
            "name": "æœºå™¨ä¹‹å¿ƒ",
            "url": "https://www.jiqizhixin.com/rss",
            "category": "research",
            "language": "zh",
        },
        {
            "name": "å°‘æ•°æ´¾",
            "url": "https://sspai.com/feed",
            "category": "product",
            "language": "zh",
        },
    ],
    
    # Newsletter
    "newsletter": [
        {
            "name": "The Rundown AI",
            "url": "https://www.therundown.ai/feed",
            "category": "news",
            "language": "en",
        },
        {
            "name": "Ben's Bites",
            "url": "https://bensbites.beehiiv.com/feed",
            "category": "tools",
            "language": "en",
        },
    ],
}

# YouTube é¢‘é“
YOUTUBE_CHANNELS = [
    {
        "name": "Two Minute Papers",
        "channel_id": "UCbfYPyITQ-7l4upoX8nvctg",
        "category": "research",
    },
    {
        "name": "AI Explained",
        "channel_id": "UCNJ1Ymd5yFuUPtn21xtRbbw",
        "category": "explainer",
    },
    {
        "name": "Matt Wolfe",
        "channel_id": "UCJIfeSCssxSC_Dhc5s7woww",
        "category": "tools",
    },
    {
        "name": "Fireship",
        "channel_id": "UCsBjURrPoezykLs9EqgamOA",
        "category": "dev",
    },
    {
        "name": "Yannic Kilcher",
        "channel_id": "UCZHmQk67mN2CWjyrAqjhmCQ",
        "category": "research",
    },
    {
        "name": "The AI Advantage",
        "channel_id": "UCjq5DjGAP57dv_zdi1HVtXg",
        "category": "tools",
    },
    {
        "name": "All About AI",
        "channel_id": "UCUyeluBRhGPCW4rPe_UvBZQ",
        "category": "tools",
    },
    {
        "name": "è·Ÿææ²å­¦AI",
        "channel_id": "UCfeLGcQHpqVFBG1hCqVV6bg",
        "category": "research",
    },
]

# ============================================
# RSS è§£æ
# ============================================

def get_youtube_rss(channel_id: str) -> str:
    """ç”Ÿæˆ YouTube é¢‘é“ RSS URL"""
    return f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"


def parse_date(date_str: str) -> Optional[datetime]:
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
    if not date_str:
        return None
    try:
        # feedparser é€šå¸¸è¿”å› struct_time
        if hasattr(date_str, 'tm_year'):
            return datetime(*date_str[:6])
        # å°è¯•å¸¸è§æ ¼å¼
        for fmt in [
            "%a, %d %b %Y %H:%M:%S %z",
            "%Y-%m-%dT%H:%M:%S%z",
            "%Y-%m-%d %H:%M:%S",
        ]:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
    except Exception:
        pass
    return None


def fetch_feed(feed_config: Dict) -> List[Dict]:
    """æŠ“å–å•ä¸ª RSS æº"""
    if not HAS_FEEDPARSER:
        return []
    
    url = feed_config.get("url")
    name = feed_config.get("name", url)
    
    try:
        feed = feedparser.parse(url)
        articles = []
        
        for entry in feed.entries[:20]:  # æ¯ä¸ªæºæœ€å¤š 20 æ¡
            published = parse_date(entry.get("published_parsed") or entry.get("updated_parsed"))
            
            # åªä¿ç•™æœ€è¿‘ 7 å¤©çš„æ–‡ç« 
            if published and published < datetime.now() - timedelta(days=7):
                continue
            
            article = {
                "title": entry.get("title", "").strip(),
                "link": entry.get("link", ""),
                "summary": entry.get("summary", "")[:500] if entry.get("summary") else "",
                "published": published.isoformat() if published else None,
                "source": name,
                "category": feed_config.get("category", "other"),
                "language": feed_config.get("language", "en"),
            }
            
            # æ¸…ç† HTML
            article["summary"] = re.sub(r'<[^>]+>', '', article["summary"]).strip()
            
            if article["title"] and article["link"]:
                articles.append(article)
        
        print(f"  âœ… {name}: {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    except Exception as e:
        print(f"  âŒ {name}: {e}")
        return []


def fetch_youtube_feeds() -> List[Dict]:
    """æŠ“å–æ‰€æœ‰ YouTube é¢‘é“"""
    articles = []
    print("\nğŸ“º æŠ“å– YouTube é¢‘é“...")
    
    for channel in YOUTUBE_CHANNELS:
        feed_config = {
            "url": get_youtube_rss(channel["channel_id"]),
            "name": f"YouTube: {channel['name']}",
            "category": channel.get("category", "video"),
            "language": "en",
        }
        articles.extend(fetch_feed(feed_config))
    
    return articles


def fetch_all_feeds(categories: List[str] = None) -> List[Dict]:
    """æŠ“å–æ‰€æœ‰ RSS æº"""
    all_articles = []
    
    if categories is None:
        categories = list(RSS_FEEDS.keys())
    
    for category in categories:
        if category not in RSS_FEEDS:
            continue
        
        print(f"\nğŸ“° æŠ“å– {category}...")
        for feed_config in RSS_FEEDS[category]:
            all_articles.extend(fetch_feed(feed_config))
    
    return all_articles


def identify_product_mentions(articles: List[Dict]) -> List[Dict]:
    """è¯†åˆ«æ–‡ç« ä¸­çš„äº§å“æåŠ (ç®€å•å…³é”®è¯åŒ¹é…)"""
    product_keywords = [
        "launch", "raises", "funding", "Series A", "Series B", "seed",
        "startup", "announces", "releases", "introduces", "unveils",
        "èèµ„", "å‘å¸ƒ", "æ¨å‡º", "ä¸Šçº¿", "è·æŠ•", "ä¼°å€¼",
    ]
    
    for article in articles:
        text = f"{article['title']} {article['summary']}".lower()
        article["has_product_mention"] = any(kw.lower() in text for kw in product_keywords)
    
    return articles


def save_articles(articles: List[Dict], output_file: str = None):
    """ä¿å­˜æ–‡ç« åˆ° JSON"""
    if output_file is None:
        output_file = os.path.join(PROJECT_ROOT, "data", "blogs_news.json")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
    articles.sort(key=lambda x: x.get("published") or "", reverse=True)
    
    # å»é‡
    seen = set()
    unique_articles = []
    for article in articles:
        key = article["link"]
        if key not in seen:
            seen.add(key)
            unique_articles.append(article)
    
    # ä¿å­˜
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique_articles, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ä¿å­˜ {len(unique_articles)} ç¯‡æ–‡ç« åˆ° {output_file}")
    return unique_articles


# ============================================
# CLI
# ============================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="RSS æ–°é—»èšåˆ")
    parser.add_argument("--youtube", action="store_true", help="åªæŠ“å– YouTube")
    parser.add_argument("--tech", action="store_true", help="åªæŠ“å–ç§‘æŠ€åª’ä½“")
    parser.add_argument("--chinese", action="store_true", help="åªæŠ“å–ä¸­æ–‡åª’ä½“")
    parser.add_argument("--community", action="store_true", help="åªæŠ“å–ç¤¾åŒº")
    parser.add_argument("--all", action="store_true", help="æŠ“å–æ‰€æœ‰æº")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if not HAS_FEEDPARSER:
        print("âŒ è¯·å…ˆå®‰è£… feedparser: pip install feedparser")
        return
    
    print("ğŸ”„ RSS æ–°é—»èšåˆ")
    print("=" * 50)
    
    all_articles = []
    
    if args.youtube:
        all_articles = fetch_youtube_feeds()
    elif args.tech:
        all_articles = fetch_all_feeds(["tech_media"])
    elif args.chinese:
        all_articles = fetch_all_feeds(["chinese"])
    elif args.community:
        all_articles = fetch_all_feeds(["community"])
    else:
        # é»˜è®¤æŠ“å–æ‰€æœ‰
        all_articles = fetch_all_feeds()
        all_articles.extend(fetch_youtube_feeds())
    
    # è¯†åˆ«äº§å“æåŠ
    all_articles = identify_product_mentions(all_articles)
    
    # ä¿å­˜
    save_articles(all_articles, args.output)
    
    # ç»Ÿè®¡
    product_articles = [a for a in all_articles if a.get("has_product_mention")]
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  - æ€»æ–‡ç« æ•°: {len(all_articles)}")
    print(f"  - åŒ…å«äº§å“æåŠ: {len(product_articles)}")


if __name__ == "__main__":
    main()
