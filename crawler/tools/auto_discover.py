#!/usr/bin/env python3
"""
è‡ªåŠ¨å‘ç°å…¨çƒ AI äº§å“ (v2.0 - é›†æˆ Web Search MCP)

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Zhipu Web Search MCP å®æ—¶æœç´¢å…¨çƒ AI äº§å“
2. æŒ‰åœ°åŒºåˆ†é…æœç´¢ä»»åŠ¡ (ç¾å›½40%/ä¸­å›½25%/æ¬§æ´²15%/æ—¥éŸ©10%/ä¸œå—äºš10%)
3. ä½¿ç”¨ä¸“ä¸š Prompt æå–äº§å“ä¿¡æ¯å¹¶è¯„åˆ†
4. è‡ªåŠ¨åˆ†ç±»åˆ°é»‘é©¬(4-5åˆ†)/æ½œåŠ›è‚¡(2-3åˆ†)

ç”¨æ³•ï¼š
    python tools/auto_discover.py                    # è¿è¡Œæ‰€æœ‰åœ°åŒº
    python tools/auto_discover.py --region us       # åªæœç´¢ç¾å›½
    python tools/auto_discover.py --region cn       # åªæœç´¢ä¸­å›½
    python tools/auto_discover.py --dry-run         # é¢„è§ˆä¸ä¿å­˜
    python tools/auto_discover.py --test-search     # æµ‹è¯• Web Search MCP
"""

import json
import os
import sys
import argparse
import re
import time
import requests
from datetime import datetime, timezone
from urllib.parse import urlparse
import subprocess

# æ™ºè°± AI é…ç½®
API_RATE_LIMIT_DELAY = 3  # æ¯æ¬¡ API è°ƒç”¨åç­‰å¾…ç§’æ•°
ZHIPU_API_KEY = os.environ.get('ZHIPU_API_KEY', '9c842f4999534eeba595b9fd142a699a.XXaPIGhbZTdzYIu8')
ZHIPU_MODEL = 'glm-4.7'

# Web Search MCP é…ç½®
WEB_SEARCH_MCP_URL = "https://open.bigmodel.cn/api/mcp/web_search/sse"
WEB_SEARCH_AUTH = ZHIPU_API_KEY

# Perplexity API é…ç½®
PERPLEXITY_API_KEY = os.environ.get('PERPLEXITY_API_KEY', '')
PERPLEXITY_MODEL = os.environ.get('PERPLEXITY_MODEL', 'sonar')  # sonar or sonar-pro

# Provider routing (toggle for testing)
USE_PERPLEXITY = os.environ.get('USE_PERPLEXITY', 'false').lower() == 'true'
REGION_PROVIDER_MAP = {
    'cn': 'glm',      # Always use GLM for Chinese content
    'us': 'perplexity' if USE_PERPLEXITY else 'glm',
    'eu': 'perplexity' if USE_PERPLEXITY else 'glm',
    'jp': 'perplexity' if USE_PERPLEXITY else 'glm',
    'kr': 'perplexity' if USE_PERPLEXITY else 'glm',
    'sea': 'perplexity' if USE_PERPLEXITY else 'glm',
}

# ============================================
# æ¯æ—¥é…é¢ç³»ç»Ÿ
# ============================================
import random

DAILY_QUOTA = {
    "dark_horses": 5,      # 4-5 åˆ†é»‘é©¬äº§å“
    "rising_stars": 10,    # 2-3 åˆ†æ½œåŠ›è‚¡
}

# æ¯åœ°åŒºæœ€å¤§äº§å“æ•°ï¼ˆé˜²æ­¢å•ä¸€åœ°åŒºä¸»å¯¼ï¼‰
REGION_MAX = {
    "us": 6, "cn": 4, "eu": 3, "jp": 2, "kr": 2, "sea": 2
}

MAX_ATTEMPTS = 3  # æœ€å¤§æœç´¢è½®æ•°

# ============================================
# å¤šè¯­è¨€å…³é”®è¯åº“ï¼ˆåŸç”Ÿè¯­è¨€æœç´¢æ•ˆæœæ›´å¥½ï¼‰
# ============================================
KEYWORDS_BY_REGION = {
    "us": [
        "AI startup funding 2026",
        "YC AI companies winter 2026",
        "AI Series A 2026",
        "artificial intelligence company raised funding",
        "AI unicorn startup valuation 2026",
    ],
    "cn": [
        "AIèèµ„ 2026",
        "äººå·¥æ™ºèƒ½åˆ›ä¸šå…¬å¸",
        "AIGCèèµ„",
        "å¤§æ¨¡å‹åˆ›ä¸š",
        "AIåˆ›ä¸šå…¬å¸ Aè½® Bè½®",
        "äººå·¥æ™ºèƒ½ ç‹¬è§’å…½ ä¼°å€¼",
    ],
    "eu": [
        "European AI startup funding 2026",
        "KI Startup Finanzierung",
        "AI Series A Europe",
        "UK France Germany AI startup",
    ],
    "jp": [
        "AI ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— è³‡é‡‘èª¿é” 2026",
        "æ—¥æœ¬ AIä¼æ¥­ ã‚·ãƒªãƒ¼ã‚ºA",
        "äººå·¥çŸ¥èƒ½ ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—",
        "Japan AI startup funding",
    ],
    "kr": [
        "AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì 2026",
        "í•œêµ­ ì¸ê³µì§€ëŠ¥ ê¸°ì—…",
        "AI ì‹œë¦¬ì¦ˆA",
        "Korean AI startup investment",
    ],
    "sea": [
        "Singapore AI startup funding 2026",
        "Southeast Asia AI company",
        "AI startup Indonesia Vietnam",
        "Tech in Asia artificial intelligence",
    ],
}

# ============================================
# ç«™ç‚¹å®šå‘æœç´¢ï¼ˆç›´æ¥æœç´¢ç›®æ ‡åª’ä½“ï¼‰
# ============================================
SITE_SEARCHES = {
    "us": [
        "site:techcrunch.com AI startup funding",
        "site:producthunt.com AI launch 2026",
        "site:venturebeat.com AI funding",
    ],
    "cn": [
        "site:36kr.com AIèèµ„",
        "site:tmtpost.com äººå·¥æ™ºèƒ½",
        "site:jiqizhixin.com èèµ„",
    ],
    "eu": [
        "site:sifted.eu AI funding",
        "site:tech.eu AI startup",
        "site:eu-startups.com AI",
    ],
    "jp": [
        "site:thebridge.jp AI startup",
        "site:jp.techcrunch.com AI",
    ],
    "kr": [
        "site:platum.kr AI ìŠ¤íƒ€íŠ¸ì—…",
        "site:besuccess.com AI",
    ],
    "sea": [
        "site:e27.co AI startup",
        "site:techinasia.com AI funding",
    ],
}

def get_keywords_for_today(region: str) -> list:
    """
    æ ¹æ®æ—¥æœŸè½®æ¢å…³é”®è¯æ± 

    Day 0,3,6 (Mon/Thu/Sun): é€šç”¨å…³é”®è¯
    Day 1,4 (Tue/Fri): ç«™ç‚¹å®šå‘æœç´¢
    Day 2,5 (Wed/Sat): åŸç”Ÿè¯­è¨€æ·±åº¦æœç´¢
    """
    day = datetime.now().weekday()
    pool_type = day % 3

    if pool_type == 0:
        # é€šç”¨å…³é”®è¯
        keywords = KEYWORDS_BY_REGION.get(region, KEYWORDS_BY_REGION["us"])
    elif pool_type == 1:
        # ç«™ç‚¹å®šå‘
        keywords = SITE_SEARCHES.get(region, SITE_SEARCHES["us"])
    else:
        # åŸç”Ÿè¯­è¨€ + è¡¥å……é€šç”¨
        native = KEYWORDS_BY_REGION.get(region, [])
        general = KEYWORDS_BY_REGION.get("us", [])[:2]
        keywords = native + general

    # éšæœºæ‰“ä¹±é¡ºåº
    shuffled = keywords.copy()
    random.shuffle(shuffled)
    return shuffled

def get_region_order() -> list:
    """éšæœºåŒ–åœ°åŒºæœç´¢é¡ºåºï¼Œé¿å…å›ºå®šåå·®"""
    regions = list(REGION_CONFIG.keys())
    random.shuffle(regions)
    return regions

# ============================================
# åœ°åŒºé…ç½® (æŒ‰æ¯”ä¾‹åˆ†é…æœç´¢ä»»åŠ¡)
# ============================================
REGION_CONFIG = {
    'us': {
        'name': 'ğŸ‡ºğŸ‡¸ ç¾å›½',
        'weight': 40,  # 40%
        'search_engine': 'bing',
        'keywords': [
            'AI startup funding Series A B 2026',
            'artificial intelligence company raised funding',
            'YC AI startup demo day 2026',
            'AI unicorn startup valuation',
        ],
    },
    'cn': {
        'name': 'ğŸ‡¨ğŸ‡³ ä¸­å›½',
        'weight': 25,  # 25%
        'search_engine': 'sogou',
        'keywords': [
            'AIåˆ›ä¸šå…¬å¸ èèµ„ AIGC å¤§æ¨¡å‹ è·æŠ•',
            'äººå·¥æ™ºèƒ½ åˆåˆ›å…¬å¸ Aè½® Bè½® èèµ„',
            'å¤§æ¨¡å‹ åˆ›ä¸šå…¬å¸ ä¼°å€¼ èèµ„æ–°é—»',
            'AIGC ç‹¬è§’å…½ èèµ„ 2026',
        ],
    },
    'eu': {
        'name': 'ğŸ‡ªğŸ‡º æ¬§æ´²',
        'weight': 15,  # 15%
        'search_engine': 'bing',
        'keywords': [
            'European AI startup funding Sifted',
            'Europe artificial intelligence company raised',
            'UK France Germany AI startup Series A',
        ],
    },
    'jp': {
        'name': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡· æ—¥éŸ©',
        'weight': 10,  # 10%
        'search_engine': 'bing',
        'keywords': [
            'Japan Korea AI startup funding',
            'Japanese artificial intelligence company raised',
            'Korean AI startup investment',
        ],
    },
    'sea': {
        'name': 'ğŸ‡¸ğŸ‡¬ ä¸œå—äºš',
        'weight': 10,  # 10%
        'search_engine': 'bing',
        'keywords': [
            'Southeast Asia AI startup e27 funding',
            'Singapore Indonesia Vietnam AI company raised',
            'Tech in Asia artificial intelligence funding',
        ],
    },
}

# ============================================
# ä¸“ä¸š Prompts (åŒè¯­ç‰ˆ - åˆå¹¶æå–+è¯„åˆ†)
# ============================================

# è‹±æ–‡ç‰ˆ Prompt (us/eu/jp/kr/sea)
PROMPT_EXTRACTION_EN = """You are WeeklyAI's AI Product Analyst. Extract and score AI products from search results.

## Search Results
{search_results}

## STRICT EXCLUSIONS (Never Include):
### Well-Known Products
- ChatGPT, Claude, Gemini, Copilot, DALL-E, Sora, Midjourney
- Cursor, Perplexity, ElevenLabs, Synthesia, Runway, Pika, Bolt.new, v0.dev

### Not Products
- Dev libraries: LangChain, PyTorch, TensorFlow, HuggingFace models
- Papers only, demos only, GitHub repos without product
- Tool directories: "Best AI tools for X"

### Big Tech Products
- Google Gemini, Meta Llama, OpenAI products, Microsoft Copilot

## DARK HORSE CRITERIA (Score 4-5) - MUST meet at least 2:
| Dimension | Signal |
|-----------|--------|
| growth_anomaly | Fast funding, ARR growth >100%/yr |
| founder_background | Ex-OpenAI/Google/Meta exec |
| funding_signal | Seed >$50M, valuation >3x growth |
| category_innovation | First of its kind |
| community_buzz | HN/Reddit viral but still small |

**5 points**: Funding >$100M OR Top-tier founder OR Category creator
**4 points**: Funding >$30M OR YC/a16z backed OR ARR >$10M

## RISING STAR CRITERIA (Score 2-3) - Need only 1:
**3 points**: Funding $1M-$5M OR ProductHunt top 10
**2 points**: Just launched, clear innovation

## CRITICAL: why_matters Requirements
Products with generic descriptions will be REJECTED.

âœ… GOOD: "Sequoiaé¢†æŠ•$50Mï¼Œ8ä¸ªæœˆARRä»0åˆ°$10Mï¼Œé¦–ä¸ªAIåŸç”Ÿä»£ç ç¼–è¾‘å™¨"
âŒ BAD: "This is a promising AI product"

## Output (JSON only)
```json
[
  {{
    "name": "Product name",
    "website": "https://...",
    "description": "ä¸€å¥è¯äº§å“æè¿°ï¼ˆä¸­æ–‡ï¼Œ20å­—ä»¥ä¸Šï¼‰",
    "category": "coding/image/video/voice/writing/hardware/finance/education/healthcare/other",
    "region": "{region}",
    "funding_total": "$50M Series A",
    "dark_horse_index": 4,
    "criteria_met": ["funding_signal", "category_innovation"],
    "why_matters": "å…·ä½“æ•°å­—+å…·ä½“å·®å¼‚åŒ–ï¼ˆä¸­æ–‡ï¼‰",
    "latest_news": "2026-01: Event",
    "source": "Source",
    "confidence": 0.85
  }}
]
```

Quota: Dark Horses (4-5): {quota_dark_horses} | Rising Stars (2-3): {quota_rising_stars}
Return empty array [] if no qualifying products found. Quality over quantity."""

# ä¸­æ–‡ç‰ˆ Prompt (cn)
PROMPT_EXTRACTION_CN = """ä½ æ˜¯ WeeklyAI çš„ AI äº§å“åˆ†æå¸ˆã€‚ä»æœç´¢ç»“æœä¸­æå–å¹¶è¯„åˆ† AI äº§å“ã€‚

## æœç´¢ç»“æœ
{search_results}

## ä¸¥æ ¼æ’é™¤ï¼š
### å·²ç»äººå°½çš†çŸ¥
- ChatGPT, Claude, Gemini, Copilot, DALL-E, Sora, Midjourney
- Cursor, Perplexity, Kimi, è±†åŒ…, é€šä¹‰åƒé—®, æ–‡å¿ƒä¸€è¨€

### ä¸æ˜¯äº§å“
- å¼€å‘åº“: LangChain, PyTorch, TensorFlow, HuggingFace models
- åªæœ‰è®ºæ–‡/demo/GitHubé¡¹ç›®
- å·¥å…·èšåˆ: "xxx AIå·¥å…·åˆé›†"

### å¤§å‚äº§å“
- Google Gemini, Meta Llama, OpenAI, ç™¾åº¦æ–‡å¿ƒ, é˜¿é‡Œé€šä¹‰

## é»‘é©¬æ ‡å‡† (4-5åˆ†) - å¿…é¡»æ»¡è¶³è‡³å°‘2æ¡ï¼š
| ç»´åº¦ | ä¿¡å· |
|------|------|
| growth_anomaly | èèµ„é€Ÿåº¦å¿«ã€ARRå¹´å¢é•¿>100% |
| founder_background | å¤§å‚é«˜ç®¡å‡ºèµ° (å‰OpenAI/Google/Meta) |
| funding_signal | ç§å­è½®>$50Mã€ä¼°å€¼å¢é•¿>3x |
| category_innovation | é¦–åˆ›æ–°å“ç±» |
| community_buzz | HN/Redditçˆ†ç«ä½†äº§å“è¿˜å° |

**5åˆ†**: èèµ„>$100M æˆ– é¡¶çº§åˆ›å§‹äºº æˆ– å“ç±»å¼€åˆ›è€…
**4åˆ†**: èèµ„>$30M æˆ– YC/a16zèƒŒä¹¦ æˆ– ARR>$10M

## æ½œåŠ›è‚¡æ ‡å‡† (2-3åˆ†) - åªéœ€1æ¡ï¼š
**3åˆ†**: èèµ„$1M-$5M æˆ– ProductHunt Top 10
**2åˆ†**: åˆšå‘å¸ƒä½†æœ‰æ˜æ˜¾åˆ›æ–°

## why_matters è¦æ±‚ï¼ˆæé‡è¦ï¼ï¼‰
æ³›åŒ–æè¿°ä¼šè¢«è¿‡æ»¤ã€‚

âœ… GOOD: "Sequoiaé¢†æŠ•$50Mï¼Œ8ä¸ªæœˆARRä»0åˆ°$10Mï¼Œé¦–ä¸ªAIåŸç”Ÿä»£ç ç¼–è¾‘å™¨"
âŒ BAD: "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ½œåŠ›çš„AIäº§å“"

## è¾“å‡ºï¼ˆåªè¿”å›JSONï¼‰
```json
[
  {{
    "name": "äº§å“å",
    "website": "https://...",
    "description": "ä¸€å¥è¯äº§å“æè¿°ï¼ˆä¸­æ–‡ï¼Œ20å­—ä»¥ä¸Šï¼‰",
    "category": "coding/image/video/...",
    "region": "{region}",
    "funding_total": "$50M Series A",
    "dark_horse_index": 4,
    "criteria_met": ["funding_signal", "category_innovation"],
    "why_matters": "å…·ä½“æ•°å­—+å…·ä½“å·®å¼‚åŒ–",
    "latest_news": "2026-01: äº‹ä»¶",
    "source": "æ¥æº",
    "confidence": 0.85
  }}
]
```

é…é¢: é»‘é©¬ (4-5): {quota_dark_horses} | æ½œåŠ›è‚¡ (2-3): {quota_rising_stars}
å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„äº§å“ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚ä¼˜å…ˆè´¨é‡ï¼Œå®ç¼ºæ¯‹æ»¥ã€‚"""

# ä¿ç•™æ—§çš„è¯„åˆ† prompt ç”¨äºå…¼å®¹ï¼ˆå¯é€‰ï¼Œå•ç‹¬è¯„åˆ†æ—¶ä½¿ç”¨ï¼‰
PROMPT_DARK_HORSE_SCORING = """è¯„ä¼°äº§å“çš„"é»‘é©¬æŒ‡æ•°"(1-5åˆ†)ï¼š

## äº§å“
{product}

## è¯„åˆ†æ ‡å‡†
5åˆ†: èèµ„>$100M æˆ– é¡¶çº§åˆ›å§‹äººèƒŒæ™¯ æˆ– å“ç±»å¼€åˆ›è€… æˆ– ARR>$50M
4åˆ†: èèµ„>$30M æˆ– YC/a16zæŠ•èµ„ æˆ– ä¼°å€¼å¢é•¿>3x æˆ– ARR>$10M
3åˆ†: èèµ„$5M-$30M æˆ– ProductHunt Top 5 æˆ– æœ¬åœ°å¸‚åœºçƒ­åº¦é«˜
2åˆ†: æœ‰åˆ›æ–°ç‚¹ä½†æ•°æ®ä¸è¶³ æˆ– æ—©æœŸäº§å“æœ‰æ½œåŠ›
1åˆ†: è¾¹ç¼˜äº§å“ æˆ– å¾…éªŒè¯ æˆ– ä¿¡æ¯å¤ªå°‘

## è¿”å›æ ¼å¼ï¼ˆåªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰
```json
{{
  "dark_horse_index": 4,
  "reason": "è¯„åˆ†ç†ç”±ï¼ˆå…·ä½“è¯´æ˜ä¾æ®ï¼‰"
}}
```"""


def get_extraction_prompt(region_key: str) -> str:
    """
    æ ¹æ®åœ°åŒºé€‰æ‹©åˆé€‚çš„ prompt

    Args:
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea)

    Returns:
        å¯¹åº”åœ°åŒºçš„ prompt æ¨¡æ¿
    """
    if region_key == "cn":
        return PROMPT_EXTRACTION_CN
    else:
        return PROMPT_EXTRACTION_EN


PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

# æ•°æ®æ–‡ä»¶è·¯å¾„
DARK_HORSES_DIR = os.path.join(PROJECT_ROOT, 'data', 'dark_horses')
RISING_STARS_DIR = os.path.join(PROJECT_ROOT, 'data', 'rising_stars')
CANDIDATES_DIR = os.path.join(PROJECT_ROOT, 'data', 'candidates')

# æ¸ é“é…ç½®
SOURCES = {
    # ç¾å›½æ¸ é“
    'techcrunch': {
        'name': 'TechCrunch',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://techcrunch.com/category/artificial-intelligence/',
        'rss': 'https://techcrunch.com/category/artificial-intelligence/feed/',
        'keywords': ['raises', 'Series A', 'Series B', 'funding', 'AI startup'],
        'tier': 1,
    },
    'producthunt': {
        'name': 'ProductHunt',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://www.producthunt.com/topics/artificial-intelligence',
        'api': 'https://api.producthunt.com/v2/api/graphql',
        'keywords': ['AI', 'machine learning', 'LLM'],
        'tier': 2,
    },
    'ycombinator': {
        'name': 'Y Combinator',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://www.ycombinator.com/companies?tags=AI',
        'keywords': ['YC', 'Demo Day'],
        'tier': 1,
    },

    # ä¸­å›½æ¸ é“
    '36kr': {
        'name': '36æ°ª',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://36kr.com/information/AI/',
        'rss': 'https://36kr.com/feed',
        'keywords': ['AIèèµ„', 'äººå·¥æ™ºèƒ½', 'AIGC', 'å¤§æ¨¡å‹', 'è·æŠ•'],
        'tier': 1,
    },
    'itjuzi': {
        'name': 'ITæ¡”å­',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://www.itjuzi.com/investevent',
        'keywords': ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ '],
        'tier': 1,
    },
    'jiqizhixin': {
        'name': 'æœºå™¨ä¹‹å¿ƒ',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://www.jiqizhixin.com/',
        'rss': 'https://www.jiqizhixin.com/rss',
        'keywords': ['AI', 'èèµ„', 'åˆ›ä¸š'],
        'tier': 2,
    },

    # æ¬§æ´²æ¸ é“
    'sifted': {
        'name': 'Sifted',
        'region': 'ğŸ‡ªğŸ‡º',
        'url': 'https://sifted.eu/sector/artificial-intelligence',
        'keywords': ['AI', 'funding', 'European startup'],
        'tier': 1,
    },
    'eu_startups': {
        'name': 'EU-Startups',
        'region': 'ğŸ‡ªğŸ‡º',
        'url': 'https://www.eu-startups.com/category/artificial-intelligence/',
        'rss': 'https://www.eu-startups.com/feed/',
        'keywords': ['AI', 'raises', 'funding'],
        'tier': 2,
    },

    # æ—¥éŸ©æ¸ é“
    'bridge': {
        'name': 'Bridge',
        'region': 'ğŸ‡¯ğŸ‡µ',
        'url': 'https://thebridge.jp/en/',
        'keywords': ['AI', 'startup', 'funding', 'Japan'],
        'tier': 1,
    },
    'platum': {
        'name': 'Platum',
        'region': 'ğŸ‡°ğŸ‡·',
        'url': 'https://platum.kr/archives/category/ai',
        'keywords': ['AI', 'startup', 'Korea'],
        'tier': 1,
    },

    # ä¸œå—äºšæ¸ é“
    'e27': {
        'name': 'e27',
        'region': 'ğŸ‡¸ğŸ‡¬',
        'url': 'https://e27.co/tag/artificial-intelligence/',
        'keywords': ['AI', 'Southeast Asia', 'funding'],
        'tier': 1,
    },
    'techinasia': {
        'name': 'Tech in Asia',
        'region': 'ğŸ‡¸ğŸ‡¬',
        'url': 'https://www.techinasia.com/tag/artificial-intelligence',
        'keywords': ['AI', 'Asia', 'startup'],
        'tier': 1,
    },
}


def get_current_week():
    """è·å–å½“å‰å‘¨æ•°"""
    now = datetime.now()
    return f"{now.year}_{now.isocalendar()[1]:02d}"


def load_existing_products():
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„äº§å“åç§°å’Œç½‘å€"""
    existing = set()

    # åŠ è½½é»‘é©¬
    if os.path.exists(DARK_HORSES_DIR):
        for f in os.listdir(DARK_HORSES_DIR):
            if f.endswith('.json'):
                with open(os.path.join(DARK_HORSES_DIR, f), 'r') as file:
                    products = json.load(file)
                    for p in products:
                        existing.add(p.get('name', '').lower())
                        existing.add(p.get('website', '').lower())

    # åŠ è½½æ½œåŠ›è‚¡
    if os.path.exists(RISING_STARS_DIR):
        for f in os.listdir(RISING_STARS_DIR):
            if f.endswith('.json'):
                with open(os.path.join(RISING_STARS_DIR, f), 'r') as file:
                    products = json.load(file)
                    for p in products:
                        existing.add(p.get('name', '').lower())
                        existing.add(p.get('website', '').lower())

    return existing


def is_duplicate(name: str, website: str, existing: set) -> bool:
    """æ£€æŸ¥æ˜¯å¦é‡å¤"""
    return name.lower() in existing or website.lower() in existing


def normalize_url(url: str) -> str:
    """
    æ ‡å‡†åŒ– URLï¼Œæå–ä¸»åŸŸåç”¨äºå»é‡

    "https://www.example.com/page" â†’ "example.com"
    """
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.replace("www.", "")
        return domain.lower()
    except:
        return url.lower()


def is_duplicate_domain(product: dict, existing_domains: set) -> bool:
    """æ£€æŸ¥åŸŸåæ˜¯å¦å·²å­˜åœ¨"""
    domain = normalize_url(product.get("website", ""))
    return domain in existing_domains if domain else False


# ============================================
# è´¨é‡è¿‡æ»¤å™¨
# ============================================

# æ³›åŒ–çš„ why_matters é»‘åå•ï¼ˆä¼šè¢«è¿‡æ»¤æ‰ï¼‰
GENERIC_WHY_MATTERS = [
    "å¾ˆæœ‰æ½œåŠ›",
    "å€¼å¾—å…³æ³¨",
    "æœ‰å‰æ™¯",
    "è¡¨ç°ä¸é”™",
    "å›¢é˜ŸèƒŒæ™¯ä¸é”™",
    "èèµ„æƒ…å†µè‰¯å¥½",
    "å¸‚åœºå‰æ™¯å¹¿é˜”",
    "æŠ€æœ¯å®åŠ›å¼º",
    "ç”¨æˆ·åé¦ˆè‰¯å¥½",
    "å¢é•¿è¿…é€Ÿ",
]

# çŸ¥åäº§å“æ’é™¤åå•ï¼ˆä¸æ˜¯é»‘é©¬ï¼‰
WELL_KNOWN_PRODUCTS = {
    # å›½é™…çŸ¥å AI äº§å“
    "chatgpt", "openai", "claude", "anthropic", "gemini", "bard",
    "copilot", "github copilot", "dall-e", "dall-e 3", "sora",
    "midjourney", "stable diffusion", "stability ai",
    "cursor", "perplexity", "elevenlabs", "eleven labs",
    "synthesia", "runway", "runway ml", "pika", "pika labs",
    "bolt.new", "bolt", "v0.dev", "v0", "replit", "together ai", "groq",
    "character.ai", "character ai", "jasper", "jasper ai",
    "notion ai", "grammarly", "copy.ai", "writesonic",
    "huggingface", "hugging face", "langchain", "llamaindex",
    # ä¸­å›½çŸ¥å AI äº§å“
    "kimi", "æœˆä¹‹æš—é¢", "moonshot", "doubao", "è±†åŒ…", "å­—èŠ‚è·³åŠ¨",
    "tongyi", "é€šä¹‰åƒé—®", "é€šä¹‰", "qwen", "wenxin", "æ–‡å¿ƒä¸€è¨€", "æ–‡å¿ƒ",
    "ernie", "ç™¾åº¦", "baidu", "æ™ºè°±", "zhipu", "chatglm", "glm",
    "è®¯é£æ˜Ÿç«", "æ˜Ÿç«", "spark", "minimax", "abab",
    # å¤§å‚äº§å“
    "google gemini", "google bard", "meta llama", "llama",
    "microsoft copilot", "bing chat", "amazon q", "aws bedrock",
}


def validate_product(product: dict) -> tuple[bool, str]:
    """
    éªŒè¯äº§å“è´¨é‡ï¼Œè¿”å› (æ˜¯å¦é€šè¿‡, åŸå› )

    è¿‡æ»¤æ¡ä»¶:
    1. å¿…é¡»æœ‰æœ‰æ•ˆçš„ website URL
    2. description å¿…é¡» >20 å­—ç¬¦
    3. why_matters ä¸èƒ½æ˜¯æ³›åŒ–æè¿°
    4. name ä¸èƒ½æ˜¯æ–°é—»æ ‡é¢˜
    5. çŸ¥åäº§å“æ’é™¤ï¼ˆä½¿ç”¨ WELL_KNOWN_PRODUCTSï¼‰
    6. é»‘é©¬(4-5åˆ†)å¿…é¡»æ»¡è¶³è‡³å°‘2æ¡æ ‡å‡† (criteria_met)
    7. ç½®ä¿¡åº¦æ£€æŸ¥ (confidence >= 0.6)
    """
    name = product.get("name", "").strip()
    website = product.get("website", "").strip()
    description = product.get("description", "").strip()
    why_matters = product.get("why_matters", "").strip()

    # 1. æ£€æŸ¥å¿…å¡«å­—æ®µ
    if not name:
        return False, "missing name"
    if not website:
        return False, "missing website"
    if not description:
        return False, "missing description"
    if not why_matters:
        return False, "missing why_matters"

    # 2. æ£€æŸ¥ website æ˜¯å¦æ˜¯æœ‰æ•ˆ URL
    if not website.startswith(("http://", "https://")):
        return False, "invalid website URL"

    # 3. æ£€æŸ¥ description é•¿åº¦
    if len(description) < 20:
        return False, f"description too short ({len(description)} chars)"

    # 4. æ£€æŸ¥ why_matters æ˜¯å¦å¤ªæ³›åŒ–
    why_lower = why_matters.lower()
    for generic in GENERIC_WHY_MATTERS:
        if generic in why_lower and len(why_matters) < 50:
            return False, f"generic why_matters: contains '{generic}'"

    # 5. æ£€æŸ¥ why_matters æ˜¯å¦åŒ…å«å…·ä½“æ•°å­—ï¼ˆèèµ„/ARR/ç”¨æˆ·æ•°ï¼‰
    has_number = bool(re.search(r'[\$Â¥â‚¬]\d+|ARR|\d+[MBKä¸‡äº¿]|\d+%', why_matters))
    has_specific = any(kw in why_matters for kw in [
        'é¢†æŠ•', 'èèµ„', 'ä¼°å€¼', 'ç”¨æˆ·', 'å¢é•¿', 'ARR', 'é¦–åˆ›', 'é¦–ä¸ª',
        'å‰OpenAI', 'å‰Google', 'å‰Meta', 'YC', 'a16z', 'Sequoia',
    ])
    if not has_number and not has_specific:
        return False, "why_matters lacks specific details"

    # 6. æ£€æŸ¥ name æ˜¯å¦åƒæ–°é—»æ ‡é¢˜
    news_patterns = ['èèµ„', 'å®£å¸ƒ', 'å‘å¸ƒ', 'è·å¾—', 'å®Œæˆ', 'æ¨å‡º', 'ä¸Šçº¿']
    if any(p in name for p in news_patterns) and len(name) > 15:
        return False, "name looks like news headline"

    # 7. æ£€æŸ¥æ˜¯å¦æ˜¯çŸ¥åäº§å“
    name_lower = name.lower()
    if name_lower in WELL_KNOWN_PRODUCTS:
        return False, f"well-known product: {name}"
    # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆä¾‹å¦‚ "ChatGPT Plus" åŒ…å« "chatgpt"ï¼‰
    for known in WELL_KNOWN_PRODUCTS:
        if known in name_lower or name_lower in known:
            return False, f"well-known product match: {known}"

    # 8. æ£€æŸ¥é»‘é©¬(4-5åˆ†)æ˜¯å¦æ»¡è¶³è‡³å°‘2æ¡æ ‡å‡†
    score = product.get("dark_horse_index", 0)
    criteria = product.get("criteria_met", [])
    if score >= 4 and len(criteria) < 2:
        return False, f"dark_horse needs â‰¥2 criteria (has {len(criteria)})"

    # 9. æ£€æŸ¥ç½®ä¿¡åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
    confidence = product.get("confidence", 1.0)
    if confidence < 0.6:
        return False, f"low confidence ({confidence:.2f})"

    return True, "passed"


def load_existing_domains() -> set:
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„äº§å“åŸŸå"""
    domains = set()

    for dir_path in [DARK_HORSES_DIR, RISING_STARS_DIR]:
        if os.path.exists(dir_path):
            for f in os.listdir(dir_path):
                if f.endswith('.json'):
                    try:
                        with open(os.path.join(dir_path, f), 'r') as file:
                            products = json.load(file)
                            for p in products:
                                domain = normalize_url(p.get('website', ''))
                                if domain:
                                    domains.add(domain)
                    except:
                        pass

    return domains


def get_zhipu_client():
    """è·å–æ™ºè°± AI å®¢æˆ·ç«¯"""
    try:
        from zhipuai import ZhipuAI
        return ZhipuAI(api_key=ZHIPU_API_KEY)
    except ImportError:
        print("  Error: zhipuai SDK not installed. Run: pip install zhipuai")
        return None


def get_perplexity_client():
    """Get Perplexity API client (uses requests)"""
    if not PERPLEXITY_API_KEY:
        print("  Error: PERPLEXITY_API_KEY not set")
        return None
    return {"api_key": PERPLEXITY_API_KEY, "model": PERPLEXITY_MODEL}


def perplexity_search(query: str, count: int = 10) -> list:
    """
    Use Perplexity Search API for web search

    Returns: [{"title": "", "url": "", "content": ""}, ...]
    """
    client = get_perplexity_client()
    if not client:
        return []

    try:
        print(f"  ğŸ” Perplexity Search: {query[:50]}...")

        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers={
                "Authorization": f"Bearer {client['api_key']}",
                "Content-Type": "application/json"
            },
            json={
                "model": client['model'],
                "messages": [{"role": "user", "content": f"Search for: {query}"}],
                "search_domain_filter": [],  # No domain filter
                "return_citations": True,
                "return_related_questions": False
            },
            timeout=30
        )
        response.raise_for_status()
        data = response.json()

        # Extract citations as search results
        results = []
        if 'citations' in data:
            for citation in data['citations'][:count]:
                results.append({
                    "title": citation.get('title', ''),
                    "url": citation.get('url', ''),
                    "content": citation.get('snippet', '')
                })

        # Also extract from content if no citations
        if not results and data.get('choices'):
            content = data['choices'][0]['message']['content']
            results = [{"title": "Search Results", "url": "", "content": content}]

        print(f"  âœ… Found {len(results)} results")
        return results

    except Exception as e:
        print(f"  âŒ Perplexity Search Error: {e}")
        return []
    finally:
        time.sleep(API_RATE_LIMIT_DELAY)


def web_search_mcp(query: str, search_engine: str = "bing", count: int = 10) -> list:
    """
    ä½¿ç”¨ Zhipu Web Search MCP è¿›è¡Œå®æ—¶ç½‘ç»œæœç´¢

    Args:
        query: æœç´¢å…³é”®è¯
        search_engine: æœç´¢å¼•æ“ (bing/sogou/quark/jina)
        count: è¿”å›ç»“æœæ•°é‡

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ [{"title": "", "url": "", "content": ""}, ...]
    """
    # ä½¿ç”¨æ™ºè°± AI API è¿›è¡Œ web_search å·¥å…·è°ƒç”¨
    client = get_zhipu_client()
    if not client:
        return []

    try:
        print(f"  ğŸ” Web Search: {query[:50]}...")

        # ä½¿ç”¨ GLM-4.7 çš„ web_search å·¥å…·
        response = client.chat.completions.create(
            model="glm-4-plus",  # æ”¯æŒ web_search çš„æ¨¡å‹
            messages=[{
                "role": "user",
                "content": f"æœç´¢æœ€æ–°çš„ AI åˆ›ä¸šå…¬å¸èèµ„æ–°é—»: {query}"
            }],
            tools=[{
                "type": "web_search",
                "web_search": {
                    "enable": True,
                    "search_engine": search_engine,
                    "search_result": True
                }
            }],
            tool_choice="auto",
            max_tokens=4096
        )

        # æå–æœç´¢ç»“æœ
        results = []

        # æ£€æŸ¥æ˜¯å¦æœ‰ web_search ç»“æœ
        if hasattr(response, 'web_search') and response.web_search:
            for item in response.web_search:
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("link", item.get("url", "")),
                    "content": item.get("content", item.get("snippet", ""))
                })

        # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–ç»“æœï¼Œä»å›å¤ä¸­æå–
        if not results and response.choices:
            content = response.choices[0].message.content
            # è¿”å›åŸå§‹å†…å®¹ä¾›åç»­å¤„ç†
            results = [{"title": "Search Results", "url": "", "content": content}]

        print(f"  âœ… Found {len(results)} results")
        return results

    except Exception as e:
        print(f"  âŒ Web Search Error: {e}")
        # é™çº§ï¼šä½¿ç”¨ GLM çŸ¥è¯†åº“
        return []
    finally:
        time.sleep(API_RATE_LIMIT_DELAY)


def analyze_with_glm(content: str, task: str = "extract", region: str = "ğŸ‡ºğŸ‡¸",
                     quota_remaining: dict = None, region_key: str = "us") -> dict:
    """
    ä½¿ç”¨ GLM-4.7 åˆ†æå†…å®¹ (ä½¿ç”¨åŒè¯­ Prompt - åˆå¹¶æå–+è¯„åˆ†)

    Args:
        content: è¦åˆ†æçš„å†…å®¹ï¼ˆæœç´¢ç»“æœã€äº§å“ä¿¡æ¯ç­‰ï¼‰
        task: ä»»åŠ¡ç±»å‹ (extract/score/translate)
        region: åœ°åŒºæ ‡è¯† (emoji flag)
        quota_remaining: å‰©ä½™é…é¢ {"dark_horses": n, "rising_stars": m}
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea) ç”¨äºé€‰æ‹© prompt è¯­è¨€

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    client = get_zhipu_client()
    if not client:
        return {}

    # é»˜è®¤é…é¢
    if quota_remaining is None:
        quota_remaining = DAILY_QUOTA.copy()

    if task == "extract":
        # ä½¿ç”¨åŒè¯­ prompt é€‰æ‹©å™¨ï¼ˆåˆå¹¶æå–+è¯„åˆ†ï¼‰
        prompt_template = get_extraction_prompt(region_key)
        prompt = prompt_template.format(
            search_results=content[:10000],
            region=region,
            quota_dark_horses=quota_remaining.get("dark_horses", 5),
            quota_rising_stars=quota_remaining.get("rising_stars", 10)
        )

    elif task == "score":
        # ä¿ç•™å•ç‹¬è¯„åˆ†åŠŸèƒ½ï¼ˆç”¨äº fallbackï¼‰
        prompt = PROMPT_DARK_HORSE_SCORING.format(
            product=json.dumps(content, ensure_ascii=False, indent=2)
        )

    elif task == "translate":
        prompt = f"""å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­ï¼š

{content}

åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4096
        )

        result_text = response.choices[0].message.content

        # æå– JSON
        if task in ["extract", "score"]:
            # å°è¯•æå– JSON
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
            if json_match:
                return json.loads(json_match.group(1))
            # å°è¯•ç›´æ¥è§£æ
            try:
                return json.loads(result_text)
            except:
                return {}
        else:
            return {"text": result_text}

    except Exception as e:
        print(f"  GLM Error: {e}")
        return {}
    finally:
        # é™æµï¼šæ¯æ¬¡ API è°ƒç”¨åç­‰å¾…
        time.sleep(API_RATE_LIMIT_DELAY)


def analyze_with_perplexity(content: str, task: str = "extract", region: str = "ğŸ‡ºğŸ‡¸",
                            quota_remaining: dict = None, region_key: str = "us") -> dict:
    """
    Use Perplexity Sonar for content analysis (extraction/scoring)
    Same interface as analyze_with_glm()

    Args:
        content: è¦åˆ†æçš„å†…å®¹
        task: ä»»åŠ¡ç±»å‹ (extract/score)
        region: åœ°åŒºæ ‡è¯† (emoji flag)
        quota_remaining: å‰©ä½™é…é¢
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea) ç”¨äºé€‰æ‹© prompt è¯­è¨€
    """
    client = get_perplexity_client()
    if not client:
        return {}

    if quota_remaining is None:
        quota_remaining = DAILY_QUOTA.copy()

    if task == "extract":
        # ä½¿ç”¨åŒè¯­ prompt é€‰æ‹©å™¨ï¼ˆåˆå¹¶æå–+è¯„åˆ†ï¼‰
        prompt_template = get_extraction_prompt(region_key)
        prompt = prompt_template.format(
            search_results=content[:10000],
            region=region,
            quota_dark_horses=quota_remaining.get("dark_horses", 5),
            quota_rising_stars=quota_remaining.get("rising_stars", 10)
        )
    elif task == "score":
        # ä¿ç•™å•ç‹¬è¯„åˆ†åŠŸèƒ½ï¼ˆç”¨äº fallbackï¼‰
        prompt = PROMPT_DARK_HORSE_SCORING.format(
            product=json.dumps(content, ensure_ascii=False, indent=2)
        )
    else:
        return {}

    try:
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers={
                "Authorization": f"Bearer {client['api_key']}",
                "Content-Type": "application/json"
            },
            json={
                "model": client['model'],
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 4096
            },
            timeout=60
        )
        response.raise_for_status()
        data = response.json()

        result_text = data['choices'][0]['message']['content']

        # Extract JSON (same logic as GLM)
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
        if json_match:
            return json.loads(json_match.group(1))
        try:
            return json.loads(result_text)
        except:
            return {}

    except Exception as e:
        print(f"  Perplexity Error: {e}")
        return {}
    finally:
        time.sleep(API_RATE_LIMIT_DELAY)


# ============================================
# Provider Routing Functions
# ============================================

def get_provider_for_region(region_key: str) -> str:
    """Get provider name for region"""
    return REGION_PROVIDER_MAP.get(region_key, 'glm')


def search_with_provider(query: str, region_key: str, search_engine: str = "bing") -> list:
    """Route search to appropriate provider"""
    provider = get_provider_for_region(region_key)
    if provider == 'perplexity':
        return perplexity_search(query)
    else:
        return web_search_mcp(query, search_engine)


def analyze_with_provider(content, task: str, region_key: str, region_flag: str = "ğŸ‡ºğŸ‡¸",
                          quota_remaining: dict = None):
    """
    Route analysis to appropriate provider

    Args:
        content: è¦åˆ†æçš„å†…å®¹
        task: ä»»åŠ¡ç±»å‹ (extract/score)
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea) ç”¨äºé€‰æ‹© provider å’Œ prompt è¯­è¨€
        region_flag: åœ°åŒºæ ‡è¯† (emoji flag)
        quota_remaining: å‰©ä½™é…é¢
    """
    provider = get_provider_for_region(region_key)
    if provider == 'perplexity':
        return analyze_with_perplexity(content, task, region_flag, quota_remaining, region_key)
    else:
        return analyze_with_glm(content, task, region_flag, quota_remaining, region_key)


def fetch_url_content(url: str) -> str:
    """æŠ“å– URL å†…å®¹"""
    try:
        import urllib.request
        req = urllib.request.Request(
            url,
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        )
        with urllib.request.urlopen(req, timeout=15) as response:
            content = response.read().decode('utf-8', errors='ignore')

            # ç®€å•æå–æ­£æ–‡ï¼ˆå»é™¤ HTML æ ‡ç­¾ï¼‰
            content = re.sub(r'<script[^>]*>[\s\S]*?</script>', '', content)
            content = re.sub(r'<style[^>]*>[\s\S]*?</style>', '', content)
            content = re.sub(r'<[^>]+>', ' ', content)
            content = re.sub(r'\s+', ' ', content)
            return content[:15000]  # é™åˆ¶é•¿åº¦
    except Exception as e:
        print(f"  Fetch error: {e}")
        return ""


def search_with_glm(query: str, region: str = "ğŸ‡ºğŸ‡¸") -> list:
    """
    ä½¿ç”¨ GLM-4.7 çš„çŸ¥è¯†åº“æœç´¢ AI äº§å“

    GLM-4.7 æœ‰è¾ƒæ–°çš„çŸ¥è¯†ï¼Œå¯ä»¥ç›´æ¥è¯¢é—®æœ€æ–°çš„ AI äº§å“
    """
    client = get_zhipu_client()
    if not client:
        return []

    if region == "ğŸ‡¨ğŸ‡³":
        prompt = f"""è¯·åˆ—å‡ºæœ€è¿‘ï¼ˆ2024-2025å¹´ï¼‰{query}ç›¸å…³çš„ AI åˆ›ä¸šå…¬å¸/äº§å“ï¼Œç‰¹åˆ«æ˜¯ï¼š
- è·å¾—èèµ„çš„å…¬å¸
- æœ‰åˆ›æ–°äº§å“çš„å…¬å¸
- åœ¨è¡Œä¸šå†…æœ‰å½±å“åŠ›çš„å…¬å¸

è¿”å› JSON æ•°ç»„æ ¼å¼ï¼š
```json
[
  {{
    "name": "å…¬å¸/äº§å“å",
    "website": "å®˜ç½‘ï¼ˆå¦‚æœçŸ¥é“ï¼‰",
    "description": "ä¸€å¥è¯æè¿°",
    "funding": "èèµ„ä¿¡æ¯ï¼ˆå¦‚æœçŸ¥é“ï¼‰",
    "category": "åˆ†ç±»",
    "why_matters": "ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨"
  }}
]
```
åªè¿”å› JSONï¼Œè‡³å°‘è¿”å› 5 ä¸ªäº§å“ã€‚"""
    else:
        prompt = f"""List recent (2024-2025) AI startups/products related to {query}, especially:
- Companies that raised funding
- Companies with innovative products
- Influential companies in the industry

Return JSON array format:
```json
[
  {{
    "name": "Company/Product name",
    "website": "Website if known",
    "description": "One sentence description",
    "funding": "Funding info if known",
    "category": "Category",
    "why_matters": "Why it matters"
  }}
]
```
Return JSON only, at least 5 products."""

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=4096
        )

        result_text = response.choices[0].message.content

        # æå– JSON
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
        if json_match:
            return json.loads(json_match.group(1))
        try:
            return json.loads(result_text)
        except:
            return []

    except Exception as e:
        print(f"  GLM Search Error: {e}")
        return []
    finally:
        time.sleep(API_RATE_LIMIT_DELAY)


def fetch_with_glm(source_config: dict, limit: int = 10) -> list:
    """
    ä½¿ç”¨ GLM-4.7 ä»æ¸ é“å‘ç°äº§å“

    ç­–ç•¥ï¼š
    1. å…ˆå°è¯•æŠ“å–ç½‘é¡µ
    2. å¦‚æœç½‘é¡µå†…å®¹ä¸è¶³ï¼Œä½¿ç”¨ GLM çŸ¥è¯†åº“æœç´¢
    3. ç”¨ GLM è¯„åˆ†
    """
    source_name = source_config['name']
    region = source_config['region']
    url = source_config.get('url', '')
    keywords = source_config.get('keywords', [])

    print(f"  Fetching: {url}")

    # æŠ“å–ç½‘é¡µå†…å®¹
    content = fetch_url_content(url)
    products = []

    if content and len(content) > 500:
        print(f"  Analyzing page content with GLM-4.7...")
        products = analyze_with_glm(content, task="extract")
        if not isinstance(products, list):
            products = []

    # å¦‚æœç½‘é¡µæå–å¤±è´¥ï¼Œä½¿ç”¨ GLM çŸ¥è¯†åº“æœç´¢
    if len(products) < 3:
        print(f"  Page content insufficient, using GLM knowledge search...")
        search_query = ' '.join(keywords[:3]) if keywords else source_name
        products = search_with_glm(search_query, region)
        if not isinstance(products, list):
            products = []

    print(f"  Found {len(products)} potential products")

    # è¡¥å……ä¿¡æ¯å¹¶è¯„åˆ†
    result = []
    for p in products[:limit]:
        # æ·»åŠ æ¥æºä¿¡æ¯
        p['source'] = source_name
        p['region'] = region
        p['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')

        # ç”¨ GLM è¯„åˆ†
        score_result = analyze_with_glm(p, task="score")
        if score_result:
            p['dark_horse_index'] = score_result.get('score', 2)
            if 'reason' in score_result:
                p['score_reason'] = score_result['reason']

        result.append(p)

    return result


def analyze_and_score(product: dict) -> dict:
    """
    ä½¿ç”¨ AI åˆ†æäº§å“å¹¶è¯„åˆ†

    è¯„åˆ†æ ‡å‡†ï¼š
    - 5åˆ†: èèµ„ >$100M æˆ– é¡¶çº§åˆ›å§‹äºº æˆ– å“ç±»å¼€åˆ›è€…
    - 4åˆ†: èèµ„ >$30M æˆ– YC/é¡¶çº§VC
    - 3åˆ†: èèµ„ >$5M æˆ– ProductHunt Top 5
    - 2åˆ†: æœ‰æ½œåŠ›ä½†æ•°æ®ä¸è¶³
    - 1åˆ†: è¾¹ç¼˜
    """
    funding = product.get('funding_total', '')
    source = product.get('source', '')

    # ç®€å•çš„è§„åˆ™è¯„åˆ†ï¼ˆå¯ä»¥æ›¿æ¢ä¸º AI è¯„åˆ†ï¼‰
    score = 2  # é»˜è®¤

    # è§£æèèµ„é‡‘é¢
    funding_amount = 0
    if funding:
        match = re.search(r'\$?([\d.]+)\s*([BMK])?', funding, re.I)
        if match:
            amount = float(match.group(1))
            unit = (match.group(2) or '').upper()
            if unit == 'B':
                funding_amount = amount * 1000
            elif unit == 'M':
                funding_amount = amount
            elif unit == 'K':
                funding_amount = amount / 1000
            else:
                funding_amount = amount

    # è¯„åˆ†é€»è¾‘
    if funding_amount >= 100:
        score = 5
    elif funding_amount >= 30:
        score = 4
    elif funding_amount >= 5:
        score = 3
    elif source in ['Y Combinator', 'ProductHunt']:
        score = 3

    product['dark_horse_index'] = score
    return product


def save_product(product: dict, dry_run: bool = False):
    """ä¿å­˜äº§å“åˆ°ç›¸åº”ç›®å½•"""
    score = product.get('dark_horse_index', 2)
    week = get_current_week()

    if score >= 4:
        # é»‘é©¬
        target_dir = DARK_HORSES_DIR
        target_file = os.path.join(target_dir, f'week_{week}.json')
    else:
        # æ½œåŠ›è‚¡
        target_dir = RISING_STARS_DIR
        target_file = os.path.join(target_dir, f'global_{week}.json')

    if dry_run:
        print(f"  [DRY RUN] Would save to: {target_file}")
        print(f"  {json.dumps(product, ensure_ascii=False, indent=2)}")
        return

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(target_dir, exist_ok=True)

    # åŠ è½½ç°æœ‰æ•°æ®
    if os.path.exists(target_file):
        with open(target_file, 'r', encoding='utf-8') as f:
            products = json.load(f)
    else:
        products = []

    # æ·»åŠ æ–°äº§å“
    products.append(product)

    # ä¿å­˜
    with open(target_file, 'w', encoding='utf-8') as f:
        json.dump(products, f, ensure_ascii=False, indent=2)

    print(f"  Saved to: {target_file}")


def discover_from_source(source_key: str, dry_run: bool = False):
    """ä»å•ä¸ªæ¸ é“å‘ç°äº§å“"""
    if source_key not in SOURCES:
        print(f"Unknown source: {source_key}")
        return

    config = SOURCES[source_key]
    print(f"\n{'='*50}")
    print(f"  Discovering from: {config['name']} {config['region']}")
    print(f"{'='*50}")

    existing = load_existing_products()

    # ä½¿ç”¨ GLM-4.7 å‘ç°äº§å“
    products = fetch_with_glm(config)

    new_count = 0
    for product in products:
        if is_duplicate(product.get('name', ''), product.get('website', ''), existing):
            print(f"  Skip duplicate: {product.get('name')}")
            continue

        # å¦‚æœ GLM æ²¡æœ‰è¯„åˆ†ï¼Œä½¿ç”¨è§„åˆ™è¯„åˆ†
        if 'dark_horse_index' not in product:
            product = analyze_and_score(product)

        save_product(product, dry_run)
        new_count += 1
        existing.add(product.get('name', '').lower())

    print(f"\n  Found {new_count} new products from {config['name']}")


def discover_all(dry_run: bool = False, tier: int = None):
    """ä»æ‰€æœ‰æ¸ é“å‘ç°äº§å“"""
    for source_key, config in SOURCES.items():
        if tier and config.get('tier', 1) > tier:
            continue
        discover_from_source(source_key, dry_run)


# ============================================
# æ–°å¢ï¼šåŸºäºåœ°åŒºçš„ Web Search å‘ç°
# ============================================

def discover_by_region(region_key: str, dry_run: bool = False) -> dict:
    """
    ä½¿ç”¨ Web Search MCP æŒ‰åœ°åŒºå‘ç° AI äº§å“ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦è´¨é‡è¿‡æ»¤å’Œå…³é”®è¯è½®æ¢ï¼‰

    Args:
        region_key: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        dry_run: é¢„è§ˆæ¨¡å¼

    Returns:
        ç»Ÿè®¡ä¿¡æ¯
    """
    if region_key not in REGION_CONFIG:
        print(f"âŒ Unknown region: {region_key}")
        print(f"   Available: {', '.join(REGION_CONFIG.keys())}")
        return {"error": f"Unknown region: {region_key}"}

    config = REGION_CONFIG[region_key]
    region_name = config['name']
    search_engine = config['search_engine']

    # ä½¿ç”¨å…³é”®è¯è½®æ¢
    keywords = get_keywords_for_today(region_key)

    # Get provider for this region
    provider = get_provider_for_region(region_key)

    print(f"\n{'='*60}")
    print(f"  ğŸŒ Discovering AI Products: {region_name}")
    print(f"  ğŸ“¡ Search Engine: {search_engine}")
    print(f"  ğŸ¤– Provider: {provider}")
    print(f"  ğŸ”‘ Keywords: {len(keywords)} queries (day {datetime.now().weekday()})")
    print(f"{'='*60}")

    existing_names = load_existing_products()
    existing_domains = load_existing_domains()
    all_products = []
    quality_rejections = []

    stats = {
        "region": region_key,
        "region_name": region_name,
        "search_results": 0,
        "products_found": 0,
        "products_saved": 0,
        "dark_horses": 0,
        "rising_stars": 0,
        "duplicates_skipped": 0,
        "quality_rejections": 0,
    }

    # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
    for i, keyword in enumerate(keywords, 1):
        print(f"\n  [{i}/{len(keywords)}] Searching: {keyword[:50]}...")

        # 1. Search using provider routing
        search_results = search_with_provider(keyword, region_key, search_engine)
        stats["search_results"] += len(search_results)

        if not search_results:
            print(f"    âš ï¸ No results, using GLM knowledge...")
            search_results = search_with_glm(keyword, region_name)

        # å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºæ–‡æœ¬
        search_text = "\n\n".join([
            f"### {r.get('title', 'No Title')}\n"
            f"URL: {r.get('url', 'N/A')}\n"
            f"{r.get('content', r.get('snippet', ''))}"
            for r in search_results
        ])

        if not search_text.strip():
            continue

        # 2. Extract products using provider routing
        print(f"    ğŸ“Š Extracting products with {provider}...")

        region_flag_map = {
            'us': 'ğŸ‡ºğŸ‡¸', 'cn': 'ğŸ‡¨ğŸ‡³', 'eu': 'ğŸ‡ªğŸ‡º',
            'jp': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·', 'kr': 'ğŸ‡°ğŸ‡·', 'sea': 'ğŸ‡¸ğŸ‡¬'
        }
        region_flag = region_flag_map.get(region_key, 'ğŸŒ')

        products = analyze_with_provider(search_text, "extract", region_key, region_flag)

        if not isinstance(products, list):
            products = []

        print(f"    âœ… Extracted {len(products)} products")
        stats["products_found"] += len(products)

        # 3. å¯¹æ¯ä¸ªäº§å“è¯„åˆ†
        for product in products:
            name = product.get('name', '')
            if not name:
                continue

            # åŸŸåå»é‡
            domain = normalize_url(product.get('website', ''))
            if domain and domain in existing_domains:
                stats["duplicates_skipped"] += 1
                print(f"    â­ï¸ Skip duplicate domain: {domain}")
                continue

            # åç§°å»é‡
            if is_duplicate(name, product.get('website', ''), existing_names):
                stats["duplicates_skipped"] += 1
                print(f"    â­ï¸ Skip duplicate name: {name}")
                continue

            # è´¨é‡éªŒè¯
            is_valid, reason = validate_product(product)
            if not is_valid:
                stats["quality_rejections"] += 1
                quality_rejections.append({"name": name, "reason": reason})
                print(f"    âŒ Quality fail: {name} ({reason})")
                continue

            # è¡¥å……ä¿¡æ¯
            product['region'] = region_flag
            product['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')
            product['discovery_method'] = f'{provider}_search'
            product['search_keyword'] = keyword

            # 4. ä½¿ç”¨åˆå¹¶ prompt çš„è¯„åˆ†ï¼ˆæ— éœ€é¢å¤– API è°ƒç”¨ï¼‰
            # å¦‚æœæå–ç»“æœå·²åŒ…å« dark_horse_indexï¼Œç›´æ¥ä½¿ç”¨
            # å¦åˆ™ä½¿ç”¨è§„åˆ™è¯„åˆ†ä½œä¸º fallback
            score = product.get('dark_horse_index')
            if score is None:
                print(f"    ğŸ¯ Fallback scoring: {product.get('name')}...")
                product = analyze_and_score(product)
                score = product.get('dark_horse_index', 2)

            criteria = product.get('criteria_met', [])
            print(f"    ğŸ“ˆ Score: {score}/5 | Criteria: {criteria}")

            # 5. ä¿å­˜äº§å“
            save_product(product, dry_run)
            stats["products_saved"] += 1

            if score >= 4:
                stats["dark_horses"] += 1
            else:
                stats["rising_stars"] += 1

            existing_names.add(name.lower())
            if domain:
                existing_domains.add(domain)
            all_products.append(product)

    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"  ğŸ“Š Summary for {region_name}")
    print(f"{'='*60}")
    print(f"  Search Results: {stats['search_results']}")
    print(f"  Products Found: {stats['products_found']}")
    print(f"  Products Saved: {stats['products_saved']}")
    print(f"  ğŸ‡ Dark Horses (4-5): {stats['dark_horses']}")
    print(f"  â­ Rising Stars (2-3): {stats['rising_stars']}")
    print(f"  Duplicates Skipped: {stats['duplicates_skipped']}")
    print(f"  Quality Rejections: {stats['quality_rejections']}")

    if quality_rejections:
        print(f"\n  Top rejection reasons:")
        reason_counts = {}
        for rej in quality_rejections:
            reason = rej['reason']
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
        for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1])[:3]:
            print(f"    - {reason}: {count}")

    return stats


def discover_all_regions(dry_run: bool = False) -> dict:
    """
    å¸¦é…é¢ç³»ç»Ÿçš„å…¨çƒ AI äº§å“å‘ç°

    ç›®æ ‡é…é¢ï¼š
    - é»‘é©¬ (4-5åˆ†): 5 ä¸ª/å¤©
    - æ½œåŠ›è‚¡ (2-3åˆ†): 10 ä¸ª/å¤©

    Returns:
        è¯¦ç»†çš„å‘ç°æŠ¥å‘Š
    """
    start_time = datetime.now()
    today_str = start_time.strftime('%Y-%m-%d')

    print("\n" + "â•"*70)
    print(f"  ğŸŒ Daily AI Product Discovery - {today_str}")
    print("â•"*70)
    print(f"  ğŸ“Š Quota: {DAILY_QUOTA['dark_horses']} Dark Horses + {DAILY_QUOTA['rising_stars']} Rising Stars")
    print(f"  ğŸ”„ Max Attempts: {MAX_ATTEMPTS} rounds")
    print(f"  ğŸ“… Keyword Pool: Day {datetime.now().weekday()} (0=Mon)")
    print(f"  ğŸ¤– Perplexity: {'enabled' if USE_PERPLEXITY else 'disabled'}")
    print("â•"*70)

    # åˆå§‹åŒ–è·Ÿè¸ª
    found = {"dark_horses": 0, "rising_stars": 0}
    region_yield = {k: 0 for k in REGION_CONFIG.keys()}
    provider_stats = {"glm": 0, "perplexity": 0}  # Track provider usage
    unique_domains = set()
    duplicates_skipped = 0
    quality_rejections = []
    attempts = 0

    # åŠ è½½å·²å­˜åœ¨çš„åŸŸå
    existing_domains = load_existing_domains()
    existing_names = load_existing_products()

    def quotas_met():
        return (found["dark_horses"] >= DAILY_QUOTA["dark_horses"] and
                found["rising_stars"] >= DAILY_QUOTA["rising_stars"])

    def get_category(score):
        return "dark_horses" if score >= 4 else "rising_stars"

    # ä¸»å‘ç°å¾ªç¯
    while not quotas_met() and attempts < MAX_ATTEMPTS:
        attempts += 1
        print(f"\n{'â”€'*70}")
        print(f"  ğŸ”„ Round {attempts}/{MAX_ATTEMPTS}")
        print(f"  Progress: DH {found['dark_horses']}/{DAILY_QUOTA['dark_horses']} | RS {found['rising_stars']}/{DAILY_QUOTA['rising_stars']}")
        print(f"{'â”€'*70}")

        # éšæœºåŒ–åœ°åŒºé¡ºåº
        region_order = get_region_order()

        for region_key in region_order:
            # æ£€æŸ¥åœ°åŒºé…é¢
            if region_yield[region_key] >= REGION_MAX.get(region_key, 3):
                print(f"\n  â­ï¸ Skip {region_key}: region max reached ({region_yield[region_key]})")
                continue

            # æ£€æŸ¥å…¨å±€é…é¢
            if quotas_met():
                break

            config = REGION_CONFIG[region_key]
            region_name = config['name']
            search_engine = config['search_engine']

            # è·å–ä»Šæ—¥å…³é”®è¯ï¼ˆå¸¦è½®æ¢ï¼‰
            keywords = get_keywords_for_today(region_key)
            # æ¯è½®åªå–éƒ¨åˆ†å…³é”®è¯ï¼Œé¿å…é‡å¤
            keywords_this_round = keywords[:2] if attempts > 1 else keywords

            # Get provider for this region
            provider = get_provider_for_region(region_key)
            print(f"\n  ğŸ“ {region_name} | Provider: {provider} | Keywords: {len(keywords_this_round)}")

            # è®¡ç®—å‰©ä½™é…é¢ï¼ˆä¼ ç»™ promptï¼‰
            quota_remaining = {
                "dark_horses": DAILY_QUOTA["dark_horses"] - found["dark_horses"],
                "rising_stars": DAILY_QUOTA["rising_stars"] - found["rising_stars"],
            }

            for keyword in keywords_this_round:
                if quotas_met():
                    break

                print(f"\n    ğŸ” Searching: {keyword[:50]}...")

                # 1. Search using provider routing
                search_results = search_with_provider(keyword, region_key, search_engine)

                if not search_results:
                    search_results = search_with_glm(keyword, region_name)

                if not search_results:
                    continue

                # æ ¼å¼åŒ–æœç´¢ç»“æœ
                search_text = "\n\n".join([
                    f"### {r.get('title', 'No Title')}\n"
                    f"URL: {r.get('url', 'N/A')}\n"
                    f"{r.get('content', r.get('snippet', ''))}"
                    for r in search_results
                ])

                if not search_text.strip():
                    continue

                # 2. Extract products using provider routing
                region_flag_map = {
                    'us': 'ğŸ‡ºğŸ‡¸', 'cn': 'ğŸ‡¨ğŸ‡³', 'eu': 'ğŸ‡ªğŸ‡º',
                    'jp': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·', 'kr': 'ğŸ‡°ğŸ‡·', 'sea': 'ğŸ‡¸ğŸ‡¬'
                }
                region_flag = region_flag_map.get(region_key, 'ğŸŒ')

                products = analyze_with_provider(
                    search_text,
                    "extract",
                    region_key,
                    region_flag,
                    quota_remaining
                )

                if not isinstance(products, list):
                    products = []

                print(f"    ğŸ“¦ Extracted: {len(products)} candidates")

                # 3. å¤„ç†æ¯ä¸ªäº§å“
                for product in products:
                    if quotas_met():
                        break

                    name = product.get('name', '')
                    if not name:
                        continue

                    # åŸŸåå»é‡
                    domain = normalize_url(product.get('website', ''))
                    if domain in existing_domains or domain in unique_domains:
                        duplicates_skipped += 1
                        print(f"    â­ï¸ Dup domain: {domain}")
                        continue

                    # åç§°å»é‡
                    if is_duplicate(name, product.get('website', ''), existing_names):
                        duplicates_skipped += 1
                        print(f"    â­ï¸ Dup name: {name}")
                        continue

                    # è´¨é‡éªŒè¯
                    is_valid, reason = validate_product(product)
                    if not is_valid:
                        quality_rejections.append({"name": name, "reason": reason})
                        print(f"    âŒ Quality fail: {name} ({reason})")
                        continue

                    # è¡¥å……å…ƒä¿¡æ¯
                    product['region'] = region_flag
                    product['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')
                    product['discovery_method'] = f'{provider}_search'
                    product['search_keyword'] = keyword

                    # ä½¿ç”¨åˆå¹¶ prompt çš„è¯„åˆ†ï¼ˆæ— éœ€é¢å¤– API è°ƒç”¨ï¼‰
                    # å¦‚æœæå–ç»“æœå·²åŒ…å« dark_horse_indexï¼Œç›´æ¥ä½¿ç”¨
                    # å¦åˆ™ä½¿ç”¨è§„åˆ™è¯„åˆ†ä½œä¸º fallback
                    score = product.get('dark_horse_index')
                    if score is None:
                        product = analyze_and_score(product)
                        score = product.get('dark_horse_index', 2)

                    category = get_category(score)

                    # æ£€æŸ¥åˆ†ç±»é…é¢
                    if found[category] >= DAILY_QUOTA[category]:
                        print(f"    â­ï¸ {category} quota full, skip: {name}")
                        continue

                    # æ£€æŸ¥åœ°åŒºé…é¢
                    if region_yield[region_key] >= REGION_MAX.get(region_key, 3):
                        print(f"    â­ï¸ Region max reached, skip: {name}")
                        continue

                    # ä¿å­˜
                    save_product(product, dry_run)

                    # æ›´æ–°è®¡æ•°
                    found[category] += 1
                    region_yield[region_key] += 1
                    provider_stats[provider] += 1  # Track provider usage
                    unique_domains.add(domain)
                    existing_names.add(name.lower())

                    status_icon = "ğŸ¦„" if category == "dark_horses" else "â­"
                    print(f"    {status_icon} SAVED: {name} (score={score}, {category}, {provider})")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    dh_status = "âœ…" if found["dark_horses"] >= DAILY_QUOTA["dark_horses"] else "âš ï¸"
    rs_status = "âœ…" if found["rising_stars"] >= DAILY_QUOTA["rising_stars"] else "âš ï¸"

    print("\n" + "â•"*70)
    print(f"  Daily Discovery Report - {today_str}")
    print("â•"*70)
    print(f"  Quotas:     Dark Horses: {found['dark_horses']}/{DAILY_QUOTA['dark_horses']} {dh_status}  Rising Stars: {found['rising_stars']}/{DAILY_QUOTA['rising_stars']} {rs_status}")
    print(f"  Attempts:   {attempts} rounds")
    print(f"  Duration:   {duration:.1f} seconds")
    print(f"  Regions:    {', '.join(f'{k}: {v}' for k, v in region_yield.items() if v > 0)}")
    print(f"  Providers:  {', '.join(f'{k}: {v}' for k, v in provider_stats.items() if v > 0)}")
    print(f"  Unique domains found: {len(unique_domains)}")
    print(f"  Duplicates skipped: {duplicates_skipped}")
    print(f"  Quality rejections: {len(quality_rejections)}")

    if quality_rejections:
        print("\n  Quality rejection reasons:")
        reason_counts = {}
        for rej in quality_rejections:
            reason = rej['reason']
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
        for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1])[:5]:
            print(f"    - {reason}: {count}")

    print("â•"*70)

    # è¿”å›æŠ¥å‘Šæ•°æ®
    return {
        "date": today_str,
        "found": found,
        "quota": DAILY_QUOTA,
        "attempts": attempts,
        "region_yield": region_yield,
        "provider_stats": provider_stats,
        "unique_domains": len(unique_domains),
        "duplicates_skipped": duplicates_skipped,
        "quality_rejections": len(quality_rejections),
        "duration_seconds": duration,
        "quotas_met": quotas_met(),
    }


def test_web_search():
    """æµ‹è¯• Web Search MCP è¿æ¥"""
    print("\n" + "="*60)
    print("  ğŸ” Testing Web Search MCP")
    print("="*60)

    test_queries = [
        ("bing", "AI startup funding 2026"),
        ("sogou", "AIåˆ›ä¸šå…¬å¸ èèµ„ 2026"),
    ]

    for engine, query in test_queries:
        print(f"\n  Testing: {engine} - {query}")
        results = web_search_mcp(query, engine, count=3)

        if results:
            print(f"  âœ… Success! Found {len(results)} results")
            for i, r in enumerate(results[:2], 1):
                print(f"    {i}. {r.get('title', 'No Title')[:50]}...")
        else:
            print(f"  âš ï¸ No results (may fallback to GLM knowledge)")


def setup_schedule():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆmacOS/Linuxï¼‰"""
    script_path = os.path.abspath(__file__)

    # ç”Ÿæˆ cron ä»»åŠ¡
    cron_line = f"0 9 * * * cd {PROJECT_ROOT} && /usr/bin/python3 {script_path} >> /tmp/auto_discover.log 2>&1"

    print("\nè®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©æ—©ä¸Š9ç‚¹è¿è¡Œï¼‰ï¼š")
    print("-" * 50)
    print("è¿è¡Œä»¥ä¸‹å‘½ä»¤æ·»åŠ  cron ä»»åŠ¡ï¼š")
    print(f"\n  (crontab -l 2>/dev/null; echo \"{cron_line}\") | crontab -")
    print("\næˆ–è€…ä½¿ç”¨ launchd (macOS)ï¼š")
    print(f"  åˆ›å»º ~/Library/LaunchAgents/com.weeklyai.autodiscover.plist")


def test_glm_connection():
    """æµ‹è¯• GLM-4.7 è¿æ¥"""
    print("\næµ‹è¯• GLM-4.7 è¿æ¥...")
    print(f"  API Key: {ZHIPU_API_KEY[:20]}...")
    print(f"  Model: {ZHIPU_MODEL}")

    client = get_zhipu_client()
    if not client:
        print("  âŒ æ— æ³•åˆ›å»ºå®¢æˆ·ç«¯ï¼Œè¯·å®‰è£…: pip install zhipuai")
        return False

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±"}],
            max_tokens=100
        )
        result = response.choices[0].message.content
        print(f"  âœ… è¿æ¥æˆåŠŸ!")
        print(f"  GLM å›å¤: {result}")
        return True
    except Exception as e:
        print(f"  âŒ è¿æ¥å¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description='è‡ªåŠ¨å‘ç°å…¨çƒ AI äº§å“ (v2.0 - Web Search MCP)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  # æŒ‰åœ°åŒºæœç´¢ï¼ˆæ¨èï¼Œä½¿ç”¨ Web Search MCPï¼‰
  python tools/auto_discover.py --region us      # æœç´¢ç¾å›½ AI äº§å“
  python tools/auto_discover.py --region cn      # æœç´¢ä¸­å›½ AI äº§å“
  python tools/auto_discover.py --region eu      # æœç´¢æ¬§æ´² AI äº§å“
  python tools/auto_discover.py --region jp      # æœç´¢æ—¥éŸ© AI äº§å“
  python tools/auto_discover.py --region sea     # æœç´¢ä¸œå—äºš AI äº§å“
  python tools/auto_discover.py --region all     # æœç´¢æ‰€æœ‰åœ°åŒº

  # æŒ‰æ¸ é“æœç´¢ï¼ˆæ—§æ–¹å¼ï¼‰
  python tools/auto_discover.py --source 36kr    # ä» 36æ°ª å‘ç°
  python tools/auto_discover.py --source producthunt

  # å…¶ä»–é€‰é¡¹
  python tools/auto_discover.py --dry-run        # é¢„è§ˆä¸ä¿å­˜
  python tools/auto_discover.py --test           # æµ‹è¯• GLM è¿æ¥
  python tools/auto_discover.py --test-search    # æµ‹è¯• Web Search MCP
"""
    )

    # æ–°å¢ï¼šåœ°åŒºå‚æ•°
    parser.add_argument('--region', '-r',
                        choices=['us', 'cn', 'eu', 'jp', 'sea', 'all'],
                        help='æŒ‰åœ°åŒºæœç´¢ (us/cn/eu/jp/sea/all)')

    # åŸæœ‰å‚æ•°
    parser.add_argument('--source', '-s', help='æŒ‡å®šæ¸ é“ (e.g., 36kr, producthunt)')
    parser.add_argument('--tier', '-t', type=int, choices=[1, 2, 3], help='åªè¿è¡ŒæŒ‡å®šçº§åˆ«çš„æ¸ é“')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¿å­˜')
    parser.add_argument('--schedule', action='store_true', help='è®¾ç½®å®šæ—¶ä»»åŠ¡')
    parser.add_argument('--list-sources', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æ¸ é“')
    parser.add_argument('--list-regions', action='store_true', help='åˆ—å‡ºæ‰€æœ‰åœ°åŒº')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯• GLM-4.7 è¿æ¥')
    parser.add_argument('--test-search', action='store_true', help='æµ‹è¯• Web Search MCP')

    args = parser.parse_args()

    # æµ‹è¯•åŠŸèƒ½
    if args.test:
        test_glm_connection()
        return

    if args.test_search:
        test_web_search()
        return

    # åˆ—è¡¨åŠŸèƒ½
    if args.list_sources:
        print("\nå¯ç”¨æ¸ é“ï¼š")
        print("-" * 60)
        for key, config in SOURCES.items():
            print(f"  {key:15} {config['region']} {config['name']:20} Tier {config.get('tier', 1)}")
        return

    if args.list_regions:
        print("\nå¯ç”¨åœ°åŒºï¼š")
        print("-" * 60)
        for key, config in REGION_CONFIG.items():
            print(f"  {key:5} {config['name']:15} æƒé‡:{config['weight']:2}% æœç´¢å¼•æ“:{config['search_engine']}")
        return

    if args.schedule:
        setup_schedule()
        return

    # å‘ç°åŠŸèƒ½
    if args.region:
        # æ–°æ–¹å¼ï¼šæŒ‰åœ°åŒºæœç´¢
        if args.region == 'all':
            discover_all_regions(args.dry_run)
        else:
            discover_by_region(args.region, args.dry_run)
    elif args.source:
        # æ—§æ–¹å¼ï¼šæŒ‰æ¸ é“æœç´¢
        discover_from_source(args.source, args.dry_run)
    else:
        # é»˜è®¤ï¼šè¿è¡Œæ‰€æœ‰åœ°åŒºçš„ Web Search
        print("\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ --region å‚æ•°è¿›è¡Œåœ°åŒºæœç´¢ï¼ˆæ¨èï¼‰")
        print("   ç¤ºä¾‹: python tools/auto_discover.py --region us")
        print("   æˆ–è€…: python tools/auto_discover.py --region all")
        print("\n   ä½¿ç”¨ --source å‚æ•°è¿›è¡Œæ—§æ¸ é“æœç´¢")
        print("   ç¤ºä¾‹: python tools/auto_discover.py --source 36kr")
        print("\nè¿è¡Œ --help æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹")


if __name__ == '__main__':
    main()
