#!/usr/bin/env python3
"""
è‡ªåŠ¨å‘ç°å…¨çƒ AI äº§å“ (v2.0 - é›†æˆ Web Search MCP)

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Zhipu Web Search MCP å®æ—¶æœç´¢å…¨çƒ AI äº§å“
2. æŒ‰åœ°åŒºåˆ†é…æœç´¢ä»»åŠ¡ (ç¾å›½40%/ä¸­å›½25%/æ¬§æ´²15%/æ—¥éŸ©10%/ä¸œå—äºš10%)
3. ä½¿ç”¨ä¸“ä¸š Prompt æå–äº§å“ä¿¡æ¯å¹¶è¯„åˆ†
4. è‡ªåŠ¨åˆ†ç±»åˆ°é»‘é©¬(4-5åˆ†)/æ½œåŠ›è‚¡(2-3åˆ†)

ç”¨æ³•ï¼š
    python tools/auto_discover.py                    # è¿è¡Œæ‰€æœ‰åœ°åŒº
    python tools/auto_discover.py --region us       # åªæœç´¢ç¾å›½
    python tools/auto_discover.py --region cn       # åªæœç´¢ä¸­å›½
    python tools/auto_discover.py --dry-run         # é¢„è§ˆä¸ä¿å­˜
    python tools/auto_discover.py --test-search     # æµ‹è¯• Web Search MCP
"""

import json
import os
import sys
import argparse
import re
import time
import requests
from datetime import datetime, timezone
from urllib.parse import urlparse
import subprocess

# æ™ºè°± AI é…ç½®
API_RATE_LIMIT_DELAY = 3  # æ¯æ¬¡ API è°ƒç”¨åç­‰å¾…ç§’æ•°
ZHIPU_API_KEY = os.environ.get('ZHIPU_API_KEY', '9c842f4999534eeba595b9fd142a699a.XXaPIGhbZTdzYIu8')
ZHIPU_MODEL = 'glm-4.7'

# Web Search MCP é…ç½®
WEB_SEARCH_MCP_URL = "https://open.bigmodel.cn/api/mcp/web_search/sse"
WEB_SEARCH_AUTH = ZHIPU_API_KEY

# ============================================
# åœ°åŒºé…ç½® (æŒ‰æ¯”ä¾‹åˆ†é…æœç´¢ä»»åŠ¡)
# ============================================
REGION_CONFIG = {
    'us': {
        'name': 'ğŸ‡ºğŸ‡¸ ç¾å›½',
        'weight': 40,  # 40%
        'search_engine': 'bing',
        'keywords': [
            'AI startup funding Series A B 2026',
            'artificial intelligence company raised funding',
            'YC AI startup demo day 2026',
            'AI unicorn startup valuation',
        ],
    },
    'cn': {
        'name': 'ğŸ‡¨ğŸ‡³ ä¸­å›½',
        'weight': 25,  # 25%
        'search_engine': 'sogou',
        'keywords': [
            'AIåˆ›ä¸šå…¬å¸ èèµ„ AIGC å¤§æ¨¡å‹ è·æŠ•',
            'äººå·¥æ™ºèƒ½ åˆåˆ›å…¬å¸ Aè½® Bè½® èèµ„',
            'å¤§æ¨¡å‹ åˆ›ä¸šå…¬å¸ ä¼°å€¼ èèµ„æ–°é—»',
            'AIGC ç‹¬è§’å…½ èèµ„ 2026',
        ],
    },
    'eu': {
        'name': 'ğŸ‡ªğŸ‡º æ¬§æ´²',
        'weight': 15,  # 15%
        'search_engine': 'bing',
        'keywords': [
            'European AI startup funding Sifted',
            'Europe artificial intelligence company raised',
            'UK France Germany AI startup Series A',
        ],
    },
    'jp': {
        'name': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡· æ—¥éŸ©',
        'weight': 10,  # 10%
        'search_engine': 'bing',
        'keywords': [
            'Japan Korea AI startup funding',
            'Japanese artificial intelligence company raised',
            'Korean AI startup investment',
        ],
    },
    'sea': {
        'name': 'ğŸ‡¸ğŸ‡¬ ä¸œå—äºš',
        'weight': 10,  # 10%
        'search_engine': 'bing',
        'keywords': [
            'Southeast Asia AI startup e27 funding',
            'Singapore Indonesia Vietnam AI company raised',
            'Tech in Asia artificial intelligence funding',
        ],
    },
}

# ============================================
# ä¸“ä¸š Prompts (éµå¾ª INSTRUCTIONS.md æ ‡å‡†)
# ============================================

# Prompt B: äº§å“æå–
PROMPT_PRODUCT_EXTRACTION = """ä½ æ˜¯ WeeklyAI çš„ AI äº§å“åˆ†æå¸ˆã€‚ä»æœç´¢ç»“æœä¸­æå– AI äº§å“ä¿¡æ¯ã€‚

## æœç´¢ç»“æœ
{search_results}

## å¿…é¡»æ’é™¤ï¼ˆä¸æ˜¯é»‘é©¬ï¼‰ï¼š
- âŒ å·²äººå°½çš†çŸ¥: ChatGPT, Midjourney, Cursor, Claude, Copilot, Gemini
- âŒ å¼€å‘åº“/æ¨¡å‹: HuggingFace models, LangChain, PyTorch, TensorFlow
- âŒ æ²¡æœ‰äº§å“: åªæœ‰è®ºæ–‡/demo/æ²¡å®˜ç½‘
- âŒ å¤§å‚äº§å“: Google Gemini, Meta Llama, OpenAI GPT

## ä¼˜å…ˆæ”¶å½•ï¼š
- âœ… èèµ„æ–°é—» (Series A/B, Seed, ä¼°å€¼)
- âœ… åˆ›å§‹äººèƒŒæ™¯äº®çœ¼ (å¤§å‚é«˜ç®¡å‡ºèµ°åˆ›ä¸š)
- âœ… å“ç±»åˆ›æ–° (å¼€åˆ›æ–°èµ›é“)
- âœ… ç¤¾åŒºçƒ­åº¦ (ProductHunt Top 5)

## è¿”å› JSON (åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹)
```json
[
  {{
    "name": "äº§å“å",
    "website": "https://å®˜ç½‘",
    "description": "ä¸€å¥è¯æè¿°ï¼ˆä¸­æ–‡ï¼‰",
    "category": "coding/image/video/voice/writing/hardware/finance/education/healthcare/other",
    "region": "{region}",
    "funding_total": "$50M Series A",
    "why_matters": "ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼ˆè¦å…·ä½“ï¼Œ2-3å¥è¯ï¼‰",
    "latest_news": "2026-01: å…·ä½“äº‹ä»¶",
    "source": "æ¥æºç½‘ç«™"
  }}
]
```

å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº§å“ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚è‡³å°‘æå– 3 ä¸ªäº§å“ï¼Œæœ€å¤š 10 ä¸ªã€‚"""

# Prompt C: é»‘é©¬è¯„åˆ†
PROMPT_DARK_HORSE_SCORING = """è¯„ä¼°äº§å“çš„"é»‘é©¬æŒ‡æ•°"(1-5åˆ†)ï¼š

## äº§å“
{product}

## è¯„åˆ†æ ‡å‡†
5åˆ†: èèµ„>$100M æˆ– é¡¶çº§åˆ›å§‹äººèƒŒæ™¯ æˆ– å“ç±»å¼€åˆ›è€… æˆ– ARR>$50M
4åˆ†: èèµ„>$30M æˆ– YC/a16zæŠ•èµ„ æˆ– ä¼°å€¼å¢é•¿>3x æˆ– ARR>$10M
3åˆ†: èèµ„$5M-$30M æˆ– ProductHunt Top 5 æˆ– æœ¬åœ°å¸‚åœºçƒ­åº¦é«˜
2åˆ†: æœ‰åˆ›æ–°ç‚¹ä½†æ•°æ®ä¸è¶³ æˆ– æ—©æœŸäº§å“æœ‰æ½œåŠ›
1åˆ†: è¾¹ç¼˜äº§å“ æˆ– å¾…éªŒè¯ æˆ– ä¿¡æ¯å¤ªå°‘

## è¿”å›æ ¼å¼ï¼ˆåªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰
```json
{{
  "dark_horse_index": 4,
  "reason": "è¯„åˆ†ç†ç”±ï¼ˆå…·ä½“è¯´æ˜ä¾æ®ï¼‰"
}}
```"""

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

# æ•°æ®æ–‡ä»¶è·¯å¾„
DARK_HORSES_DIR = os.path.join(PROJECT_ROOT, 'data', 'dark_horses')
RISING_STARS_DIR = os.path.join(PROJECT_ROOT, 'data', 'rising_stars')
CANDIDATES_DIR = os.path.join(PROJECT_ROOT, 'data', 'candidates')

# æ¸ é“é…ç½®
SOURCES = {
    # ç¾å›½æ¸ é“
    'techcrunch': {
        'name': 'TechCrunch',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://techcrunch.com/category/artificial-intelligence/',
        'rss': 'https://techcrunch.com/category/artificial-intelligence/feed/',
        'keywords': ['raises', 'Series A', 'Series B', 'funding', 'AI startup'],
        'tier': 1,
    },
    'producthunt': {
        'name': 'ProductHunt',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://www.producthunt.com/topics/artificial-intelligence',
        'api': 'https://api.producthunt.com/v2/api/graphql',
        'keywords': ['AI', 'machine learning', 'LLM'],
        'tier': 2,
    },
    'ycombinator': {
        'name': 'Y Combinator',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://www.ycombinator.com/companies?tags=AI',
        'keywords': ['YC', 'Demo Day'],
        'tier': 1,
    },

    # ä¸­å›½æ¸ é“
    '36kr': {
        'name': '36æ°ª',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://36kr.com/information/AI/',
        'rss': 'https://36kr.com/feed',
        'keywords': ['AIèèµ„', 'äººå·¥æ™ºèƒ½', 'AIGC', 'å¤§æ¨¡å‹', 'è·æŠ•'],
        'tier': 1,
    },
    'itjuzi': {
        'name': 'ITæ¡”å­',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://www.itjuzi.com/investevent',
        'keywords': ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ '],
        'tier': 1,
    },
    'jiqizhixin': {
        'name': 'æœºå™¨ä¹‹å¿ƒ',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://www.jiqizhixin.com/',
        'rss': 'https://www.jiqizhixin.com/rss',
        'keywords': ['AI', 'èèµ„', 'åˆ›ä¸š'],
        'tier': 2,
    },

    # æ¬§æ´²æ¸ é“
    'sifted': {
        'name': 'Sifted',
        'region': 'ğŸ‡ªğŸ‡º',
        'url': 'https://sifted.eu/sector/artificial-intelligence',
        'keywords': ['AI', 'funding', 'European startup'],
        'tier': 1,
    },
    'eu_startups': {
        'name': 'EU-Startups',
        'region': 'ğŸ‡ªğŸ‡º',
        'url': 'https://www.eu-startups.com/category/artificial-intelligence/',
        'rss': 'https://www.eu-startups.com/feed/',
        'keywords': ['AI', 'raises', 'funding'],
        'tier': 2,
    },

    # æ—¥éŸ©æ¸ é“
    'bridge': {
        'name': 'Bridge',
        'region': 'ğŸ‡¯ğŸ‡µ',
        'url': 'https://thebridge.jp/en/',
        'keywords': ['AI', 'startup', 'funding', 'Japan'],
        'tier': 1,
    },
    'platum': {
        'name': 'Platum',
        'region': 'ğŸ‡°ğŸ‡·',
        'url': 'https://platum.kr/archives/category/ai',
        'keywords': ['AI', 'startup', 'Korea'],
        'tier': 1,
    },

    # ä¸œå—äºšæ¸ é“
    'e27': {
        'name': 'e27',
        'region': 'ğŸ‡¸ğŸ‡¬',
        'url': 'https://e27.co/tag/artificial-intelligence/',
        'keywords': ['AI', 'Southeast Asia', 'funding'],
        'tier': 1,
    },
    'techinasia': {
        'name': 'Tech in Asia',
        'region': 'ğŸ‡¸ğŸ‡¬',
        'url': 'https://www.techinasia.com/tag/artificial-intelligence',
        'keywords': ['AI', 'Asia', 'startup'],
        'tier': 1,
    },
}


def get_current_week():
    """è·å–å½“å‰å‘¨æ•°"""
    now = datetime.now()
    return f"{now.year}_{now.isocalendar()[1]:02d}"


def load_existing_products():
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„äº§å“åç§°å’Œç½‘å€"""
    existing = set()

    # åŠ è½½é»‘é©¬
    if os.path.exists(DARK_HORSES_DIR):
        for f in os.listdir(DARK_HORSES_DIR):
            if f.endswith('.json'):
                with open(os.path.join(DARK_HORSES_DIR, f), 'r') as file:
                    products = json.load(file)
                    for p in products:
                        existing.add(p.get('name', '').lower())
                        existing.add(p.get('website', '').lower())

    # åŠ è½½æ½œåŠ›è‚¡
    if os.path.exists(RISING_STARS_DIR):
        for f in os.listdir(RISING_STARS_DIR):
            if f.endswith('.json'):
                with open(os.path.join(RISING_STARS_DIR, f), 'r') as file:
                    products = json.load(file)
                    for p in products:
                        existing.add(p.get('name', '').lower())
                        existing.add(p.get('website', '').lower())

    return existing


def is_duplicate(name: str, website: str, existing: set) -> bool:
    """æ£€æŸ¥æ˜¯å¦é‡å¤"""
    return name.lower() in existing or website.lower() in existing


def get_zhipu_client():
    """è·å–æ™ºè°± AI å®¢æˆ·ç«¯"""
    try:
        from zhipuai import ZhipuAI
        return ZhipuAI(api_key=ZHIPU_API_KEY)
    except ImportError:
        print("  Error: zhipuai SDK not installed. Run: pip install zhipuai")
        return None


def web_search_mcp(query: str, search_engine: str = "bing", count: int = 10) -> list:
    """
    ä½¿ç”¨ Zhipu Web Search MCP è¿›è¡Œå®æ—¶ç½‘ç»œæœç´¢

    Args:
        query: æœç´¢å…³é”®è¯
        search_engine: æœç´¢å¼•æ“ (bing/sogou/quark/jina)
        count: è¿”å›ç»“æœæ•°é‡

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ [{"title": "", "url": "", "content": ""}, ...]
    """
    # ä½¿ç”¨æ™ºè°± AI API è¿›è¡Œ web_search å·¥å…·è°ƒç”¨
    client = get_zhipu_client()
    if not client:
        return []

    try:
        print(f"  ğŸ” Web Search: {query[:50]}...")

        # ä½¿ç”¨ GLM-4.7 çš„ web_search å·¥å…·
        response = client.chat.completions.create(
            model="glm-4-plus",  # æ”¯æŒ web_search çš„æ¨¡å‹
            messages=[{
                "role": "user",
                "content": f"æœç´¢æœ€æ–°çš„ AI åˆ›ä¸šå…¬å¸èèµ„æ–°é—»: {query}"
            }],
            tools=[{
                "type": "web_search",
                "web_search": {
                    "enable": True,
                    "search_engine": search_engine,
                    "search_result": True
                }
            }],
            tool_choice="auto",
            max_tokens=4096
        )

        # æå–æœç´¢ç»“æœ
        results = []

        # æ£€æŸ¥æ˜¯å¦æœ‰ web_search ç»“æœ
        if hasattr(response, 'web_search') and response.web_search:
            for item in response.web_search:
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("link", item.get("url", "")),
                    "content": item.get("content", item.get("snippet", ""))
                })

        # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–ç»“æœï¼Œä»å›å¤ä¸­æå–
        if not results and response.choices:
            content = response.choices[0].message.content
            # è¿”å›åŸå§‹å†…å®¹ä¾›åç»­å¤„ç†
            results = [{"title": "Search Results", "url": "", "content": content}]

        print(f"  âœ… Found {len(results)} results")
        return results

    except Exception as e:
        print(f"  âŒ Web Search Error: {e}")
        # é™çº§ï¼šä½¿ç”¨ GLM çŸ¥è¯†åº“
        return []
    finally:
        time.sleep(API_RATE_LIMIT_DELAY)


def analyze_with_glm(content: str, task: str = "extract", region: str = "ğŸ‡ºğŸ‡¸") -> dict:
    """
    ä½¿ç”¨ GLM-4.7 åˆ†æå†…å®¹ (ä½¿ç”¨ä¸“ä¸š Prompt)

    Args:
        content: è¦åˆ†æçš„å†…å®¹ï¼ˆæœç´¢ç»“æœã€äº§å“ä¿¡æ¯ç­‰ï¼‰
        task: ä»»åŠ¡ç±»å‹ (extract/score/translate)
        region: åœ°åŒºæ ‡è¯†

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    client = get_zhipu_client()
    if not client:
        return {}

    if task == "extract":
        # ä½¿ç”¨ä¸“ä¸šçš„äº§å“æå– Prompt
        prompt = PROMPT_PRODUCT_EXTRACTION.format(
            search_results=content[:10000],
            region=region
        )

    elif task == "score":
        # ä½¿ç”¨ä¸“ä¸šçš„é»‘é©¬è¯„åˆ† Prompt
        prompt = PROMPT_DARK_HORSE_SCORING.format(
            product=json.dumps(content, ensure_ascii=False, indent=2)
        )

    elif task == "translate":
        prompt = f"""å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­ï¼š

{content}

åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4096
        )

        result_text = response.choices[0].message.content

        # æå– JSON
        if task in ["extract", "score"]:
            # å°è¯•æå– JSON
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
            if json_match:
                return json.loads(json_match.group(1))
            # å°è¯•ç›´æ¥è§£æ
            try:
                return json.loads(result_text)
            except:
                return {}
        else:
            return {"text": result_text}

    except Exception as e:
        print(f"  GLM Error: {e}")
        return {}
    finally:
        # é™æµï¼šæ¯æ¬¡ API è°ƒç”¨åç­‰å¾…
        time.sleep(API_RATE_LIMIT_DELAY)


def fetch_url_content(url: str) -> str:
    """æŠ“å– URL å†…å®¹"""
    try:
        import urllib.request
        req = urllib.request.Request(
            url,
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        )
        with urllib.request.urlopen(req, timeout=15) as response:
            content = response.read().decode('utf-8', errors='ignore')

            # ç®€å•æå–æ­£æ–‡ï¼ˆå»é™¤ HTML æ ‡ç­¾ï¼‰
            content = re.sub(r'<script[^>]*>[\s\S]*?</script>', '', content)
            content = re.sub(r'<style[^>]*>[\s\S]*?</style>', '', content)
            content = re.sub(r'<[^>]+>', ' ', content)
            content = re.sub(r'\s+', ' ', content)
            return content[:15000]  # é™åˆ¶é•¿åº¦
    except Exception as e:
        print(f"  Fetch error: {e}")
        return ""


def search_with_glm(query: str, region: str = "ğŸ‡ºğŸ‡¸") -> list:
    """
    ä½¿ç”¨ GLM-4.7 çš„çŸ¥è¯†åº“æœç´¢ AI äº§å“

    GLM-4.7 æœ‰è¾ƒæ–°çš„çŸ¥è¯†ï¼Œå¯ä»¥ç›´æ¥è¯¢é—®æœ€æ–°çš„ AI äº§å“
    """
    client = get_zhipu_client()
    if not client:
        return []

    if region == "ğŸ‡¨ğŸ‡³":
        prompt = f"""è¯·åˆ—å‡ºæœ€è¿‘ï¼ˆ2024-2025å¹´ï¼‰{query}ç›¸å…³çš„ AI åˆ›ä¸šå…¬å¸/äº§å“ï¼Œç‰¹åˆ«æ˜¯ï¼š
- è·å¾—èèµ„çš„å…¬å¸
- æœ‰åˆ›æ–°äº§å“çš„å…¬å¸
- åœ¨è¡Œä¸šå†…æœ‰å½±å“åŠ›çš„å…¬å¸

è¿”å› JSON æ•°ç»„æ ¼å¼ï¼š
```json
[
  {{
    "name": "å…¬å¸/äº§å“å",
    "website": "å®˜ç½‘ï¼ˆå¦‚æœçŸ¥é“ï¼‰",
    "description": "ä¸€å¥è¯æè¿°",
    "funding": "èèµ„ä¿¡æ¯ï¼ˆå¦‚æœçŸ¥é“ï¼‰",
    "category": "åˆ†ç±»",
    "why_matters": "ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨"
  }}
]
```
åªè¿”å› JSONï¼Œè‡³å°‘è¿”å› 5 ä¸ªäº§å“ã€‚"""
    else:
        prompt = f"""List recent (2024-2025) AI startups/products related to {query}, especially:
- Companies that raised funding
- Companies with innovative products
- Influential companies in the industry

Return JSON array format:
```json
[
  {{
    "name": "Company/Product name",
    "website": "Website if known",
    "description": "One sentence description",
    "funding": "Funding info if known",
    "category": "Category",
    "why_matters": "Why it matters"
  }}
]
```
Return JSON only, at least 5 products."""

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=4096
        )

        result_text = response.choices[0].message.content

        # æå– JSON
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
        if json_match:
            return json.loads(json_match.group(1))
        try:
            return json.loads(result_text)
        except:
            return []

    except Exception as e:
        print(f"  GLM Search Error: {e}")
        return []
    finally:
        time.sleep(API_RATE_LIMIT_DELAY)


def fetch_with_glm(source_config: dict, limit: int = 10) -> list:
    """
    ä½¿ç”¨ GLM-4.7 ä»æ¸ é“å‘ç°äº§å“

    ç­–ç•¥ï¼š
    1. å…ˆå°è¯•æŠ“å–ç½‘é¡µ
    2. å¦‚æœç½‘é¡µå†…å®¹ä¸è¶³ï¼Œä½¿ç”¨ GLM çŸ¥è¯†åº“æœç´¢
    3. ç”¨ GLM è¯„åˆ†
    """
    source_name = source_config['name']
    region = source_config['region']
    url = source_config.get('url', '')
    keywords = source_config.get('keywords', [])

    print(f"  Fetching: {url}")

    # æŠ“å–ç½‘é¡µå†…å®¹
    content = fetch_url_content(url)
    products = []

    if content and len(content) > 500:
        print(f"  Analyzing page content with GLM-4.7...")
        products = analyze_with_glm(content, task="extract")
        if not isinstance(products, list):
            products = []

    # å¦‚æœç½‘é¡µæå–å¤±è´¥ï¼Œä½¿ç”¨ GLM çŸ¥è¯†åº“æœç´¢
    if len(products) < 3:
        print(f"  Page content insufficient, using GLM knowledge search...")
        search_query = ' '.join(keywords[:3]) if keywords else source_name
        products = search_with_glm(search_query, region)
        if not isinstance(products, list):
            products = []

    print(f"  Found {len(products)} potential products")

    # è¡¥å……ä¿¡æ¯å¹¶è¯„åˆ†
    result = []
    for p in products[:limit]:
        # æ·»åŠ æ¥æºä¿¡æ¯
        p['source'] = source_name
        p['region'] = region
        p['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')

        # ç”¨ GLM è¯„åˆ†
        score_result = analyze_with_glm(p, task="score")
        if score_result:
            p['dark_horse_index'] = score_result.get('score', 2)
            if 'reason' in score_result:
                p['score_reason'] = score_result['reason']

        result.append(p)

    return result


def analyze_and_score(product: dict) -> dict:
    """
    ä½¿ç”¨ AI åˆ†æäº§å“å¹¶è¯„åˆ†

    è¯„åˆ†æ ‡å‡†ï¼š
    - 5åˆ†: èèµ„ >$100M æˆ– é¡¶çº§åˆ›å§‹äºº æˆ– å“ç±»å¼€åˆ›è€…
    - 4åˆ†: èèµ„ >$30M æˆ– YC/é¡¶çº§VC
    - 3åˆ†: èèµ„ >$5M æˆ– ProductHunt Top 5
    - 2åˆ†: æœ‰æ½œåŠ›ä½†æ•°æ®ä¸è¶³
    - 1åˆ†: è¾¹ç¼˜
    """
    funding = product.get('funding_total', '')
    source = product.get('source', '')

    # ç®€å•çš„è§„åˆ™è¯„åˆ†ï¼ˆå¯ä»¥æ›¿æ¢ä¸º AI è¯„åˆ†ï¼‰
    score = 2  # é»˜è®¤

    # è§£æèèµ„é‡‘é¢
    funding_amount = 0
    if funding:
        match = re.search(r'\$?([\d.]+)\s*([BMK])?', funding, re.I)
        if match:
            amount = float(match.group(1))
            unit = (match.group(2) or '').upper()
            if unit == 'B':
                funding_amount = amount * 1000
            elif unit == 'M':
                funding_amount = amount
            elif unit == 'K':
                funding_amount = amount / 1000
            else:
                funding_amount = amount

    # è¯„åˆ†é€»è¾‘
    if funding_amount >= 100:
        score = 5
    elif funding_amount >= 30:
        score = 4
    elif funding_amount >= 5:
        score = 3
    elif source in ['Y Combinator', 'ProductHunt']:
        score = 3

    product['dark_horse_index'] = score
    return product


def save_product(product: dict, dry_run: bool = False):
    """ä¿å­˜äº§å“åˆ°ç›¸åº”ç›®å½•"""
    score = product.get('dark_horse_index', 2)
    week = get_current_week()

    if score >= 4:
        # é»‘é©¬
        target_dir = DARK_HORSES_DIR
        target_file = os.path.join(target_dir, f'week_{week}.json')
    else:
        # æ½œåŠ›è‚¡
        target_dir = RISING_STARS_DIR
        target_file = os.path.join(target_dir, f'global_{week}.json')

    if dry_run:
        print(f"  [DRY RUN] Would save to: {target_file}")
        print(f"  {json.dumps(product, ensure_ascii=False, indent=2)}")
        return

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(target_dir, exist_ok=True)

    # åŠ è½½ç°æœ‰æ•°æ®
    if os.path.exists(target_file):
        with open(target_file, 'r', encoding='utf-8') as f:
            products = json.load(f)
    else:
        products = []

    # æ·»åŠ æ–°äº§å“
    products.append(product)

    # ä¿å­˜
    with open(target_file, 'w', encoding='utf-8') as f:
        json.dump(products, f, ensure_ascii=False, indent=2)

    print(f"  Saved to: {target_file}")


def discover_from_source(source_key: str, dry_run: bool = False):
    """ä»å•ä¸ªæ¸ é“å‘ç°äº§å“"""
    if source_key not in SOURCES:
        print(f"Unknown source: {source_key}")
        return

    config = SOURCES[source_key]
    print(f"\n{'='*50}")
    print(f"  Discovering from: {config['name']} {config['region']}")
    print(f"{'='*50}")

    existing = load_existing_products()

    # ä½¿ç”¨ GLM-4.7 å‘ç°äº§å“
    products = fetch_with_glm(config)

    new_count = 0
    for product in products:
        if is_duplicate(product.get('name', ''), product.get('website', ''), existing):
            print(f"  Skip duplicate: {product.get('name')}")
            continue

        # å¦‚æœ GLM æ²¡æœ‰è¯„åˆ†ï¼Œä½¿ç”¨è§„åˆ™è¯„åˆ†
        if 'dark_horse_index' not in product:
            product = analyze_and_score(product)

        save_product(product, dry_run)
        new_count += 1
        existing.add(product.get('name', '').lower())

    print(f"\n  Found {new_count} new products from {config['name']}")


def discover_all(dry_run: bool = False, tier: int = None):
    """ä»æ‰€æœ‰æ¸ é“å‘ç°äº§å“"""
    for source_key, config in SOURCES.items():
        if tier and config.get('tier', 1) > tier:
            continue
        discover_from_source(source_key, dry_run)


# ============================================
# æ–°å¢ï¼šåŸºäºåœ°åŒºçš„ Web Search å‘ç°
# ============================================

def discover_by_region(region_key: str, dry_run: bool = False) -> dict:
    """
    ä½¿ç”¨ Web Search MCP æŒ‰åœ°åŒºå‘ç° AI äº§å“

    Args:
        region_key: åœ°åŒºä»£ç  (us/cn/eu/jp/sea)
        dry_run: é¢„è§ˆæ¨¡å¼

    Returns:
        ç»Ÿè®¡ä¿¡æ¯
    """
    if region_key not in REGION_CONFIG:
        print(f"âŒ Unknown region: {region_key}")
        print(f"   Available: {', '.join(REGION_CONFIG.keys())}")
        return {"error": f"Unknown region: {region_key}"}

    config = REGION_CONFIG[region_key]
    region_name = config['name']
    search_engine = config['search_engine']
    keywords = config['keywords']

    print(f"\n{'='*60}")
    print(f"  ğŸŒ Discovering AI Products: {region_name}")
    print(f"  ğŸ“¡ Search Engine: {search_engine}")
    print(f"  ğŸ”‘ Keywords: {len(keywords)} queries")
    print(f"{'='*60}")

    existing = load_existing_products()
    all_products = []
    stats = {
        "region": region_key,
        "region_name": region_name,
        "search_results": 0,
        "products_found": 0,
        "products_saved": 0,
        "dark_horses": 0,
        "rising_stars": 0,
    }

    # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
    for i, keyword in enumerate(keywords, 1):
        print(f"\n  [{i}/{len(keywords)}] Searching: {keyword[:40]}...")

        # 1. Web Search è·å–å®æ—¶ç»“æœ
        search_results = web_search_mcp(keyword, search_engine)
        stats["search_results"] += len(search_results)

        if not search_results:
            print(f"    âš ï¸ No results, using GLM knowledge...")
            # é™çº§ï¼šä½¿ç”¨ GLM çŸ¥è¯†åº“
            search_results = search_with_glm(keyword, region_name)

        # å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºæ–‡æœ¬
        search_text = "\n\n".join([
            f"### {r.get('title', 'No Title')}\n"
            f"URL: {r.get('url', 'N/A')}\n"
            f"{r.get('content', r.get('snippet', ''))}"
            for r in search_results
        ])

        if not search_text.strip():
            continue

        # 2. ç”¨ä¸“ä¸š Prompt æå–äº§å“
        print(f"    ğŸ“Š Extracting products with GLM...")

        # æ˜ å°„åœ°åŒºä»£ç åˆ°åœ°åŒºæ——å¸œ
        region_flag_map = {
            'us': 'ğŸ‡ºğŸ‡¸',
            'cn': 'ğŸ‡¨ğŸ‡³',
            'eu': 'ğŸ‡ªğŸ‡º',
            'jp': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·',
            'sea': 'ğŸ‡¸ğŸ‡¬'
        }
        region_flag = region_flag_map.get(region_key, 'ğŸŒ')

        products = analyze_with_glm(search_text, task="extract", region=region_flag)

        if not isinstance(products, list):
            products = []

        print(f"    âœ… Extracted {len(products)} products")
        stats["products_found"] += len(products)

        # 3. å¯¹æ¯ä¸ªäº§å“è¯„åˆ†
        for product in products:
            if not product.get('name'):
                continue

            # æ£€æŸ¥é‡å¤
            if is_duplicate(product.get('name', ''), product.get('website', ''), existing):
                print(f"    â­ï¸ Skip duplicate: {product.get('name')}")
                continue

            # è¡¥å……ä¿¡æ¯
            product['region'] = region_flag
            product['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')
            product['discovery_method'] = 'web_search_mcp'
            product['search_keyword'] = keyword

            # 4. ç”¨ä¸“ä¸š Prompt è¯„åˆ†
            print(f"    ğŸ¯ Scoring: {product.get('name')}...")
            score_result = analyze_with_glm(product, task="score")

            if score_result and 'dark_horse_index' in score_result:
                product['dark_horse_index'] = score_result['dark_horse_index']
                product['score_reason'] = score_result.get('reason', '')
            else:
                # é™çº§ï¼šä½¿ç”¨è§„åˆ™è¯„åˆ†
                product = analyze_and_score(product)

            score = product.get('dark_horse_index', 2)
            print(f"    ğŸ“ˆ Score: {score}/5 - {product.get('score_reason', '')[:50]}...")

            # 5. ä¿å­˜äº§å“
            save_product(product, dry_run)
            stats["products_saved"] += 1

            if score >= 4:
                stats["dark_horses"] += 1
            else:
                stats["rising_stars"] += 1

            existing.add(product.get('name', '').lower())
            all_products.append(product)

    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"  ğŸ“Š Summary for {region_name}")
    print(f"{'='*60}")
    print(f"  Search Results: {stats['search_results']}")
    print(f"  Products Found: {stats['products_found']}")
    print(f"  Products Saved: {stats['products_saved']}")
    print(f"  ğŸ‡ Dark Horses (4-5): {stats['dark_horses']}")
    print(f"  â­ Rising Stars (2-3): {stats['rising_stars']}")

    return stats


def discover_all_regions(dry_run: bool = False) -> list:
    """
    æŒ‰åœ°åŒºæƒé‡å‘ç°æ‰€æœ‰åœ°åŒºçš„ AI äº§å“

    Returns:
        æ‰€æœ‰åœ°åŒºçš„ç»Ÿè®¡ä¿¡æ¯
    """
    print("\n" + "="*70)
    print("  ğŸŒ Global AI Product Discovery (Web Search MCP)")
    print("="*70)

    # æŒ‰æƒé‡æ’åº
    sorted_regions = sorted(
        REGION_CONFIG.items(),
        key=lambda x: x[1]['weight'],
        reverse=True
    )

    all_stats = []
    for region_key, config in sorted_regions:
        print(f"\n  ğŸ“ {config['name']} (Weight: {config['weight']}%)")
        stats = discover_by_region(region_key, dry_run)
        all_stats.append(stats)

    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "="*70)
    print("  ğŸ“Š Global Summary")
    print("="*70)

    total_search = sum(s.get('search_results', 0) for s in all_stats)
    total_found = sum(s.get('products_found', 0) for s in all_stats)
    total_saved = sum(s.get('products_saved', 0) for s in all_stats)
    total_dark_horses = sum(s.get('dark_horses', 0) for s in all_stats)
    total_rising_stars = sum(s.get('rising_stars', 0) for s in all_stats)

    print(f"  Total Search Results: {total_search}")
    print(f"  Total Products Found: {total_found}")
    print(f"  Total Products Saved: {total_saved}")
    print(f"  ğŸ‡ Total Dark Horses: {total_dark_horses}")
    print(f"  â­ Total Rising Stars: {total_rising_stars}")

    return all_stats


def test_web_search():
    """æµ‹è¯• Web Search MCP è¿æ¥"""
    print("\n" + "="*60)
    print("  ğŸ” Testing Web Search MCP")
    print("="*60)

    test_queries = [
        ("bing", "AI startup funding 2026"),
        ("sogou", "AIåˆ›ä¸šå…¬å¸ èèµ„ 2026"),
    ]

    for engine, query in test_queries:
        print(f"\n  Testing: {engine} - {query}")
        results = web_search_mcp(query, engine, count=3)

        if results:
            print(f"  âœ… Success! Found {len(results)} results")
            for i, r in enumerate(results[:2], 1):
                print(f"    {i}. {r.get('title', 'No Title')[:50]}...")
        else:
            print(f"  âš ï¸ No results (may fallback to GLM knowledge)")


def setup_schedule():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆmacOS/Linuxï¼‰"""
    script_path = os.path.abspath(__file__)

    # ç”Ÿæˆ cron ä»»åŠ¡
    cron_line = f"0 9 * * * cd {PROJECT_ROOT} && /usr/bin/python3 {script_path} >> /tmp/auto_discover.log 2>&1"

    print("\nè®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©æ—©ä¸Š9ç‚¹è¿è¡Œï¼‰ï¼š")
    print("-" * 50)
    print("è¿è¡Œä»¥ä¸‹å‘½ä»¤æ·»åŠ  cron ä»»åŠ¡ï¼š")
    print(f"\n  (crontab -l 2>/dev/null; echo \"{cron_line}\") | crontab -")
    print("\næˆ–è€…ä½¿ç”¨ launchd (macOS)ï¼š")
    print(f"  åˆ›å»º ~/Library/LaunchAgents/com.weeklyai.autodiscover.plist")


def test_glm_connection():
    """æµ‹è¯• GLM-4.7 è¿æ¥"""
    print("\næµ‹è¯• GLM-4.7 è¿æ¥...")
    print(f"  API Key: {ZHIPU_API_KEY[:20]}...")
    print(f"  Model: {ZHIPU_MODEL}")

    client = get_zhipu_client()
    if not client:
        print("  âŒ æ— æ³•åˆ›å»ºå®¢æˆ·ç«¯ï¼Œè¯·å®‰è£…: pip install zhipuai")
        return False

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±"}],
            max_tokens=100
        )
        result = response.choices[0].message.content
        print(f"  âœ… è¿æ¥æˆåŠŸ!")
        print(f"  GLM å›å¤: {result}")
        return True
    except Exception as e:
        print(f"  âŒ è¿æ¥å¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description='è‡ªåŠ¨å‘ç°å…¨çƒ AI äº§å“ (v2.0 - Web Search MCP)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  # æŒ‰åœ°åŒºæœç´¢ï¼ˆæ¨èï¼Œä½¿ç”¨ Web Search MCPï¼‰
  python tools/auto_discover.py --region us      # æœç´¢ç¾å›½ AI äº§å“
  python tools/auto_discover.py --region cn      # æœç´¢ä¸­å›½ AI äº§å“
  python tools/auto_discover.py --region eu      # æœç´¢æ¬§æ´² AI äº§å“
  python tools/auto_discover.py --region jp      # æœç´¢æ—¥éŸ© AI äº§å“
  python tools/auto_discover.py --region sea     # æœç´¢ä¸œå—äºš AI äº§å“
  python tools/auto_discover.py --region all     # æœç´¢æ‰€æœ‰åœ°åŒº

  # æŒ‰æ¸ é“æœç´¢ï¼ˆæ—§æ–¹å¼ï¼‰
  python tools/auto_discover.py --source 36kr    # ä» 36æ°ª å‘ç°
  python tools/auto_discover.py --source producthunt

  # å…¶ä»–é€‰é¡¹
  python tools/auto_discover.py --dry-run        # é¢„è§ˆä¸ä¿å­˜
  python tools/auto_discover.py --test           # æµ‹è¯• GLM è¿æ¥
  python tools/auto_discover.py --test-search    # æµ‹è¯• Web Search MCP
"""
    )

    # æ–°å¢ï¼šåœ°åŒºå‚æ•°
    parser.add_argument('--region', '-r',
                        choices=['us', 'cn', 'eu', 'jp', 'sea', 'all'],
                        help='æŒ‰åœ°åŒºæœç´¢ (us/cn/eu/jp/sea/all)')

    # åŸæœ‰å‚æ•°
    parser.add_argument('--source', '-s', help='æŒ‡å®šæ¸ é“ (e.g., 36kr, producthunt)')
    parser.add_argument('--tier', '-t', type=int, choices=[1, 2, 3], help='åªè¿è¡ŒæŒ‡å®šçº§åˆ«çš„æ¸ é“')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¿å­˜')
    parser.add_argument('--schedule', action='store_true', help='è®¾ç½®å®šæ—¶ä»»åŠ¡')
    parser.add_argument('--list-sources', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æ¸ é“')
    parser.add_argument('--list-regions', action='store_true', help='åˆ—å‡ºæ‰€æœ‰åœ°åŒº')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯• GLM-4.7 è¿æ¥')
    parser.add_argument('--test-search', action='store_true', help='æµ‹è¯• Web Search MCP')

    args = parser.parse_args()

    # æµ‹è¯•åŠŸèƒ½
    if args.test:
        test_glm_connection()
        return

    if args.test_search:
        test_web_search()
        return

    # åˆ—è¡¨åŠŸèƒ½
    if args.list_sources:
        print("\nå¯ç”¨æ¸ é“ï¼š")
        print("-" * 60)
        for key, config in SOURCES.items():
            print(f"  {key:15} {config['region']} {config['name']:20} Tier {config.get('tier', 1)}")
        return

    if args.list_regions:
        print("\nå¯ç”¨åœ°åŒºï¼š")
        print("-" * 60)
        for key, config in REGION_CONFIG.items():
            print(f"  {key:5} {config['name']:15} æƒé‡:{config['weight']:2}% æœç´¢å¼•æ“:{config['search_engine']}")
        return

    if args.schedule:
        setup_schedule()
        return

    # å‘ç°åŠŸèƒ½
    if args.region:
        # æ–°æ–¹å¼ï¼šæŒ‰åœ°åŒºæœç´¢
        if args.region == 'all':
            discover_all_regions(args.dry_run)
        else:
            discover_by_region(args.region, args.dry_run)
    elif args.source:
        # æ—§æ–¹å¼ï¼šæŒ‰æ¸ é“æœç´¢
        discover_from_source(args.source, args.dry_run)
    else:
        # é»˜è®¤ï¼šè¿è¡Œæ‰€æœ‰åœ°åŒºçš„ Web Search
        print("\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ --region å‚æ•°è¿›è¡Œåœ°åŒºæœç´¢ï¼ˆæ¨èï¼‰")
        print("   ç¤ºä¾‹: python tools/auto_discover.py --region us")
        print("   æˆ–è€…: python tools/auto_discover.py --region all")
        print("\n   ä½¿ç”¨ --source å‚æ•°è¿›è¡Œæ—§æ¸ é“æœç´¢")
        print("   ç¤ºä¾‹: python tools/auto_discover.py --source 36kr")
        print("\nè¿è¡Œ --help æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹")


if __name__ == '__main__':
    main()
