#!/usr/bin/env python3
"""
è‡ªåŠ¨å‘ç°å…¨çƒ AI äº§å“ (v2.0 - é›†æˆ Web Search MCP)

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Zhipu Web Search MCP å®æ—¶æœç´¢å…¨çƒ AI äº§å“
2. æŒ‰åœ°åŒºåˆ†é…æœç´¢ä»»åŠ¡ (ç¾å›½40%/ä¸­å›½25%/æ¬§æ´²15%/æ—¥éŸ©10%/ä¸œå—äºš10%)
3. ä½¿ç”¨ä¸“ä¸š Prompt æå–äº§å“ä¿¡æ¯å¹¶è¯„åˆ†
4. è‡ªåŠ¨åˆ†ç±»åˆ°é»‘é©¬(4-5åˆ†)/æ½œåŠ›è‚¡(2-3åˆ†)

ç”¨æ³•ï¼š
    python tools/auto_discover.py                    # è¿è¡Œæ‰€æœ‰åœ°åŒº
    python tools/auto_discover.py --region us       # åªæœç´¢ç¾å›½
    python tools/auto_discover.py --region cn       # åªæœç´¢ä¸­å›½
    python tools/auto_discover.py --dry-run         # é¢„è§ˆä¸ä¿å­˜
    python tools/auto_discover.py --test-search     # æµ‹è¯• Web Search MCP
"""

import json
import os
import sys
import argparse
import re
import time
import requests
from datetime import datetime, timezone
from urllib.parse import urlparse
import subprocess
from typing import Optional

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv
    # æŸ¥æ‰¾ .env æ–‡ä»¶ï¼ˆåœ¨ crawler ç›®å½•ä¸‹ï¼‰
    env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
    if os.path.exists(env_path):
        load_dotenv(env_path)
        print(f"âœ… Loaded .env from {env_path}")
except ImportError:
    pass  # dotenv æœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡

# æ™ºè°± AI é…ç½®
API_RATE_LIMIT_DELAY = 3  # æ¯æ¬¡ API è°ƒç”¨åç­‰å¾…ç§’æ•°
ZHIPU_API_KEY = os.environ.get('ZHIPU_API_KEY', '9c842f4999534eeba595b9fd142a699a.XXaPIGhbZTdzYIu8')
ZHIPU_MODEL = 'glm-4.7'

# Web Search MCP é…ç½®
WEB_SEARCH_MCP_URL = "https://open.bigmodel.cn/api/mcp/web_search/sse"
WEB_SEARCH_AUTH = ZHIPU_API_KEY

# Perplexity API é…ç½®
PERPLEXITY_API_KEY = os.environ.get('PERPLEXITY_API_KEY', '')
PERPLEXITY_MODEL = os.environ.get('PERPLEXITY_MODEL', 'sonar')  # sonar or sonar-pro

# Provider routing (toggle for testing)
USE_PERPLEXITY = os.environ.get('USE_PERPLEXITY', 'false').lower() == 'true'
REGION_PROVIDER_MAP = {
    'cn': 'glm',      # Always use GLM for Chinese content
    'us': 'perplexity' if USE_PERPLEXITY else 'glm',
    'eu': 'perplexity' if USE_PERPLEXITY else 'glm',
    'jp': 'perplexity' if USE_PERPLEXITY else 'glm',
    'kr': 'perplexity' if USE_PERPLEXITY else 'glm',
    'sea': 'perplexity' if USE_PERPLEXITY else 'glm',
}

# ============================================
# æ¯æ—¥é…é¢ç³»ç»Ÿ
# ============================================
import random

DAILY_QUOTA = {
    "dark_horses": 5,      # 4-5 åˆ†é»‘é©¬äº§å“
    "rising_stars": 10,    # 2-3 åˆ†æ½œåŠ›è‚¡
}

# æ¯åœ°åŒºæœ€å¤§äº§å“æ•°ï¼ˆé˜²æ­¢å•ä¸€åœ°åŒºä¸»å¯¼ï¼‰
REGION_MAX = {
    "us": 6, "cn": 4, "eu": 3, "jp": 2, "kr": 2, "sea": 2
}

MAX_ATTEMPTS = 3  # æœ€å¤§æœç´¢è½®æ•°

# ============================================
# å¤šè¯­è¨€å…³é”®è¯åº“ï¼ˆåŸç”Ÿè¯­è¨€æœç´¢æ•ˆæœæ›´å¥½ï¼‰
# ============================================

# è½¯ä»¶ AI å…³é”®è¯
KEYWORDS_SOFTWARE = {
    "us": [
        "AI startup funding 2026",
        "YC AI companies winter 2026",
        "AI Series A 2026",
        "artificial intelligence company raised funding",
        "AI unicorn startup valuation 2026",
        "AI agent startup funding",
        "generative AI startup Series A",
    ],
    "cn": [
        "AIèèµ„ 2026",
        "äººå·¥æ™ºèƒ½åˆ›ä¸šå…¬å¸",
        "AIGCèèµ„",
        "å¤§æ¨¡å‹åˆ›ä¸š",
        "AIåˆ›ä¸šå…¬å¸ Aè½® Bè½®",
        "äººå·¥æ™ºèƒ½ ç‹¬è§’å…½ ä¼°å€¼",
        "AI Agent åˆ›ä¸šå…¬å¸",
    ],
    "eu": [
        "European AI startup funding 2026",
        "KI Startup Finanzierung",
        "AI Series A Europe",
        "UK France Germany AI startup",
    ],
    "jp": [
        "AI ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— è³‡é‡‘èª¿é” 2026",
        "æ—¥æœ¬ AIä¼æ¥­ ã‚·ãƒªãƒ¼ã‚ºA",
        "äººå·¥çŸ¥èƒ½ ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—",
        "Japan AI startup funding",
    ],
    "kr": [
        "AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì 2026",
        "í•œêµ­ ì¸ê³µì§€ëŠ¥ ê¸°ì—…",
        "AI ì‹œë¦¬ì¦ˆA",
        "Korean AI startup investment",
    ],
    "sea": [
        "Singapore AI startup funding 2026",
        "Southeast Asia AI company",
        "AI startup Indonesia Vietnam",
        "Tech in Asia artificial intelligence",
    ],
}

# ç¡¬ä»¶ AI å…³é”®è¯ï¼ˆä¸“é—¨æœç´¢ç¡¬ä»¶äº§å“ï¼‰
KEYWORDS_HARDWARE = {
    "us": [
        "AI chip startup funding 2026",
        "humanoid robot company funding",
        "AI hardware startup Series A",
        "AI semiconductor startup investment",
        "robotics AI company raised funding",
        "AI accelerator chip startup",
        "edge AI hardware startup",
        "AI inference chip company",
        # æ–°å¢: ä» Product Hunt/Kickstarter å‘ç°
        "AI wearable device startup 2026",
        "AI smart glasses startup funding",
        "AI robot kickstarter 2026",
    ],
    "cn": [
        "AIèŠ¯ç‰‡ åˆ›ä¸šå…¬å¸ èèµ„",
        "äººå½¢æœºå™¨äºº åˆ›ä¸šå…¬å¸",
        "AIç¡¬ä»¶ èèµ„ 2026",
        "æ™ºèƒ½æœºå™¨äºº åˆ›ä¸šå…¬å¸ Aè½®",
        "AIèŠ¯ç‰‡ ç‹¬è§’å…½",
        "å…·èº«æ™ºèƒ½ åˆ›ä¸šå…¬å¸",
        "è¾¹ç¼˜AIèŠ¯ç‰‡ èèµ„",
        # æ–°å¢: ä» 36æ°ª å‘ç°
        "AIæ™ºèƒ½çœ¼é•œ åˆ›ä¸šå…¬å¸",
        "AIå¯ç©¿æˆ´è®¾å¤‡ èèµ„ 2026",
    ],
    "eu": [
        "European AI chip startup funding",
        "robotics startup Europe funding",
        "AI hardware company Germany UK",
        "semiconductor AI startup Europe",
        # æ–°å¢
        "AI robot startup Europe 2026",
    ],
    "jp": [
        "AIåŠå°ä½“ ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— è³‡é‡‘èª¿é”",
        "ãƒ­ãƒœãƒƒãƒˆ AIä¼æ¥­ æ—¥æœ¬",
        "Japan robotics AI startup",
        "AI chip startup Japan",
    ],
    "kr": [
        "AI ë°˜ë„ì²´ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì",
        "ë¡œë´‡ AI ê¸°ì—… í•œêµ­",
        "Korean AI chip startup",
    ],
    "sea": [
        "AI hardware startup Singapore",
        "robotics company Southeast Asia",
        "AI chip startup Asia",
    ],
}

# å…¼å®¹æ—§ä»£ç çš„åˆ«å
KEYWORDS_BY_REGION = KEYWORDS_SOFTWARE

# ============================================
# ç«™ç‚¹å®šå‘æœç´¢ï¼ˆç›´æ¥æœç´¢ç›®æ ‡åª’ä½“ï¼‰
# ============================================
SITE_SEARCHES = {
    "us": [
        "site:techcrunch.com AI startup funding",
        "site:producthunt.com AI launch 2026",
        "site:venturebeat.com AI funding",
        # ç¡¬ä»¶ç«™ç‚¹ (æ–°å¢)
        "site:producthunt.com AI hardware robot device 2026",
        "site:kickstarter.com AI robot wearable 2026",
    ],
    "cn": [
        "site:36kr.com AIèèµ„",
        "site:tmtpost.com äººå·¥æ™ºèƒ½",
        "site:jiqizhixin.com èèµ„",
        # ç¡¬ä»¶ç«™ç‚¹ (æ–°å¢)
        "site:36kr.com AIç¡¬ä»¶ æœºå™¨äºº 2026",
        "site:36kr.com å…·èº«æ™ºèƒ½ äººå½¢æœºå™¨äºº 2026",
    ],
    "eu": [
        "site:sifted.eu AI funding",
        "site:tech.eu AI startup",
        "site:eu-startups.com AI",
        # ç¡¬ä»¶ç«™ç‚¹ (æ–°å¢)
        "site:kickstarter.com AI robot Europe 2026",
    ],
    "jp": [
        "site:thebridge.jp AI startup",
        "site:jp.techcrunch.com AI",
        # ç¡¬ä»¶ç«™ç‚¹ (æ–°å¢)
        "site:kickstarter.com AI robot Japan 2026",
    ],
    "kr": [
        "site:platum.kr AI ìŠ¤íƒ€íŠ¸ì—…",
        "site:besuccess.com AI",
    ],
    "sea": [
        "site:e27.co AI startup",
        "site:techinasia.com AI funding",
        # ç¡¬ä»¶ç«™ç‚¹ (æ–°å¢)
        "site:kickstarter.com AI hardware Singapore 2026",
    ],
}

def get_keywords_for_today(region: str, product_type: str = "mixed") -> list:
    """
    æ ¹æ®æ—¥æœŸè½®æ¢å…³é”®è¯æ± 
    
    Args:
        region: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        product_type: äº§å“ç±»å‹ ("software"/"hardware"/"mixed")

    ç­–ç•¥ï¼š
    - mixed æ¨¡å¼ä¸‹ç¡¬ä»¶:è½¯ä»¶ = 40%:60%
    - æ¯å¤©è½®æ¢ä¸åŒçš„å…³é”®è¯ç»„åˆ
    """
    day = datetime.now().weekday()

    if product_type == "hardware":
        # åªè¿”å›ç¡¬ä»¶å…³é”®è¯
        keywords = KEYWORDS_HARDWARE.get(region, KEYWORDS_HARDWARE["us"])
    elif product_type == "software":
        # åªè¿”å›è½¯ä»¶å…³é”®è¯
        keywords = KEYWORDS_SOFTWARE.get(region, KEYWORDS_SOFTWARE["us"])
    else:
        # mixed æ¨¡å¼ï¼š40% ç¡¬ä»¶ + 60% è½¯ä»¶
        hw_keywords = KEYWORDS_HARDWARE.get(region, KEYWORDS_HARDWARE["us"])
        sw_keywords = KEYWORDS_SOFTWARE.get(region, KEYWORDS_SOFTWARE["us"])
        site_searches = SITE_SEARCHES.get(region, [])
        
        # è®¡ç®—æ•°é‡ï¼šç¡¬ä»¶ 40%ï¼Œè½¯ä»¶ 60%
        hw_count = max(2, len(hw_keywords) * 2 // 5)  # è‡³å°‘ 2 ä¸ªç¡¬ä»¶å…³é”®è¯
        sw_count = max(3, len(sw_keywords) * 3 // 5)  # è‡³å°‘ 3 ä¸ªè½¯ä»¶å…³é”®è¯
        
        # æ ¹æ®æ˜ŸæœŸå‡ è½®æ¢
        hw_start = (day * 2) % max(1, len(hw_keywords))
        sw_start = (day * 2) % max(1, len(sw_keywords))
        
        hw_selected = (hw_keywords[hw_start:] + hw_keywords[:hw_start])[:hw_count]
        sw_selected = (sw_keywords[sw_start:] + sw_keywords[:sw_start])[:sw_count]
        
        keywords = hw_selected + sw_selected + site_searches[:1]

    # éšæœºæ‰“ä¹±é¡ºåº
    shuffled = keywords.copy()
    random.shuffle(shuffled)
    return shuffled


def get_hardware_keywords(region: str) -> list:
    """è·å–ç¡¬ä»¶ä¸“ç”¨å…³é”®è¯"""
    return KEYWORDS_HARDWARE.get(region, KEYWORDS_HARDWARE["us"])


def get_software_keywords(region: str) -> list:
    """è·å–è½¯ä»¶ä¸“ç”¨å…³é”®è¯"""
    return KEYWORDS_SOFTWARE.get(region, KEYWORDS_SOFTWARE["us"])

def get_region_order() -> list:
    """éšæœºåŒ–åœ°åŒºæœç´¢é¡ºåºï¼Œé¿å…å›ºå®šåå·®"""
    regions = list(REGION_CONFIG.keys())
    random.shuffle(regions)
    return regions

# ============================================
# åœ°åŒºé…ç½® (æŒ‰æ¯”ä¾‹åˆ†é…æœç´¢ä»»åŠ¡)
# ============================================
REGION_CONFIG = {
    'us': {
        'name': 'ğŸ‡ºğŸ‡¸ ç¾å›½',
        'weight': 40,  # 40%
        'search_engine': 'bing',
        'keywords': [
            'AI startup funding Series A B 2026',
            'artificial intelligence company raised funding',
            'YC AI startup demo day 2026',
            'AI unicorn startup valuation',
        ],
    },
    'cn': {
        'name': 'ğŸ‡¨ğŸ‡³ ä¸­å›½',
        'weight': 25,  # 25%
        'search_engine': 'sogou',
        'keywords': [
            'AIåˆ›ä¸šå…¬å¸ èèµ„ AIGC å¤§æ¨¡å‹ è·æŠ•',
            'äººå·¥æ™ºèƒ½ åˆåˆ›å…¬å¸ Aè½® Bè½® èèµ„',
            'å¤§æ¨¡å‹ åˆ›ä¸šå…¬å¸ ä¼°å€¼ èèµ„æ–°é—»',
            'AIGC ç‹¬è§’å…½ èèµ„ 2026',
        ],
    },
    'eu': {
        'name': 'ğŸ‡ªğŸ‡º æ¬§æ´²',
        'weight': 15,  # 15%
        'search_engine': 'bing',
        'keywords': [
            'European AI startup funding Sifted',
            'Europe artificial intelligence company raised',
            'UK France Germany AI startup Series A',
        ],
    },
    'jp': {
        'name': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡· æ—¥éŸ©',
        'weight': 10,  # 10%
        'search_engine': 'bing',
        'keywords': [
            'Japan Korea AI startup funding',
            'Japanese artificial intelligence company raised',
            'Korean AI startup investment',
        ],
    },
    'sea': {
        'name': 'ğŸ‡¸ğŸ‡¬ ä¸œå—äºš',
        'weight': 10,  # 10%
        'search_engine': 'bing',
        'keywords': [
            'Southeast Asia AI startup e27 funding',
            'Singapore Indonesia Vietnam AI company raised',
            'Tech in Asia artificial intelligence funding',
        ],
    },
}

# ============================================
# é¡¹ç›®è·¯å¾„è®¾ç½® (å¿…é¡»åœ¨å¯¼å…¥ prompts ä¹‹å‰)
# ============================================
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

# ============================================
# Prompt æ¨¡å— (ç‹¬ç«‹ä¼˜åŒ–çš„æœç´¢å’Œåˆ†æ Prompt)
# ============================================

# å¯¼å…¥æ¨¡å—åŒ– Prompt
try:
    from prompts.search_prompts import (
        generate_search_queries,
        generate_discovery_query,
        get_search_params,
        SEARCH_QUERIES_BY_REGION,
    )
    from prompts.analysis_prompts import (
        ANALYSIS_PROMPT_EN,
        ANALYSIS_PROMPT_CN,
        SCORING_PROMPT,
        get_analysis_prompt,
        get_scoring_prompt,
        WELL_KNOWN_PRODUCTS as PROMPT_WELL_KNOWN,
        GENERIC_WHY_MATTERS as PROMPT_GENERIC,
    )
    USE_MODULAR_PROMPTS = True
    print("âœ… Loaded modular prompts from prompts/")
except ImportError as e:
    USE_MODULAR_PROMPTS = False
    print(f"âš ï¸ prompts/ module not found: {e}")

# Fallback: å†…è” Promptï¼ˆå½“æ¨¡å—æœªåŠ è½½æ—¶ä½¿ç”¨ï¼‰
if not USE_MODULAR_PROMPTS:
    # è‹±æ–‡ç‰ˆ Prompt (us/eu/jp/kr/sea)
    ANALYSIS_PROMPT_EN = """You are WeeklyAI's AI Product Analyst. Extract and score AI products from search results.

## Search Results
{search_results}

## STRICT EXCLUSIONS (Never Include):
- Well-Known: ChatGPT, Claude, Gemini, Copilot, DALL-E, Sora, Midjourney, Cursor, Perplexity
- Not Products: LangChain, PyTorch, papers only, tool directories
- Big Tech: Google Gemini, Meta Llama, Microsoft Copilot

## DARK HORSE (4-5) - Must meet â‰¥2:
| growth_anomaly | founder_background | funding_signal | category_innovation | community_buzz |

**5 points**: Funding >$100M OR Top-tier founder OR Category creator
**4 points**: Funding >$30M OR YC/a16z backed OR ARR >$10M

## RISING STAR (2-3) - Need 1:
**3 points**: Funding $1M-$5M OR ProductHunt top 10
**2 points**: Just launched, clear innovation

## CRITICAL: why_matters must have specific numbers!
âœ… GOOD: "Sequoiaé¢†æŠ•$50Mï¼Œ8ä¸ªæœˆARRä»0åˆ°$10M"
âŒ BAD: "This is a promising AI product"

## Output (JSON only)
```json
[{{"name": "...", "website": "https://...", "description": "ä¸­æ–‡æè¿°(>20å­—)", "category": "coding|image|video|...", "region": "{region}", "funding_total": "$50M", "dark_horse_index": 4, "criteria_met": ["funding_signal"], "why_matters": "å…·ä½“æ•°å­—+å·®å¼‚åŒ–", "source": "...", "confidence": 0.85}}]
```

Quota: Dark Horses: {quota_dark_horses} | Rising Stars: {quota_rising_stars}
Return [] if nothing qualifies."""

    # ä¸­æ–‡ç‰ˆ Prompt (cn)
    ANALYSIS_PROMPT_CN = """ä½ æ˜¯ WeeklyAI çš„ AI äº§å“åˆ†æå¸ˆã€‚ä»æœç´¢ç»“æœä¸­æå–å¹¶è¯„åˆ† AI äº§å“ã€‚

## æœç´¢ç»“æœ
{search_results}

## ä¸¥æ ¼æ’é™¤ï¼š
- å·²çŸ¥å: ChatGPT, Claude, Gemini, Cursor, Kimi, è±†åŒ…, é€šä¹‰åƒé—®, æ–‡å¿ƒä¸€è¨€
- éäº§å“: LangChain, PyTorch, åªæœ‰è®ºæ–‡/demo
- å¤§å‚: Google Gemini, ç™¾åº¦æ–‡å¿ƒ, é˜¿é‡Œé€šä¹‰

## é»‘é©¬ (4-5åˆ†) - æ»¡è¶³â‰¥2æ¡:
| growth_anomaly | founder_background | funding_signal | category_innovation | community_buzz |

**5åˆ†**: èèµ„>$100M æˆ– é¡¶çº§åˆ›å§‹äºº æˆ– å“ç±»å¼€åˆ›è€…
**4åˆ†**: èèµ„>$30M æˆ– YC/a16zèƒŒä¹¦ æˆ– ARR>$10M

## æ½œåŠ›è‚¡ (2-3åˆ†) - æ»¡è¶³1æ¡:
**3åˆ†**: èèµ„$1M-$5M æˆ– ProductHunt Top 10
**2åˆ†**: åˆšå‘å¸ƒä½†æœ‰æ˜æ˜¾åˆ›æ–°

## why_matters å¿…é¡»æœ‰å…·ä½“æ•°å­—!
âœ… GOOD: "Sequoiaé¢†æŠ•$50Mï¼Œ8ä¸ªæœˆARRä»0åˆ°$10M"
âŒ BAD: "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ½œåŠ›çš„AIäº§å“"

## è¾“å‡º (ä»…JSON)
```json
[{{"name": "äº§å“å", "website": "https://...", "description": "ä¸­æ–‡æè¿°(>20å­—)", "category": "coding|image|video|...", "region": "{region}", "funding_total": "$50M", "dark_horse_index": 4, "criteria_met": ["funding_signal"], "why_matters": "å…·ä½“æ•°å­—+å·®å¼‚åŒ–", "source": "...", "confidence": 0.85}}]
```

é…é¢: é»‘é©¬: {quota_dark_horses} | æ½œåŠ›è‚¡: {quota_rising_stars}
æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„è¿”å› []ã€‚"""

    # è¯„åˆ† Prompt
    SCORING_PROMPT = """è¯„ä¼°äº§å“çš„"é»‘é©¬æŒ‡æ•°"(1-5åˆ†)ï¼š

## äº§å“
{product}

## è¯„åˆ†æ ‡å‡†
5åˆ†: èèµ„>$100M æˆ– é¡¶çº§åˆ›å§‹äººèƒŒæ™¯ æˆ– å“ç±»å¼€åˆ›è€…
4åˆ†: èèµ„>$30M æˆ– YC/a16zæŠ•èµ„ æˆ– ARR>$10M
3åˆ†: èèµ„$5M-$30M æˆ– ProductHunt Top 5
2åˆ†: æœ‰åˆ›æ–°ç‚¹ä½†æ•°æ®ä¸è¶³
1åˆ†: è¾¹ç¼˜äº§å“æˆ–å¾…éªŒè¯

## è¿”å›æ ¼å¼ï¼ˆä»…JSONï¼‰
```json
{{"dark_horse_index": 4, "criteria_met": ["funding_signal"], "reason": "è¯„åˆ†ç†ç”±"}}
```"""


def get_extraction_prompt(region_key: str) -> str:
    """
    æ ¹æ®åœ°åŒºé€‰æ‹©åˆé€‚çš„åˆ†æ prompt
    
    Args:
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea)

    Returns:
        å¯¹åº”åœ°åŒºçš„ prompt æ¨¡æ¿
    """
    if region_key == "cn":
        return ANALYSIS_PROMPT_CN
    else:
        return ANALYSIS_PROMPT_EN


# åˆ«åï¼šå…¼å®¹æ—§ä»£ç 
PROMPT_EXTRACTION_EN = ANALYSIS_PROMPT_EN if not USE_MODULAR_PROMPTS else ANALYSIS_PROMPT_EN
PROMPT_EXTRACTION_CN = ANALYSIS_PROMPT_CN if not USE_MODULAR_PROMPTS else ANALYSIS_PROMPT_CN
PROMPT_DARK_HORSE_SCORING = SCORING_PROMPT if not USE_MODULAR_PROMPTS else SCORING_PROMPT

# æ•°æ®æ–‡ä»¶è·¯å¾„
DARK_HORSES_DIR = os.path.join(PROJECT_ROOT, 'data', 'dark_horses')
RISING_STARS_DIR = os.path.join(PROJECT_ROOT, 'data', 'rising_stars')
CANDIDATES_DIR = os.path.join(PROJECT_ROOT, 'data', 'candidates')

# æ¸ é“é…ç½®
SOURCES = {
    # ç¾å›½æ¸ é“
    'techcrunch': {
        'name': 'TechCrunch',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://techcrunch.com/category/artificial-intelligence/',
        'rss': 'https://techcrunch.com/category/artificial-intelligence/feed/',
        'keywords': ['raises', 'Series A', 'Series B', 'funding', 'AI startup'],
        'tier': 1,
    },
    'producthunt': {
        'name': 'ProductHunt',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://www.producthunt.com/topics/artificial-intelligence',
        'api': 'https://api.producthunt.com/v2/api/graphql',
        'keywords': ['AI', 'machine learning', 'LLM'],
        'tier': 2,
    },
    'ycombinator': {
        'name': 'Y Combinator',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://www.ycombinator.com/companies?tags=AI',
        'keywords': ['YC', 'Demo Day'],
        'tier': 1,
    },

    # ä¸­å›½æ¸ é“
    '36kr': {
        'name': '36æ°ª',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://36kr.com/information/AI/',
        'rss': 'https://36kr.com/feed',
        'keywords': ['AIèèµ„', 'äººå·¥æ™ºèƒ½', 'AIGC', 'å¤§æ¨¡å‹', 'è·æŠ•'],
        'tier': 1,
    },
    'itjuzi': {
        'name': 'ITæ¡”å­',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://www.itjuzi.com/investevent',
        'keywords': ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ '],
        'tier': 1,
    },
    'jiqizhixin': {
        'name': 'æœºå™¨ä¹‹å¿ƒ',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://www.jiqizhixin.com/',
        'rss': 'https://www.jiqizhixin.com/rss',
        'keywords': ['AI', 'èèµ„', 'åˆ›ä¸š'],
        'tier': 2,
    },

    # æ¬§æ´²æ¸ é“
    'sifted': {
        'name': 'Sifted',
        'region': 'ğŸ‡ªğŸ‡º',
        'url': 'https://sifted.eu/sector/artificial-intelligence',
        'keywords': ['AI', 'funding', 'European startup'],
        'tier': 1,
    },
    'eu_startups': {
        'name': 'EU-Startups',
        'region': 'ğŸ‡ªğŸ‡º',
        'url': 'https://www.eu-startups.com/category/artificial-intelligence/',
        'rss': 'https://www.eu-startups.com/feed/',
        'keywords': ['AI', 'raises', 'funding'],
        'tier': 2,
    },

    # æ—¥éŸ©æ¸ é“
    'bridge': {
        'name': 'Bridge',
        'region': 'ğŸ‡¯ğŸ‡µ',
        'url': 'https://thebridge.jp/en/',
        'keywords': ['AI', 'startup', 'funding', 'Japan'],
        'tier': 1,
    },
    'platum': {
        'name': 'Platum',
        'region': 'ğŸ‡°ğŸ‡·',
        'url': 'https://platum.kr/archives/category/ai',
        'keywords': ['AI', 'startup', 'Korea'],
        'tier': 1,
    },

    # ä¸œå—äºšæ¸ é“
    'e27': {
        'name': 'e27',
        'region': 'ğŸ‡¸ğŸ‡¬',
        'url': 'https://e27.co/tag/artificial-intelligence/',
        'keywords': ['AI', 'Southeast Asia', 'funding'],
        'tier': 1,
    },
    'techinasia': {
        'name': 'Tech in Asia',
        'region': 'ğŸ‡¸ğŸ‡¬',
        'url': 'https://www.techinasia.com/tag/artificial-intelligence',
        'keywords': ['AI', 'Asia', 'startup'],
        'tier': 1,
    },
}


def get_current_week():
    """è·å–å½“å‰å‘¨æ•°"""
    now = datetime.now()
    return f"{now.year}_{now.isocalendar()[1]:02d}"


def load_existing_products():
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„äº§å“åç§°å’Œç½‘å€"""
    existing = set()

    # åŠ è½½é»‘é©¬
    if os.path.exists(DARK_HORSES_DIR):
        for f in os.listdir(DARK_HORSES_DIR):
            if f.endswith('.json'):
                with open(os.path.join(DARK_HORSES_DIR, f), 'r') as file:
                    products = json.load(file)
                    for p in products:
                        existing.add(p.get('name', '').lower())
                        existing.add(p.get('website', '').lower())

    # åŠ è½½æ½œåŠ›è‚¡
    if os.path.exists(RISING_STARS_DIR):
        for f in os.listdir(RISING_STARS_DIR):
            if f.endswith('.json'):
                with open(os.path.join(RISING_STARS_DIR, f), 'r') as file:
                    products = json.load(file)
                    for p in products:
                        existing.add(p.get('name', '').lower())
                        existing.add(p.get('website', '').lower())

    return existing


def is_duplicate(name: str, website: str, existing: set) -> bool:
    """æ£€æŸ¥æ˜¯å¦é‡å¤"""
    return name.lower() in existing or website.lower() in existing


def normalize_url(url: str) -> str:
    """
    æ ‡å‡†åŒ– URLï¼Œæå–ä¸»åŸŸåç”¨äºå»é‡

    "https://www.example.com/page" â†’ "example.com"
    """
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.replace("www.", "")
        return domain.lower()
    except:
        return url.lower()


def verify_url_exists(url: str, timeout: int = 5) -> bool:
    """
    éªŒè¯ URL æ˜¯å¦çœŸå®å­˜åœ¨ï¼ˆå¯è®¿é—®ï¼‰
    
    Args:
        url: è¦éªŒè¯çš„ URL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        True å¦‚æœ URL å¯è®¿é—®ï¼ŒFalse å¦åˆ™
    """
    if not url or url.lower() == "unknown":
        return False
    
    try:
        # ç¡®ä¿æœ‰åè®®
        if not url.startswith(("http://", "https://")):
            url = f"https://{url}"
        
        # ç¦ç”¨ SSL è­¦å‘Šï¼ˆLibreSSL ç‰ˆæœ¬é—®é¢˜ï¼‰
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # å‘é€ GET è¯·æ±‚ï¼ˆHEAD æœ‰æ—¶è¢«æ‹’ç»ï¼‰
        response = requests.get(
            url,
            timeout=timeout,
            allow_redirects=True,
            headers={"User-Agent": "Mozilla/5.0 (compatible; WeeklyAI Bot)"},
            verify=False,  # ç¦ç”¨ SSL éªŒè¯ï¼ˆLibreSSL å…¼å®¹æ€§ï¼‰
            stream=True  # ä¸ä¸‹è½½å†…å®¹
        )
        response.close()
        return response.status_code < 400
    except requests.exceptions.RequestException:
        return False


def is_duplicate_domain(product: dict, existing_domains: set) -> bool:
    """æ£€æŸ¥åŸŸåæ˜¯å¦å·²å­˜åœ¨"""
    domain = normalize_url(product.get("website", ""))
    return domain in existing_domains if domain else False


# ============================================
# è´¨é‡è¿‡æ»¤å™¨
# ============================================

# æ³›åŒ–çš„ why_matters é»‘åå•ï¼ˆä¼šè¢«è¿‡æ»¤æ‰ï¼‰
GENERIC_WHY_MATTERS = [
    "å¾ˆæœ‰æ½œåŠ›",
    "å€¼å¾—å…³æ³¨",
    "æœ‰å‰æ™¯",
    "è¡¨ç°ä¸é”™",
    "å›¢é˜ŸèƒŒæ™¯ä¸é”™",
    "èèµ„æƒ…å†µè‰¯å¥½",
    "å¸‚åœºå‰æ™¯å¹¿é˜”",
    "æŠ€æœ¯å®åŠ›å¼º",
    "ç”¨æˆ·åé¦ˆè‰¯å¥½",
    "å¢é•¿è¿…é€Ÿ",
]

# çŸ¥åäº§å“æ’é™¤åå•ï¼ˆä¸æ˜¯é»‘é©¬ï¼‰
WELL_KNOWN_PRODUCTS = {
    # å›½é™…çŸ¥å AI äº§å“
    "chatgpt", "openai", "claude", "anthropic", "gemini", "bard",
    "copilot", "github copilot", "dall-e", "dall-e 3", "sora",
    "midjourney", "stable diffusion", "stability ai",
    "cursor", "perplexity", "elevenlabs", "eleven labs",
    "synthesia", "runway", "runway ml", "pika", "pika labs",
    "bolt.new", "bolt", "v0.dev", "v0", "replit", "together ai", "groq",
    "character.ai", "character ai", "jasper", "jasper ai",
    "notion ai", "grammarly", "copy.ai", "writesonic",
    "huggingface", "hugging face", "langchain", "llamaindex",
    # ä¸­å›½çŸ¥å AI äº§å“
    "kimi", "æœˆä¹‹æš—é¢", "moonshot", "doubao", "è±†åŒ…", "å­—èŠ‚è·³åŠ¨",
    "tongyi", "é€šä¹‰åƒé—®", "é€šä¹‰", "qwen", "wenxin", "æ–‡å¿ƒä¸€è¨€", "æ–‡å¿ƒ",
    "ernie", "ç™¾åº¦", "baidu", "æ™ºè°±", "zhipu", "chatglm", "glm",
    "è®¯é£æ˜Ÿç«", "æ˜Ÿç«", "spark", "minimax", "abab",
    # å¤§å‚äº§å“
    "google gemini", "google bard", "meta llama", "llama",
    "microsoft copilot", "bing chat", "amazon q", "aws bedrock",
}


def validate_product(product: dict) -> tuple[bool, str]:
    """
    éªŒè¯äº§å“è´¨é‡ï¼Œè¿”å› (æ˜¯å¦é€šè¿‡, åŸå› )

    è¿‡æ»¤æ¡ä»¶:
    1. å¿…é¡»æœ‰æœ‰æ•ˆçš„ website URL
    2. description å¿…é¡» >20 å­—ç¬¦
    3. why_matters ä¸èƒ½æ˜¯æ³›åŒ–æè¿°
    4. name ä¸èƒ½æ˜¯æ–°é—»æ ‡é¢˜
    5. çŸ¥åäº§å“æ’é™¤ï¼ˆä½¿ç”¨ WELL_KNOWN_PRODUCTSï¼‰
    6. é»‘é©¬(4-5åˆ†)å¿…é¡»æ»¡è¶³è‡³å°‘2æ¡æ ‡å‡† (criteria_met)
    7. ç½®ä¿¡åº¦æ£€æŸ¥ (confidence >= 0.6)
    """
    name = product.get("name", "").strip()
    website = product.get("website", "").strip()
    description = product.get("description", "").strip()
    why_matters = product.get("why_matters", "").strip()

    # 1. æ£€æŸ¥å¿…å¡«å­—æ®µ
    if not name:
        return False, "missing name"
    if not description:
        return False, "missing description"
    if not why_matters:
        return False, "missing why_matters"

    # 2. æ£€æŸ¥ website
    if not website:
        return False, "missing website"
    
    # ä¿®å¤ç¼ºå°‘åè®®çš„ URL
    if not website.startswith(("http://", "https://")) and "." in website:
        website = f"https://{website}"
        product["website"] = website
    
    if website.lower() == "unknown":
        # å…è®¸ unknownï¼Œä½†åç»­éœ€è¦äººå·¥éªŒè¯
        product["needs_verification"] = True
    elif not website.startswith(("http://", "https://")):
        return False, "invalid website URL"

    # 3. æ£€æŸ¥ description é•¿åº¦
    if len(description) < 20:
        return False, f"description too short ({len(description)} chars)"

    # 4. æ£€æŸ¥ why_matters æ˜¯å¦å¤ªæ³›åŒ–
    why_lower = why_matters.lower()
    for generic in GENERIC_WHY_MATTERS:
        if generic in why_lower and len(why_matters) < 50:
            return False, f"generic why_matters: contains '{generic}'"

    # 5. æ£€æŸ¥ why_matters æ˜¯å¦åŒ…å«å…·ä½“æ•°å­—ï¼ˆèèµ„/ARR/ç”¨æˆ·æ•°ï¼‰
    has_number = bool(re.search(r'[\$Â¥â‚¬]\d+|ARR|\d+[MBKä¸‡äº¿]|\d+%', why_matters))
    has_specific = any(kw in why_matters for kw in [
        'é¢†æŠ•', 'èèµ„', 'ä¼°å€¼', 'ç”¨æˆ·', 'å¢é•¿', 'ARR', 'é¦–åˆ›', 'é¦–ä¸ª',
        'å‰OpenAI', 'å‰Google', 'å‰Meta', 'YC', 'a16z', 'Sequoia',
    ])
    if not has_number and not has_specific:
        return False, "why_matters lacks specific details"

    # 6. æ£€æŸ¥ name æ˜¯å¦åƒæ–°é—»æ ‡é¢˜
    news_patterns = ['èèµ„', 'å®£å¸ƒ', 'å‘å¸ƒ', 'è·å¾—', 'å®Œæˆ', 'æ¨å‡º', 'ä¸Šçº¿']
    if any(p in name for p in news_patterns) and len(name) > 15:
        return False, "name looks like news headline"

    # 7. æ£€æŸ¥æ˜¯å¦æ˜¯çŸ¥åäº§å“
    name_lower = name.lower()
    if name_lower in WELL_KNOWN_PRODUCTS:
        return False, f"well-known product: {name}"
    # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆä¾‹å¦‚ "ChatGPT Plus" åŒ…å« "chatgpt"ï¼‰
    for known in WELL_KNOWN_PRODUCTS:
        if known in name_lower or name_lower in known:
            return False, f"well-known product match: {known}"

    # 8. æ£€æŸ¥é»‘é©¬(4-5åˆ†)æ˜¯å¦æ»¡è¶³è‡³å°‘1æ¡æ ‡å‡†ï¼ˆæ”¾å®½è¦æ±‚ï¼‰
    # æ³¨ï¼šåŸæ¥è¦æ±‚ â‰¥2 æ¡æ ‡å‡†å¤ªä¸¥æ ¼ï¼Œå¯¼è‡´äº§å‡ºå¤ªå°‘
    score = product.get("dark_horse_index", 0)
    criteria = product.get("criteria_met", [])
    if score >= 5 and len(criteria) < 2:
        # 5åˆ†é»‘é©¬éœ€è¦ â‰¥2 æ¡æ ‡å‡†
        return False, f"5-star dark_horse needs â‰¥2 criteria (has {len(criteria)})"
    if score == 4 and len(criteria) < 1:
        # 4åˆ†é»‘é©¬åªéœ€è¦ â‰¥1 æ¡æ ‡å‡†
        return False, f"4-star dark_horse needs â‰¥1 criteria (has {len(criteria)})"

    # 9. æ£€æŸ¥ç½®ä¿¡åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
    confidence = product.get("confidence", 1.0)
    if confidence < 0.6:
        return False, f"low confidence ({confidence:.2f})"

    return True, "passed"


def load_existing_domains() -> set:
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„äº§å“åŸŸå"""
    domains = set()

    for dir_path in [DARK_HORSES_DIR, RISING_STARS_DIR]:
        if os.path.exists(dir_path):
            for f in os.listdir(dir_path):
                if f.endswith('.json'):
                    try:
                        with open(os.path.join(dir_path, f), 'r') as file:
                            products = json.load(file)
                            for p in products:
                                domain = normalize_url(p.get('website', ''))
                                if domain:
                                    domains.add(domain)
                    except:
                        pass

    return domains


def get_zhipu_client():
    """è·å–æ™ºè°± AI å®¢æˆ·ç«¯"""
    try:
        from zhipuai import ZhipuAI
        return ZhipuAI(api_key=ZHIPU_API_KEY)
    except ImportError:
        print("  Error: zhipuai SDK not installed. Run: pip install zhipuai")
        return None


def get_perplexity_client():
    """
    è·å– Perplexity å®¢æˆ·ç«¯
    
    Returns:
        PerplexityClient å®ä¾‹æˆ– None
    """
    if not PERPLEXITY_API_KEY:
        print("  âš ï¸ PERPLEXITY_API_KEY not set")
        return None
    
    try:
        from utils.perplexity_client import PerplexityClient
        client = PerplexityClient(api_key=PERPLEXITY_API_KEY)
        if client.is_available():
            return client
        return None
    except ImportError as e:
        print(f"  âš ï¸ perplexity_client module not found: {e}")
        return None


def perplexity_search(
    query: str,
    count: int = 10,
    region: Optional[str] = None,
    domain_filter: Optional[list] = None
) -> list:
    """
    ä½¿ç”¨ Perplexity Search API è¿›è¡Œå®æ—¶ Web æœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        count: ç»“æœæ•°é‡
        region: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        domain_filter: åŸŸåè¿‡æ»¤ (["techcrunch.com", "-reddit.com"] ç­‰)
    
    Returns:
        [{"title": "", "url": "", "content": ""}, ...]
    """
    client = get_perplexity_client()
    if not client:
        return []
    
    try:
        if region:
            results = client.search_by_region(
                query,
                region=region,
                max_results=count
            )
        else:
            results = client.search(
                query,
                max_results=count,
                domain_filter=domain_filter
            )
        return [r.to_dict() for r in results]
    
    except Exception as e:
        print(f"  âŒ Perplexity Search Error: {e}")
        return []


def web_search_mcp(query: str, search_engine: str = "bing", count: int = 10) -> list:
    """
    ä½¿ç”¨ Zhipu Web Search MCP è¿›è¡Œå®æ—¶ç½‘ç»œæœç´¢

    Args:
        query: æœç´¢å…³é”®è¯
        search_engine: æœç´¢å¼•æ“ (bing/sogou/quark/jina)
        count: è¿”å›ç»“æœæ•°é‡

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ [{"title": "", "url": "", "content": ""}, ...]
    """
    # ä½¿ç”¨æ™ºè°± AI API è¿›è¡Œ web_search å·¥å…·è°ƒç”¨
    client = get_zhipu_client()
    if not client:
        return []

    try:
        print(f"  ğŸ” Web Search: {query[:50]}...")

        # ä½¿ç”¨ GLM-4.7 çš„ web_search å·¥å…·
        response = client.chat.completions.create(
            model="glm-4-plus",  # æ”¯æŒ web_search çš„æ¨¡å‹
            messages=[{
                "role": "user",
                "content": f"æœç´¢æœ€æ–°çš„ AI åˆ›ä¸šå…¬å¸èèµ„æ–°é—»: {query}"
            }],
            tools=[{
                "type": "web_search",
                "web_search": {
                    "enable": True,
                    "search_engine": search_engine,
                    "search_result": True
                }
            }],
            tool_choice="auto",
            max_tokens=4096
        )

        # æå–æœç´¢ç»“æœ
        results = []

        # æ£€æŸ¥æ˜¯å¦æœ‰ web_search ç»“æœ
        if hasattr(response, 'web_search') and response.web_search:
            for item in response.web_search:
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("link", item.get("url", "")),
                    "content": item.get("content", item.get("snippet", ""))
                })

        # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–ç»“æœï¼Œä»å›å¤ä¸­æå–
        if not results and response.choices:
            content = response.choices[0].message.content
            # è¿”å›åŸå§‹å†…å®¹ä¾›åç»­å¤„ç†
            results = [{"title": "Search Results", "url": "", "content": content}]

        print(f"  âœ… Found {len(results)} results")
        return results

    except Exception as e:
        print(f"  âŒ Web Search Error: {e}")
        # é™çº§ï¼šä½¿ç”¨ GLM çŸ¥è¯†åº“
        return []
    finally:
        time.sleep(API_RATE_LIMIT_DELAY)


def analyze_with_glm(content: str, task: str = "extract", region: str = "ğŸ‡ºğŸ‡¸",
                     quota_remaining: dict = None, region_key: str = "us") -> dict:
    """
    ä½¿ç”¨ GLM-4.7 åˆ†æå†…å®¹ (ä½¿ç”¨åŒè¯­ Prompt - åˆå¹¶æå–+è¯„åˆ†)

    Args:
        content: è¦åˆ†æçš„å†…å®¹ï¼ˆæœç´¢ç»“æœã€äº§å“ä¿¡æ¯ç­‰ï¼‰
        task: ä»»åŠ¡ç±»å‹ (extract/score/translate)
        region: åœ°åŒºæ ‡è¯† (emoji flag)
        quota_remaining: å‰©ä½™é…é¢ {"dark_horses": n, "rising_stars": m}
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea) ç”¨äºé€‰æ‹© prompt è¯­è¨€

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    client = get_zhipu_client()
    if not client:
        return {}

    # é»˜è®¤é…é¢
    if quota_remaining is None:
        quota_remaining = DAILY_QUOTA.copy()

    if task == "extract":
        # ä½¿ç”¨åŒè¯­ prompt é€‰æ‹©å™¨ï¼ˆåˆå¹¶æå–+è¯„åˆ†ï¼‰
        prompt_template = get_extraction_prompt(region_key)
        prompt = prompt_template.format(
            search_results=content[:10000],
            region=region,
            quota_dark_horses=quota_remaining.get("dark_horses", 5),
            quota_rising_stars=quota_remaining.get("rising_stars", 10)
        )

    elif task == "score":
        # ä¿ç•™å•ç‹¬è¯„åˆ†åŠŸèƒ½ï¼ˆç”¨äº fallbackï¼‰
        prompt = PROMPT_DARK_HORSE_SCORING.format(
            product=json.dumps(content, ensure_ascii=False, indent=2)
        )

    elif task == "translate":
        prompt = f"""å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­ï¼š

{content}

åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4096
        )

        result_text = response.choices[0].message.content

        # æå– JSON
        if task in ["extract", "score"]:
            # å°è¯•æå– JSON
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
            if json_match:
                return json.loads(json_match.group(1))
            # å°è¯•ç›´æ¥è§£æ
            try:
                return json.loads(result_text)
            except:
                return {}
        else:
            return {"text": result_text}

    except Exception as e:
        print(f"  GLM Error: {e}")
        return {}
    finally:
        # é™æµï¼šæ¯æ¬¡ API è°ƒç”¨åç­‰å¾…
        time.sleep(API_RATE_LIMIT_DELAY)


def analyze_with_perplexity(content: str, task: str = "extract", region: str = "ğŸ‡ºğŸ‡¸",
                            quota_remaining: dict = None, region_key: str = "us") -> dict:
    """
    ä½¿ç”¨ Perplexity Sonar æ¨¡å‹åˆ†æå†…å®¹
    
    ä¸ analyze_with_glm() æ¥å£ç›¸åŒï¼Œç”¨äºäº§å“æå–å’Œè¯„åˆ†ã€‚
    
    Args:
        content: è¦åˆ†æçš„å†…å®¹ï¼ˆæœç´¢ç»“æœæ–‡æœ¬ï¼‰
        task: ä»»åŠ¡ç±»å‹ (extract/score)
        region: åœ°åŒºæ ‡è¯† (emoji flag)
        quota_remaining: å‰©ä½™é…é¢ {"dark_horses": n, "rising_stars": m}
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea) ç”¨äºé€‰æ‹© prompt è¯­è¨€
        
    Returns:
        è§£æåçš„ JSONï¼ˆäº§å“åˆ—è¡¨æˆ–è¯„åˆ†ç»“æœï¼‰
    """
    client = get_perplexity_client()
    if not client:
        return {}
    
    if quota_remaining is None:
        quota_remaining = DAILY_QUOTA.copy()
    
    # æ„å»º prompt
    if task == "extract":
        prompt_template = get_extraction_prompt(region_key)
        prompt = prompt_template.format(
            search_results=content[:10000],
            region=region,
            quota_dark_horses=quota_remaining.get("dark_horses", 5),
            quota_rising_stars=quota_remaining.get("rising_stars", 10)
        )
    elif task == "score":
        prompt = SCORING_PROMPT.format(
            product=json.dumps(content, ensure_ascii=False, indent=2)
        ) if 'SCORING_PROMPT' in dir() else f"Score this product: {content}"
    else:
        return {}
    
    try:
        # ä½¿ç”¨ analyze æ–¹æ³• (Sonar Chat Completions)
        result = client.analyze(
            prompt=prompt,
            temperature=0.3,  # ä½æ¸©åº¦è·å¾—æ›´ç¨³å®šè¾“å‡º
            max_tokens=4096
        )
        return result if isinstance(result, (dict, list)) else {}
    
    except Exception as e:
        print(f"  âŒ Perplexity Analysis Error: {e}")
        return {}


# ============================================
# Provider Routing Functions
# ============================================

def get_provider_for_region(region_key: str) -> str:
    """Get provider name for region"""
    return REGION_PROVIDER_MAP.get(region_key, 'glm')


def search_with_provider(query: str, region_key: str, search_engine: str = "bing") -> list:
    """Route search to appropriate provider"""
    provider = get_provider_for_region(region_key)
    if provider == 'perplexity':
        return perplexity_search(query)
    else:
        return web_search_mcp(query, search_engine)


def analyze_with_provider(content, task: str, region_key: str, region_flag: str = "ğŸ‡ºğŸ‡¸",
                          quota_remaining: dict = None):
    """
    Route analysis to appropriate provider

    Args:
        content: è¦åˆ†æçš„å†…å®¹
        task: ä»»åŠ¡ç±»å‹ (extract/score)
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea) ç”¨äºé€‰æ‹© provider å’Œ prompt è¯­è¨€
        region_flag: åœ°åŒºæ ‡è¯† (emoji flag)
        quota_remaining: å‰©ä½™é…é¢
    """
    provider = get_provider_for_region(region_key)
    if provider == 'perplexity':
        return analyze_with_perplexity(content, task, region_flag, quota_remaining, region_key)
    else:
        return analyze_with_glm(content, task, region_flag, quota_remaining, region_key)


def fetch_url_content(url: str) -> str:
    """æŠ“å– URL å†…å®¹"""
    try:
        import urllib.request
        req = urllib.request.Request(
            url,
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        )
        with urllib.request.urlopen(req, timeout=15) as response:
            content = response.read().decode('utf-8', errors='ignore')

            # ç®€å•æå–æ­£æ–‡ï¼ˆå»é™¤ HTML æ ‡ç­¾ï¼‰
            content = re.sub(r'<script[^>]*>[\s\S]*?</script>', '', content)
            content = re.sub(r'<style[^>]*>[\s\S]*?</style>', '', content)
            content = re.sub(r'<[^>]+>', ' ', content)
            content = re.sub(r'\s+', ' ', content)
            return content[:15000]  # é™åˆ¶é•¿åº¦
    except Exception as e:
        print(f"  Fetch error: {e}")
        return ""


def search_with_glm(query: str, region: str = "ğŸ‡ºğŸ‡¸") -> list:
    """
    ä½¿ç”¨ GLM-4.7 çš„çŸ¥è¯†åº“æœç´¢ AI äº§å“

    GLM-4.7 æœ‰è¾ƒæ–°çš„çŸ¥è¯†ï¼Œå¯ä»¥ç›´æ¥è¯¢é—®æœ€æ–°çš„ AI äº§å“
    """
    client = get_zhipu_client()
    if not client:
        return []

    if region == "ğŸ‡¨ğŸ‡³":
        prompt = f"""è¯·åˆ—å‡ºæœ€è¿‘ï¼ˆ2024-2025å¹´ï¼‰{query}ç›¸å…³çš„ AI åˆ›ä¸šå…¬å¸/äº§å“ï¼Œç‰¹åˆ«æ˜¯ï¼š
- è·å¾—èèµ„çš„å…¬å¸
- æœ‰åˆ›æ–°äº§å“çš„å…¬å¸
- åœ¨è¡Œä¸šå†…æœ‰å½±å“åŠ›çš„å…¬å¸

è¿”å› JSON æ•°ç»„æ ¼å¼ï¼š
```json
[
  {{
    "name": "å…¬å¸/äº§å“å",
    "website": "å®˜ç½‘ï¼ˆå¦‚æœçŸ¥é“ï¼‰",
    "description": "ä¸€å¥è¯æè¿°",
    "funding": "èèµ„ä¿¡æ¯ï¼ˆå¦‚æœçŸ¥é“ï¼‰",
    "category": "åˆ†ç±»",
    "why_matters": "ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨"
  }}
]
```
åªè¿”å› JSONï¼Œè‡³å°‘è¿”å› 5 ä¸ªäº§å“ã€‚"""
    else:
        prompt = f"""List recent (2024-2025) AI startups/products related to {query}, especially:
- Companies that raised funding
- Companies with innovative products
- Influential companies in the industry

Return JSON array format:
```json
[
  {{
    "name": "Company/Product name",
    "website": "Website if known",
    "description": "One sentence description",
    "funding": "Funding info if known",
    "category": "Category",
    "why_matters": "Why it matters"
  }}
]
```
Return JSON only, at least 5 products."""

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            max_tokens=4096
        )

        result_text = response.choices[0].message.content

        # æå– JSON
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', result_text)
        if json_match:
            return json.loads(json_match.group(1))
        try:
            return json.loads(result_text)
        except:
            return []

    except Exception as e:
        print(f"  GLM Search Error: {e}")
        return []
    finally:
        time.sleep(API_RATE_LIMIT_DELAY)


def fetch_with_glm(source_config: dict, limit: int = 10) -> list:
    """
    ä½¿ç”¨ GLM-4.7 ä»æ¸ é“å‘ç°äº§å“

    ç­–ç•¥ï¼š
    1. å…ˆå°è¯•æŠ“å–ç½‘é¡µ
    2. å¦‚æœç½‘é¡µå†…å®¹ä¸è¶³ï¼Œä½¿ç”¨ GLM çŸ¥è¯†åº“æœç´¢
    3. ç”¨ GLM è¯„åˆ†
    """
    source_name = source_config['name']
    region = source_config['region']
    url = source_config.get('url', '')
    keywords = source_config.get('keywords', [])

    print(f"  Fetching: {url}")

    # æŠ“å–ç½‘é¡µå†…å®¹
    content = fetch_url_content(url)
    products = []

    if content and len(content) > 500:
        print(f"  Analyzing page content with GLM-4.7...")
        products = analyze_with_glm(content, task="extract")
        if not isinstance(products, list):
            products = []

    # å¦‚æœç½‘é¡µæå–å¤±è´¥ï¼Œä½¿ç”¨ GLM çŸ¥è¯†åº“æœç´¢
    if len(products) < 3:
        print(f"  Page content insufficient, using GLM knowledge search...")
        search_query = ' '.join(keywords[:3]) if keywords else source_name
        products = search_with_glm(search_query, region)
        if not isinstance(products, list):
            products = []

    print(f"  Found {len(products)} potential products")

    # è¡¥å……ä¿¡æ¯å¹¶è¯„åˆ†
    result = []
    for p in products[:limit]:
        # æ·»åŠ æ¥æºä¿¡æ¯
        p['source'] = source_name
        p['region'] = region
        p['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')

        # ç”¨ GLM è¯„åˆ†
        score_result = analyze_with_glm(p, task="score")
        if score_result:
            p['dark_horse_index'] = score_result.get('score', 2)
            if 'reason' in score_result:
                p['score_reason'] = score_result['reason']

        result.append(p)

    return result


def analyze_and_score(product: dict) -> dict:
    """
    ä½¿ç”¨ AI åˆ†æäº§å“å¹¶è¯„åˆ†

    è¯„åˆ†æ ‡å‡†ï¼š
    - 5åˆ†: èèµ„ >$100M æˆ– é¡¶çº§åˆ›å§‹äºº æˆ– å“ç±»å¼€åˆ›è€…
    - 4åˆ†: èèµ„ >$30M æˆ– YC/é¡¶çº§VC
    - 3åˆ†: èèµ„ >$5M æˆ– ProductHunt Top 5
    - 2åˆ†: æœ‰æ½œåŠ›ä½†æ•°æ®ä¸è¶³
    - 1åˆ†: è¾¹ç¼˜
    """
    funding = product.get('funding_total', '')
    source = product.get('source', '')

    # ç®€å•çš„è§„åˆ™è¯„åˆ†ï¼ˆå¯ä»¥æ›¿æ¢ä¸º AI è¯„åˆ†ï¼‰
    score = 2  # é»˜è®¤

    # è§£æèèµ„é‡‘é¢
    funding_amount = 0
    if funding:
        match = re.search(r'\$?([\d.]+)\s*([BMK])?', funding, re.I)
        if match:
            amount = float(match.group(1))
            unit = (match.group(2) or '').upper()
            if unit == 'B':
                funding_amount = amount * 1000
            elif unit == 'M':
                funding_amount = amount
            elif unit == 'K':
                funding_amount = amount / 1000
            else:
                funding_amount = amount

    # è¯„åˆ†é€»è¾‘
    if funding_amount >= 100:
        score = 5
    elif funding_amount >= 30:
        score = 4
    elif funding_amount >= 5:
        score = 3
    elif source in ['Y Combinator', 'ProductHunt']:
        score = 3

    product['dark_horse_index'] = score
    return product


def save_product(product: dict, dry_run: bool = False):
    """ä¿å­˜äº§å“åˆ°ç›¸åº”ç›®å½•"""
    score = product.get('dark_horse_index', 2)
    week = get_current_week()

    if score >= 4:
        # é»‘é©¬
        target_dir = DARK_HORSES_DIR
        target_file = os.path.join(target_dir, f'week_{week}.json')
    else:
        # æ½œåŠ›è‚¡
        target_dir = RISING_STARS_DIR
        target_file = os.path.join(target_dir, f'global_{week}.json')

    if dry_run:
        print(f"  [DRY RUN] Would save to: {target_file}")
        print(f"  {json.dumps(product, ensure_ascii=False, indent=2)}")
        return

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(target_dir, exist_ok=True)

    # åŠ è½½ç°æœ‰æ•°æ®
    if os.path.exists(target_file):
        with open(target_file, 'r', encoding='utf-8') as f:
            products = json.load(f)
    else:
        products = []

    # æ·»åŠ æ–°äº§å“
    products.append(product)

    # ä¿å­˜åˆ°åˆ†ç±»æ–‡ä»¶
    with open(target_file, 'w', encoding='utf-8') as f:
        json.dump(products, f, ensure_ascii=False, indent=2)

    print(f"  Saved to: {target_file}")
    
    # åŒæ—¶åŒæ­¥åˆ° products_featured.jsonï¼ˆå‰ç«¯æ•°æ®æºï¼‰
    sync_to_featured(product)


def sync_to_featured(product: dict):
    """
    åŒæ­¥äº§å“åˆ° products_featured.jsonï¼ˆå‰ç«¯æ•°æ®æºï¼‰
    
    è¿™æ ·å‘ç°çš„äº§å“å¯ä»¥ç›´æ¥åœ¨å‰ç«¯æ˜¾ç¤º
    """
    if product.get('dark_horse_index', 0) < 2:
        print(f"  â­ï¸ Skip featured (score < 2): {product.get('name')}")
        return
    featured_file = os.path.join(PROJECT_ROOT, 'data', 'products_featured.json')
    
    try:
        # åŠ è½½ç°æœ‰æ•°æ®
        if os.path.exists(featured_file):
            with open(featured_file, 'r', encoding='utf-8') as f:
                featured = json.load(f)
        else:
            featured = []
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆæŒ‰ website å»é‡ï¼‰
        existing_websites = {normalize_url(p.get('website', '')) for p in featured}
        product_domain = normalize_url(product.get('website', ''))
        
        if product_domain and product_domain in existing_websites:
            print(f"  ğŸ“‹ Already in featured: {product.get('name')}")
            return
        
        # è½¬æ¢å­—æ®µæ ¼å¼ï¼ˆé€‚é…å‰ç«¯ï¼‰
        featured_product = {
            'name': product.get('name'),
            'description': product.get('description'),
            'website': product.get('website'),
            'logo_url': product.get('logo', ''),
            'categories': [product.get('category', 'other')],
            'dark_horse_index': product.get('dark_horse_index', 2),
            'why_matters': product.get('why_matters', ''),
            'funding_total': product.get('funding_total', ''),
            'region': product.get('region', 'ğŸŒ'),
            'source': product.get('source', 'auto_discover'),
            'discovered_at': product.get('discovered_at', datetime.utcnow().strftime('%Y-%m-%d')),
            'first_seen': datetime.utcnow().isoformat() + 'Z',
            # è®¡ç®—åˆ†æ•°ï¼ˆç”¨äºæ’åºï¼‰
            'final_score': product.get('dark_horse_index', 2) * 20,
            'trending_score': product.get('dark_horse_index', 2) * 18,
        }
        
        # æ·»åŠ åˆ°åˆ—è¡¨å¼€å¤´ï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
        featured.insert(0, featured_product)
        
        # ä¿å­˜
        with open(featured_file, 'w', encoding='utf-8') as f:
            json.dump(featured, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… Synced to featured: {product.get('name')}")
        
    except Exception as e:
        print(f"  âš ï¸ Failed to sync to featured: {e}")


def discover_from_source(source_key: str, dry_run: bool = False):
    """ä»å•ä¸ªæ¸ é“å‘ç°äº§å“"""
    if source_key not in SOURCES:
        print(f"Unknown source: {source_key}")
        return

    config = SOURCES[source_key]
    print(f"\n{'='*50}")
    print(f"  Discovering from: {config['name']} {config['region']}")
    print(f"{'='*50}")

    existing = load_existing_products()

    # ä½¿ç”¨ GLM-4.7 å‘ç°äº§å“
    products = fetch_with_glm(config)

    new_count = 0
    for product in products:
        if is_duplicate(product.get('name', ''), product.get('website', ''), existing):
            print(f"  Skip duplicate: {product.get('name')}")
            continue

        # å¦‚æœ GLM æ²¡æœ‰è¯„åˆ†ï¼Œä½¿ç”¨è§„åˆ™è¯„åˆ†
        if 'dark_horse_index' not in product:
            product = analyze_and_score(product)

        save_product(product, dry_run)
        new_count += 1
        existing.add(product.get('name', '').lower())

    print(f"\n  Found {new_count} new products from {config['name']}")


def discover_all(dry_run: bool = False, tier: int = None):
    """ä»æ‰€æœ‰æ¸ é“å‘ç°äº§å“"""
    for source_key, config in SOURCES.items():
        if tier and config.get('tier', 1) > tier:
            continue
        discover_from_source(source_key, dry_run)


# ============================================
# æ–°å¢ï¼šåŸºäºåœ°åŒºçš„ Web Search å‘ç°
# ============================================

def discover_by_region(region_key: str, dry_run: bool = False, product_type: str = "mixed") -> dict:
    """
    ä½¿ç”¨ Web Search MCP æŒ‰åœ°åŒºå‘ç° AI äº§å“ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦è´¨é‡è¿‡æ»¤å’Œå…³é”®è¯è½®æ¢ï¼‰

    Args:
        region_key: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        dry_run: é¢„è§ˆæ¨¡å¼
        product_type: äº§å“ç±»å‹ (software/hardware/mixed)

    Returns:
        ç»Ÿè®¡ä¿¡æ¯
    """
    if region_key not in REGION_CONFIG:
        print(f"âŒ Unknown region: {region_key}")
        print(f"   Available: {', '.join(REGION_CONFIG.keys())}")
        return {"error": f"Unknown region: {region_key}"}

    config = REGION_CONFIG[region_key]
    region_name = config['name']
    search_engine = config['search_engine']

    # ä½¿ç”¨å…³é”®è¯è½®æ¢ï¼ˆæ”¯æŒäº§å“ç±»å‹ï¼‰
    keywords = get_keywords_for_today(region_key, product_type)

    # Get provider for this region
    provider = get_provider_for_region(region_key)

    type_label = {"software": "ğŸ’» è½¯ä»¶", "hardware": "ğŸ”§ ç¡¬ä»¶", "mixed": "ğŸ“Š æ··åˆ(40%ç¡¬ä»¶+60%è½¯ä»¶)"}.get(product_type, "æ··åˆ")
    
    print(f"\n{'='*60}")
    print(f"  ğŸŒ Discovering AI Products: {region_name}")
    print(f"  ğŸ“¡ Search Engine: {search_engine}")
    print(f"  ğŸ¤– Provider: {provider}")
    print(f"  ğŸ“¦ Product Type: {type_label}")
    print(f"  ğŸ”‘ Keywords: {len(keywords)} queries (day {datetime.now().weekday()})")
    print(f"{'='*60}")

    existing_names = load_existing_products()
    existing_domains = load_existing_domains()
    all_products = []
    quality_rejections = []

    stats = {
        "region": region_key,
        "region_name": region_name,
        "search_results": 0,
        "products_found": 0,
        "products_saved": 0,
        "dark_horses": 0,
        "rising_stars": 0,
        "duplicates_skipped": 0,
        "quality_rejections": 0,
    }

    # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
    for i, keyword in enumerate(keywords, 1):
        print(f"\n  [{i}/{len(keywords)}] Searching: {keyword[:50]}...")

        # 1. Search using provider routing
        search_results = search_with_provider(keyword, region_key, search_engine)
        stats["search_results"] += len(search_results)

        if not search_results:
            print(f"    âš ï¸ No results, using GLM knowledge...")
            search_results = search_with_glm(keyword, region_name)

        # å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºæ–‡æœ¬
        search_text = "\n\n".join([
            f"### {r.get('title', 'No Title')}\n"
            f"URL: {r.get('url', 'N/A')}\n"
            f"{r.get('content', r.get('snippet', ''))}"
            for r in search_results
        ])

        if not search_text.strip():
            continue

        # 2. Extract products using provider routing
        print(f"    ğŸ“Š Extracting products with {provider}...")

        region_flag_map = {
            'us': 'ğŸ‡ºğŸ‡¸', 'cn': 'ğŸ‡¨ğŸ‡³', 'eu': 'ğŸ‡ªğŸ‡º',
            'jp': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·', 'kr': 'ğŸ‡°ğŸ‡·', 'sea': 'ğŸ‡¸ğŸ‡¬'
        }
        region_flag = region_flag_map.get(region_key, 'ğŸŒ')

        products = analyze_with_provider(search_text, "extract", region_key, region_flag)

        if not isinstance(products, list):
            products = []

        print(f"    âœ… Extracted {len(products)} products")
        stats["products_found"] += len(products)

        # 3. å¯¹æ¯ä¸ªäº§å“è¯„åˆ†
        for product in products:
            name = product.get('name', '')
            if not name:
                continue

            # åŸŸåå»é‡
            domain = normalize_url(product.get('website', ''))
            if domain and domain in existing_domains:
                stats["duplicates_skipped"] += 1
                print(f"    â­ï¸ Skip duplicate domain: {domain}")
                continue

            # åç§°å»é‡
            if is_duplicate(name, product.get('website', ''), existing_names):
                stats["duplicates_skipped"] += 1
                print(f"    â­ï¸ Skip duplicate name: {name}")
                continue

            # è´¨é‡éªŒè¯
            is_valid, reason = validate_product(product)
            if not is_valid:
                stats["quality_rejections"] += 1
                quality_rejections.append({"name": name, "reason": reason})
                print(f"    âŒ Quality fail: {name} ({reason})")
                continue

            # è¡¥å……ä¿¡æ¯
            product['region'] = region_flag
            product['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')
            product['discovery_method'] = f'{provider}_search'
            product['search_keyword'] = keyword

            # 4. ä½¿ç”¨åˆå¹¶ prompt çš„è¯„åˆ†ï¼ˆæ— éœ€é¢å¤– API è°ƒç”¨ï¼‰
            # å¦‚æœæå–ç»“æœå·²åŒ…å« dark_horse_indexï¼Œç›´æ¥ä½¿ç”¨
            # å¦åˆ™ä½¿ç”¨è§„åˆ™è¯„åˆ†ä½œä¸º fallback
            score = product.get('dark_horse_index')
            if score is None:
                print(f"    ğŸ¯ Fallback scoring: {product.get('name')}...")
                product = analyze_and_score(product)
                score = product.get('dark_horse_index', 2)

            criteria = product.get('criteria_met', [])
            print(f"    ğŸ“ˆ Score: {score}/5 | Criteria: {criteria}")

            # 5. URL éªŒè¯ï¼ˆå¯é€‰ï¼Œè·³è¿‡ dry_run æ¨¡å¼ï¼‰
            website = product.get('website', '')
            if not dry_run and website and website.lower() != 'unknown':
                if not verify_url_exists(website, timeout=5):
                    print(f"    âš ï¸ URL not accessible: {website}")
                    product['needs_verification'] = True
                    # ä¸æ‹’ç»ï¼Œä½†æ ‡è®°éœ€è¦äººå·¥éªŒè¯

            # 6. ä¿å­˜äº§å“
            save_product(product, dry_run)
            stats["products_saved"] += 1

            if score >= 4:
                stats["dark_horses"] += 1
            else:
                stats["rising_stars"] += 1

            existing_names.add(name.lower())
            if domain:
                existing_domains.add(domain)
            all_products.append(product)

    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"  ğŸ“Š Summary for {region_name}")
    print(f"{'='*60}")
    print(f"  Search Results: {stats['search_results']}")
    print(f"  Products Found: {stats['products_found']}")
    print(f"  Products Saved: {stats['products_saved']}")
    print(f"  ğŸ‡ Dark Horses (4-5): {stats['dark_horses']}")
    print(f"  â­ Rising Stars (2-3): {stats['rising_stars']}")
    print(f"  Duplicates Skipped: {stats['duplicates_skipped']}")
    print(f"  Quality Rejections: {stats['quality_rejections']}")

    if quality_rejections:
        print(f"\n  Top rejection reasons:")
        reason_counts = {}
        for rej in quality_rejections:
            reason = rej['reason']
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
        for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1])[:3]:
            print(f"    - {reason}: {count}")

    return stats


def discover_all_regions(dry_run: bool = False, product_type: str = "mixed") -> dict:
    """
    å¸¦é…é¢ç³»ç»Ÿçš„å…¨çƒ AI äº§å“å‘ç°

    ç›®æ ‡é…é¢ï¼š
    - é»‘é©¬ (4-5åˆ†): 5 ä¸ª/å¤©
    - æ½œåŠ›è‚¡ (2-3åˆ†): 10 ä¸ª/å¤©
    
    Args:
        dry_run: é¢„è§ˆæ¨¡å¼
        product_type: äº§å“ç±»å‹ (software/hardware/mixed)

    Returns:
        è¯¦ç»†çš„å‘ç°æŠ¥å‘Š
    """
    start_time = datetime.now()
    today_str = start_time.strftime('%Y-%m-%d')

    type_label = {"software": "ğŸ’» è½¯ä»¶", "hardware": "ğŸ”§ ç¡¬ä»¶", "mixed": "ğŸ“Š æ··åˆ(40%ç¡¬ä»¶+60%è½¯ä»¶)"}.get(product_type, "æ··åˆ")
    
    print("\n" + "â•"*70)
    print(f"  ğŸŒ Daily AI Product Discovery - {today_str}")
    print("â•"*70)
    print(f"  ğŸ“Š Quota: {DAILY_QUOTA['dark_horses']} Dark Horses + {DAILY_QUOTA['rising_stars']} Rising Stars")
    print(f"  ğŸ“¦ Product Type: {type_label}")
    print(f"  ğŸ”„ Max Attempts: {MAX_ATTEMPTS} rounds")
    print(f"  ğŸ“… Keyword Pool: Day {datetime.now().weekday()} (0=Mon)")
    print(f"  ğŸ¤– Perplexity: {'enabled' if USE_PERPLEXITY else 'disabled'}")
    print("â•"*70)

    # åˆå§‹åŒ–è·Ÿè¸ª
    found = {"dark_horses": 0, "rising_stars": 0}
    region_yield = {k: 0 for k in REGION_CONFIG.keys()}
    provider_stats = {"glm": 0, "perplexity": 0}  # Track provider usage
    unique_domains = set()
    duplicates_skipped = 0
    quality_rejections = []
    attempts = 0

    # åŠ è½½å·²å­˜åœ¨çš„åŸŸå
    existing_domains = load_existing_domains()
    existing_names = load_existing_products()

    def quotas_met():
        return (found["dark_horses"] >= DAILY_QUOTA["dark_horses"] and
                found["rising_stars"] >= DAILY_QUOTA["rising_stars"])

    def get_category(score):
        return "dark_horses" if score >= 4 else "rising_stars"

    # ä¸»å‘ç°å¾ªç¯
    while not quotas_met() and attempts < MAX_ATTEMPTS:
        attempts += 1
        print(f"\n{'â”€'*70}")
        print(f"  ğŸ”„ Round {attempts}/{MAX_ATTEMPTS}")
        print(f"  Progress: DH {found['dark_horses']}/{DAILY_QUOTA['dark_horses']} | RS {found['rising_stars']}/{DAILY_QUOTA['rising_stars']}")
        print(f"{'â”€'*70}")

        # éšæœºåŒ–åœ°åŒºé¡ºåº
        region_order = get_region_order()

        for region_key in region_order:
            # æ£€æŸ¥åœ°åŒºé…é¢
            if region_yield[region_key] >= REGION_MAX.get(region_key, 3):
                print(f"\n  â­ï¸ Skip {region_key}: region max reached ({region_yield[region_key]})")
                continue

            # æ£€æŸ¥å…¨å±€é…é¢
            if quotas_met():
                break

            config = REGION_CONFIG[region_key]
            region_name = config['name']
            search_engine = config['search_engine']

            # è·å–ä»Šæ—¥å…³é”®è¯ï¼ˆå¸¦è½®æ¢ï¼Œæ”¯æŒäº§å“ç±»å‹ï¼‰
            keywords = get_keywords_for_today(region_key, product_type)
            # æ¯è½®åªå–éƒ¨åˆ†å…³é”®è¯ï¼Œé¿å…é‡å¤
            keywords_this_round = keywords[:2] if attempts > 1 else keywords

            # Get provider for this region
            provider = get_provider_for_region(region_key)
            print(f"\n  ğŸ“ {region_name} | Provider: {provider} | Keywords: {len(keywords_this_round)}")

            # è®¡ç®—å‰©ä½™é…é¢ï¼ˆä¼ ç»™ promptï¼‰
            quota_remaining = {
                "dark_horses": DAILY_QUOTA["dark_horses"] - found["dark_horses"],
                "rising_stars": DAILY_QUOTA["rising_stars"] - found["rising_stars"],
            }

            for keyword in keywords_this_round:
                if quotas_met():
                    break

                print(f"\n    ğŸ” Searching: {keyword[:50]}...")

                # 1. Search using provider routing
                search_results = search_with_provider(keyword, region_key, search_engine)

                if not search_results:
                    search_results = search_with_glm(keyword, region_name)

                if not search_results:
                    continue

                # æ ¼å¼åŒ–æœç´¢ç»“æœ
                search_text = "\n\n".join([
                    f"### {r.get('title', 'No Title')}\n"
                    f"URL: {r.get('url', 'N/A')}\n"
                    f"{r.get('content', r.get('snippet', ''))}"
                    for r in search_results
                ])

                if not search_text.strip():
                    continue

                # 2. Extract products using provider routing
                region_flag_map = {
                    'us': 'ğŸ‡ºğŸ‡¸', 'cn': 'ğŸ‡¨ğŸ‡³', 'eu': 'ğŸ‡ªğŸ‡º',
                    'jp': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·', 'kr': 'ğŸ‡°ğŸ‡·', 'sea': 'ğŸ‡¸ğŸ‡¬'
                }
                region_flag = region_flag_map.get(region_key, 'ğŸŒ')

                products = analyze_with_provider(
                    search_text,
                    "extract",
                    region_key,
                    region_flag,
                    quota_remaining
                )

                if not isinstance(products, list):
                    products = []

                print(f"    ğŸ“¦ Extracted: {len(products)} candidates")

                # 3. å¤„ç†æ¯ä¸ªäº§å“
                for product in products:
                    if quotas_met():
                        break

                    name = product.get('name', '')
                    if not name:
                        continue

                    # åŸŸåå»é‡
                    domain = normalize_url(product.get('website', ''))
                    if domain in existing_domains or domain in unique_domains:
                        duplicates_skipped += 1
                        print(f"    â­ï¸ Dup domain: {domain}")
                        continue

                    # åç§°å»é‡
                    if is_duplicate(name, product.get('website', ''), existing_names):
                        duplicates_skipped += 1
                        print(f"    â­ï¸ Dup name: {name}")
                        continue

                    # è´¨é‡éªŒè¯
                    is_valid, reason = validate_product(product)
                    if not is_valid:
                        quality_rejections.append({"name": name, "reason": reason})
                        print(f"    âŒ Quality fail: {name} ({reason})")
                        continue

                    # è¡¥å……å…ƒä¿¡æ¯
                    product['region'] = region_flag
                    product['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')
                    product['discovery_method'] = f'{provider}_search'
                    product['search_keyword'] = keyword

                    # ä½¿ç”¨åˆå¹¶ prompt çš„è¯„åˆ†ï¼ˆæ— éœ€é¢å¤– API è°ƒç”¨ï¼‰
                    # å¦‚æœæå–ç»“æœå·²åŒ…å« dark_horse_indexï¼Œç›´æ¥ä½¿ç”¨
                    # å¦åˆ™ä½¿ç”¨è§„åˆ™è¯„åˆ†ä½œä¸º fallback
                    score = product.get('dark_horse_index')
                    if score is None:
                        product = analyze_and_score(product)
                        score = product.get('dark_horse_index', 2)

                    category = get_category(score)

                    # æ£€æŸ¥åˆ†ç±»é…é¢
                    if found[category] >= DAILY_QUOTA[category]:
                        print(f"    â­ï¸ {category} quota full, skip: {name}")
                        continue

                    # æ£€æŸ¥åœ°åŒºé…é¢
                    if region_yield[region_key] >= REGION_MAX.get(region_key, 3):
                        print(f"    â­ï¸ Region max reached, skip: {name}")
                        continue

                    # ä¿å­˜
                    save_product(product, dry_run)

                    # æ›´æ–°è®¡æ•°
                    found[category] += 1
                    region_yield[region_key] += 1
                    provider_stats[provider] += 1  # Track provider usage
                    unique_domains.add(domain)
                    existing_names.add(name.lower())

                    status_icon = "ğŸ¦„" if category == "dark_horses" else "â­"
                    print(f"    {status_icon} SAVED: {name} (score={score}, {category}, {provider})")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    dh_status = "âœ…" if found["dark_horses"] >= DAILY_QUOTA["dark_horses"] else "âš ï¸"
    rs_status = "âœ…" if found["rising_stars"] >= DAILY_QUOTA["rising_stars"] else "âš ï¸"

    print("\n" + "â•"*70)
    print(f"  Daily Discovery Report - {today_str}")
    print("â•"*70)
    print(f"  Quotas:     Dark Horses: {found['dark_horses']}/{DAILY_QUOTA['dark_horses']} {dh_status}  Rising Stars: {found['rising_stars']}/{DAILY_QUOTA['rising_stars']} {rs_status}")
    print(f"  Attempts:   {attempts} rounds")
    print(f"  Duration:   {duration:.1f} seconds")
    print(f"  Regions:    {', '.join(f'{k}: {v}' for k, v in region_yield.items() if v > 0)}")
    print(f"  Providers:  {', '.join(f'{k}: {v}' for k, v in provider_stats.items() if v > 0)}")
    print(f"  Unique domains found: {len(unique_domains)}")
    print(f"  Duplicates skipped: {duplicates_skipped}")
    print(f"  Quality rejections: {len(quality_rejections)}")

    if quality_rejections:
        print("\n  Quality rejection reasons:")
        reason_counts = {}
        for rej in quality_rejections:
            reason = rej['reason']
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
        for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1])[:5]:
            print(f"    - {reason}: {count}")

    print("â•"*70)

    # è¿”å›æŠ¥å‘Šæ•°æ®
    return {
        "date": today_str,
        "found": found,
        "quota": DAILY_QUOTA,
        "attempts": attempts,
        "region_yield": region_yield,
        "provider_stats": provider_stats,
        "unique_domains": len(unique_domains),
        "duplicates_skipped": duplicates_skipped,
        "quality_rejections": len(quality_rejections),
        "duration_seconds": duration,
        "quotas_met": quotas_met(),
    }


def test_web_search():
    """æµ‹è¯• Web Search MCP è¿æ¥"""
    print("\n" + "="*60)
    print("  ğŸ” Testing Web Search MCP (Zhipu)")
    print("="*60)

    test_queries = [
        ("bing", "AI startup funding 2026"),
        ("sogou", "AIåˆ›ä¸šå…¬å¸ èèµ„ 2026"),
    ]

    for engine, query in test_queries:
        print(f"\n  Testing: {engine} - {query}")
        results = web_search_mcp(query, engine, count=3)

        if results:
            print(f"  âœ… Success! Found {len(results)} results")
            for i, r in enumerate(results[:2], 1):
                print(f"    {i}. {r.get('title', 'No Title')[:50]}...")
        else:
            print(f"  âš ï¸ No results (may fallback to GLM knowledge)")


def test_perplexity():
    """æµ‹è¯• Perplexity Search API è¿æ¥"""
    print("\n" + "="*60)
    print("  ğŸ” Testing Perplexity Search API")
    print("="*60)
    
    # æ£€æŸ¥ API Key
    if not PERPLEXITY_API_KEY:
        print("\n  âŒ PERPLEXITY_API_KEY not set")
        print("  Set it with: export PERPLEXITY_API_KEY=pplx_xxx")
        return
    
    print(f"  API Key: {PERPLEXITY_API_KEY[:12]}...")
    print(f"  Model: {PERPLEXITY_MODEL}")
    print(f"  USE_PERPLEXITY: {USE_PERPLEXITY}")
    
    # å°è¯•å¯¼å…¥æ–°æ¨¡å—
    try:
        from utils.perplexity_client import PerplexityClient
        client = PerplexityClient()
        print(f"  Client Status: {client.get_status()}")
    except ImportError as e:
        print(f"  âš ï¸ SDK not installed: {e}")
        print("  Install with: pip install perplexityai")
    
    # æµ‹è¯•æœç´¢
    test_queries = [
        ("us", "AI startup funding 2026"),
        ("cn", "AIèèµ„ 2026"),
    ]
    
    for region, query in test_queries:
        print(f"\n  ğŸ“ Testing region={region}: {query}")
        results = perplexity_search(query, count=3, region=region)
        
        if results:
            print(f"  âœ… Found {len(results)} results")
            for i, r in enumerate(results[:2], 1):
                title = r.get('title', 'No Title')[:50]
                url = r.get('url', 'N/A')[:60]
                print(f"    {i}. {title}...")
                print(f"       URL: {url}")
        else:
            print(f"  âš ï¸ No results")
    
    print("\n  âœ… Perplexity test completed!")


def setup_schedule():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆmacOS/Linuxï¼‰"""
    script_path = os.path.abspath(__file__)

    # ç”Ÿæˆ cron ä»»åŠ¡
    cron_line = f"0 9 * * * cd {PROJECT_ROOT} && /usr/bin/python3 {script_path} >> /tmp/auto_discover.log 2>&1"

    print("\nè®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©æ—©ä¸Š9ç‚¹è¿è¡Œï¼‰ï¼š")
    print("-" * 50)
    print("è¿è¡Œä»¥ä¸‹å‘½ä»¤æ·»åŠ  cron ä»»åŠ¡ï¼š")
    print(f"\n  (crontab -l 2>/dev/null; echo \"{cron_line}\") | crontab -")
    print("\næˆ–è€…ä½¿ç”¨ launchd (macOS)ï¼š")
    print(f"  åˆ›å»º ~/Library/LaunchAgents/com.weeklyai.autodiscover.plist")


def test_glm_connection():
    """æµ‹è¯• GLM-4.7 è¿æ¥"""
    print("\næµ‹è¯• GLM-4.7 è¿æ¥...")
    print(f"  API Key: {ZHIPU_API_KEY[:20]}...")
    print(f"  Model: {ZHIPU_MODEL}")

    client = get_zhipu_client()
    if not client:
        print("  âŒ æ— æ³•åˆ›å»ºå®¢æˆ·ç«¯ï¼Œè¯·å®‰è£…: pip install zhipuai")
        return False

    try:
        response = client.chat.completions.create(
            model=ZHIPU_MODEL,
            messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±"}],
            max_tokens=100
        )
        result = response.choices[0].message.content
        print(f"  âœ… è¿æ¥æˆåŠŸ!")
        print(f"  GLM å›å¤: {result}")
        return True
    except Exception as e:
        print(f"  âŒ è¿æ¥å¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description='è‡ªåŠ¨å‘ç°å…¨çƒ AI äº§å“ (v2.0 - Web Search MCP)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  # æŒ‰åœ°åŒºæœç´¢ï¼ˆæ¨èï¼Œä½¿ç”¨ Web Search MCPï¼‰
  python tools/auto_discover.py --region us      # æœç´¢ç¾å›½ AI äº§å“
  python tools/auto_discover.py --region cn      # æœç´¢ä¸­å›½ AI äº§å“
  python tools/auto_discover.py --region eu      # æœç´¢æ¬§æ´² AI äº§å“
  python tools/auto_discover.py --region jp      # æœç´¢æ—¥éŸ© AI äº§å“
  python tools/auto_discover.py --region sea     # æœç´¢ä¸œå—äºš AI äº§å“
  python tools/auto_discover.py --region all     # æœç´¢æ‰€æœ‰åœ°åŒº

  # æŒ‰æ¸ é“æœç´¢ï¼ˆæ—§æ–¹å¼ï¼‰
  python tools/auto_discover.py --source 36kr    # ä» 36æ°ª å‘ç°
  python tools/auto_discover.py --source producthunt

  # å…¶ä»–é€‰é¡¹
  python tools/auto_discover.py --dry-run        # é¢„è§ˆä¸ä¿å­˜
  python tools/auto_discover.py --test           # æµ‹è¯• GLM è¿æ¥
  python tools/auto_discover.py --test-search    # æµ‹è¯• Web Search MCP
"""
    )

    # æ–°å¢ï¼šåœ°åŒºå‚æ•°
    parser.add_argument('--region', '-r',
                        choices=['us', 'cn', 'eu', 'jp', 'sea', 'all'],
                        help='æŒ‰åœ°åŒºæœç´¢ (us/cn/eu/jp/sea/all)')
    
    # æ–°å¢ï¼šäº§å“ç±»å‹å‚æ•°
    parser.add_argument('--type', '-T',
                        choices=['software', 'hardware', 'mixed'],
                        default='mixed',
                        help='äº§å“ç±»å‹ (software/hardware/mixedï¼Œé»˜è®¤ mixed=40%%ç¡¬ä»¶+60%%è½¯ä»¶)')

    # åŸæœ‰å‚æ•°
    parser.add_argument('--source', '-s', help='æŒ‡å®šæ¸ é“ (e.g., 36kr, producthunt)')
    parser.add_argument('--tier', '-t', type=int, choices=[1, 2, 3], help='åªè¿è¡ŒæŒ‡å®šçº§åˆ«çš„æ¸ é“')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¿å­˜')
    parser.add_argument('--schedule', action='store_true', help='è®¾ç½®å®šæ—¶ä»»åŠ¡')
    parser.add_argument('--list-sources', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æ¸ é“')
    parser.add_argument('--list-regions', action='store_true', help='åˆ—å‡ºæ‰€æœ‰åœ°åŒº')
    parser.add_argument('--list-keywords', action='store_true', help='åˆ—å‡ºå…³é”®è¯ï¼ˆæŒ‰ç±»å‹ï¼‰')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯• GLM-4.7 è¿æ¥')
    parser.add_argument('--test-search', action='store_true', help='æµ‹è¯• Web Search MCP (Zhipu)')
    parser.add_argument('--test-perplexity', action='store_true', help='æµ‹è¯• Perplexity Search API')

    args = parser.parse_args()

    # æµ‹è¯•åŠŸèƒ½
    if args.test:
        test_glm_connection()
        return

    if args.test_search:
        test_web_search()
        return
    
    if args.test_perplexity:
        test_perplexity()
        return

    # åˆ—è¡¨åŠŸèƒ½
    if args.list_sources:
        print("\nå¯ç”¨æ¸ é“ï¼š")
        print("-" * 60)
        for key, config in SOURCES.items():
            print(f"  {key:15} {config['region']} {config['name']:20} Tier {config.get('tier', 1)}")
        return

    if args.list_regions:
        print("\nå¯ç”¨åœ°åŒºï¼š")
        print("-" * 60)
        for key, config in REGION_CONFIG.items():
            print(f"  {key:5} {config['name']:15} æƒé‡:{config['weight']:2}% æœç´¢å¼•æ“:{config['search_engine']}")
        return
    
    if args.list_keywords:
        region = args.region or 'us'
        print(f"\nå…³é”®è¯åˆ—è¡¨ (åœ°åŒº: {region})ï¼š")
        print("-" * 60)
        print("\nğŸ”§ ç¡¬ä»¶å…³é”®è¯:")
        for kw in get_hardware_keywords(region):
            print(f"  - {kw}")
        print("\nğŸ’» è½¯ä»¶å…³é”®è¯:")
        for kw in get_software_keywords(region):
            print(f"  - {kw}")
        print(f"\nğŸ“Š Mixed æ¨¡å¼å…³é”®è¯ (40%ç¡¬ä»¶ + 60%è½¯ä»¶):")
        for kw in get_keywords_for_today(region, "mixed"):
            print(f"  - {kw}")
        return

    if args.schedule:
        setup_schedule()
        return

    # å‘ç°åŠŸèƒ½
    if args.region:
        # æ–°æ–¹å¼ï¼šæŒ‰åœ°åŒºæœç´¢
        product_type = getattr(args, 'type', 'mixed')
        if args.region == 'all':
            discover_all_regions(args.dry_run, product_type)
        else:
            discover_by_region(args.region, args.dry_run, product_type)
    elif args.source:
        # æ—§æ–¹å¼ï¼šæŒ‰æ¸ é“æœç´¢
        discover_from_source(args.source, args.dry_run)
    else:
        # é»˜è®¤ï¼šè¿è¡Œæ‰€æœ‰åœ°åŒºçš„ Web Search
        print("\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ --region å‚æ•°è¿›è¡Œåœ°åŒºæœç´¢ï¼ˆæ¨èï¼‰")
        print("   ç¤ºä¾‹: python tools/auto_discover.py --region us")
        print("   æˆ–è€…: python tools/auto_discover.py --region all")
        print("\n   ä½¿ç”¨ --source å‚æ•°è¿›è¡Œæ—§æ¸ é“æœç´¢")
        print("   ç¤ºä¾‹: python tools/auto_discover.py --source 36kr")
        print("\nè¿è¡Œ --help æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹")


if __name__ == '__main__':
    main()
