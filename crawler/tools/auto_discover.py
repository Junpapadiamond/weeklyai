#!/usr/bin/env python3
"""
è‡ªåŠ¨å‘ç°å…¨çƒ AI äº§å“ (v2.0 - Perplexity Search)

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Perplexity Search API å®æ—¶æœç´¢å…¨çƒ AI äº§å“
2. æŒ‰åœ°åŒºåˆ†é…æœç´¢ä»»åŠ¡ (ç¾å›½40%/ä¸­å›½25%/æ¬§æ´²15%/æ—¥éŸ©10%/ä¸œå—äºš10%)
3. ä½¿ç”¨ä¸“ä¸š Prompt æå–äº§å“ä¿¡æ¯å¹¶è¯„åˆ†
4. è‡ªåŠ¨åˆ†ç±»åˆ°é»‘é©¬(4-5åˆ†)/æ½œåŠ›è‚¡(2-3åˆ†)

ç”¨æ³•ï¼š
    python tools/auto_discover.py                    # è¿è¡Œæ‰€æœ‰åœ°åŒº
    python tools/auto_discover.py --region us       # åªæœç´¢ç¾å›½
    python tools/auto_discover.py --region cn       # åªæœç´¢ä¸­å›½
    python tools/auto_discover.py --dry-run         # é¢„è§ˆä¸ä¿å­˜
"""

import json
import os
import sys
import argparse
import re
import requests
import time
from datetime import datetime
from urllib.parse import urlparse
from typing import Any, Dict, Optional, Tuple, List

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼ˆç”¨äºå¯¼å…¥ utilsï¼‰
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ–°çš„å»é‡æ¨¡å—
try:
    from utils.dedup import DuplicateChecker, get_domain_key, normalize_name
    USE_NEW_DEDUP = True
except ImportError:
    USE_NEW_DEDUP = False
    print("âš ï¸  æ–°å»é‡æ¨¡å—æœªåŠ è½½ï¼Œä½¿ç”¨æ—§é€»è¾‘")

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv
    # æŸ¥æ‰¾ .env æ–‡ä»¶ï¼ˆåœ¨ crawler ç›®å½•ä¸‹ï¼‰
    env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
    if os.path.exists(env_path):
        load_dotenv(env_path)
        print(f"âœ… Loaded .env from {env_path}")
except ImportError:
    pass  # dotenv æœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡

# Perplexity API é…ç½®
PERPLEXITY_API_KEY = os.environ.get('PERPLEXITY_API_KEY', '')
PERPLEXITY_MODEL = os.environ.get('PERPLEXITY_MODEL', 'sonar')  # sonar or sonar-pro

# æ™ºè°± GLM API é…ç½® (ä¸­å›½åŒº)
ZHIPU_API_KEY = os.environ.get('ZHIPU_API_KEY', '')
GLM_MODEL = os.environ.get('GLM_MODEL', 'glm-4.7')  # æœ€æ–°: glm-4.7 (200K context, 128K output)
GLM_SEARCH_ENGINE = os.environ.get('GLM_SEARCH_ENGINE', 'search_pro')
USE_GLM_FOR_CN = os.environ.get('USE_GLM_FOR_CN', 'true').lower() == 'true'

# Demand signals (HN + X)
ENABLE_DEMAND_SIGNALS = os.environ.get('ENABLE_DEMAND_SIGNALS', 'true').lower() == 'true'
DEMAND_WINDOW_DAYS = int(os.environ.get('DEMAND_WINDOW_DAYS', '7'))
DEMAND_MAX_PRODUCTS_PER_RUN = int(os.environ.get('DEMAND_MAX_PRODUCTS_PER_RUN', '25'))
DEMAND_OVERRIDE_MODE = os.environ.get('DEMAND_OVERRIDE_MODE', 'medium').strip().lower()
DEMAND_GITHUB_MAX_STAR_PAGES = int(os.environ.get('DEMAND_GITHUB_MAX_STAR_PAGES', '6'))
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', '')
DEFAULT_OFFICIAL_HANDLES_FILE = os.path.join(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
    'data',
    'product_official_handles.json'
)
PRODUCT_OFFICIAL_HANDLES_FILE = os.environ.get(
    'PRODUCT_OFFICIAL_HANDLES_FILE',
    DEFAULT_OFFICIAL_HANDLES_FILE
)

# Provider routing (åŠ¨æ€é€‰æ‹©)
PROVIDER_NAME = "perplexity"  # é»˜è®¤ providerï¼Œå®é™…æŒ‰åŒºåŸŸåŠ¨æ€é€‰æ‹©

# ============================================
# å›½å®¶å½’å±è§£æï¼ˆå…¬å¸å½’å±ä¼˜å…ˆï¼‰
# ============================================
UNKNOWN_COUNTRY_CODE = "UNKNOWN"
UNKNOWN_COUNTRY_NAME = "Unknown"
UNKNOWN_COUNTRY_DISPLAY = "Unknown"

COUNTRY_CODE_TO_NAME = {
    "US": "United States",
    "CN": "China",
    "SG": "Singapore",
    "JP": "Japan",
    "KR": "South Korea",
    "GB": "United Kingdom",
    "DE": "Germany",
    "FR": "France",
    "SE": "Sweden",
    "CA": "Canada",
    "IL": "Israel",
    "BE": "Belgium",
    "AE": "United Arab Emirates",
    "NL": "Netherlands",
    "CH": "Switzerland",
    "IN": "India",
}

COUNTRY_CODE_TO_FLAG = {
    "US": "ğŸ‡ºğŸ‡¸",
    "CN": "ğŸ‡¨ğŸ‡³",
    "SG": "ğŸ‡¸ğŸ‡¬",
    "JP": "ğŸ‡¯ğŸ‡µ",
    "KR": "ğŸ‡°ğŸ‡·",
    "GB": "ğŸ‡¬ğŸ‡§",
    "DE": "ğŸ‡©ğŸ‡ª",
    "FR": "ğŸ‡«ğŸ‡·",
    "SE": "ğŸ‡¸ğŸ‡ª",
    "CA": "ğŸ‡¨ğŸ‡¦",
    "IL": "ğŸ‡®ğŸ‡±",
    "BE": "ğŸ‡§ğŸ‡ª",
    "AE": "ğŸ‡¦ğŸ‡ª",
    "NL": "ğŸ‡³ğŸ‡±",
    "CH": "ğŸ‡¨ğŸ‡­",
    "IN": "ğŸ‡®ğŸ‡³",
}

COUNTRY_NAME_ALIASES = {
    "us": "US",
    "usa": "US",
    "united states": "US",
    "u.s.": "US",
    "america": "US",
    "ç¾å›½": "US",
    "cn": "CN",
    "china": "CN",
    "prc": "CN",
    "ä¸­å›½": "CN",
    "sg": "SG",
    "singapore": "SG",
    "æ–°åŠ å¡": "SG",
    "jp": "JP",
    "japan": "JP",
    "æ—¥æœ¬": "JP",
    "kr": "KR",
    "korea": "KR",
    "south korea": "KR",
    "éŸ©å›½": "KR",
    "gb": "GB",
    "uk": "GB",
    "united kingdom": "GB",
    "britain": "GB",
    "england": "GB",
    "è‹±å›½": "GB",
    "de": "DE",
    "germany": "DE",
    "å¾·å›½": "DE",
    "fr": "FR",
    "france": "FR",
    "æ³•å›½": "FR",
    "se": "SE",
    "sweden": "SE",
    "ç‘å…¸": "SE",
    "ca": "CA",
    "canada": "CA",
    "åŠ æ‹¿å¤§": "CA",
    "il": "IL",
    "israel": "IL",
    "ä»¥è‰²åˆ—": "IL",
    "be": "BE",
    "belgium": "BE",
    "æ¯”åˆ©æ—¶": "BE",
    "ae": "AE",
    "uae": "AE",
    "united arab emirates": "AE",
    "é˜¿è”é…‹": "AE",
    "nl": "NL",
    "netherlands": "NL",
    "è·å…°": "NL",
    "ch": "CH",
    "switzerland": "CH",
    "ç‘å£«": "CH",
    "in": "IN",
    "india": "IN",
    "å°åº¦": "IN",
}

FLAG_TO_COUNTRY_CODE = {flag: code for code, flag in COUNTRY_CODE_TO_FLAG.items()}

# è¿™ç»„ flag åœ¨å‘ç°é˜¶æ®µé€šå¸¸ä»£è¡¨â€œæœç´¢å¸‚åœºâ€ï¼Œä¸æ˜¯å…¬å¸å½’å±å›½
DISCOVERY_REGION_FLAGS = {"ğŸ‡ºğŸ‡¸", "ğŸ‡¨ğŸ‡³", "ğŸ‡ªğŸ‡º", "ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·", "ğŸ‡¸ğŸ‡¬", "ğŸŒ"}
REGION_DERIVED_COUNTRY_SOURCES = {"region:search_fallback", "region:fallback"}

COUNTRY_BY_CC_TLD = {
    "cn": "CN",
    "jp": "JP",
    "kr": "KR",
    "de": "DE",
    "fr": "FR",
    "se": "SE",
    "ca": "CA",
    "uk": "GB",
    "sg": "SG",
    "il": "IL",
    "be": "BE",
    "ae": "AE",
    "nl": "NL",
    "ch": "CH",
    "in": "IN",
}


def _extract_region_flag(value: Any) -> str:
    text = str(value or "").strip()
    if not text:
        return ""
    match = re.search(r"[\U0001F1E6-\U0001F1FF]{2}", text)
    return match.group(0) if match else ""


def _normalize_country_code(value: Any) -> str:
    text = str(value or "").strip()
    if not text:
        return ""

    upper = text.upper()
    if upper in COUNTRY_CODE_TO_NAME:
        return upper

    flag = _extract_region_flag(text)
    if flag and flag in FLAG_TO_COUNTRY_CODE:
        return FLAG_TO_COUNTRY_CODE[flag]

    normalized = re.sub(r"[_\-.]+", " ", text.lower()).strip()
    normalized = re.sub(r"\s+", " ", normalized)
    return COUNTRY_NAME_ALIASES.get(normalized, "")


def _country_code_from_website_tld(website: Any) -> str:
    raw = str(website or "").strip()
    if not raw:
        return ""
    if not raw.startswith(("http://", "https://")):
        raw = f"https://{raw}"

    try:
        host = (urlparse(raw).netloc or "").lower()
        host = host.split(":")[0]
        if host.startswith("www."):
            host = host[4:]
        if not host or "." not in host:
            return ""
        suffix = host.rsplit(".", 1)[-1]
        return COUNTRY_BY_CC_TLD.get(suffix, "")
    except Exception:
        return ""


def resolve_company_country(
    product: Dict[str, Any],
    fallback_region_flag: str = "",
) -> Tuple[str, str]:
    """
    è§£æäº§å“å…¬å¸å½’å±å›½ã€‚

    ä¼˜å…ˆçº§ï¼š
    1) å…¬å¸/åˆ›å§‹ç›¸å…³æ˜¾å¼å­—æ®µ
    2) ç­–å±•æ•°æ®çš„ regionï¼ˆä»… curatedï¼Œè§†ä¸ºäººå·¥ç¡®è®¤ï¼‰
    3) éå‘ç°é˜¶æ®µçš„ legacy regionï¼ˆå¦‚ ğŸ‡©ğŸ‡ª / ğŸ‡«ğŸ‡· è¿™ç±»å•å›½æ——ï¼‰
    4) å®˜ç½‘ ccTLDï¼ˆä»…å¼ºæŒ‡å‘å›½å®¶ï¼‰
    5) Unknown
    """
    country_source_hint = str(product.get("country_source") or "").strip().lower()
    skip_region_derived_country_fields = country_source_hint in REGION_DERIVED_COUNTRY_SOURCES

    explicit_fields = [
        "company_country_code",
        "company_country",
        "hq_country_code",
        "hq_country",
        "headquarters_country",
        "origin_country",
        "founder_country",
        "country_code",
        "country_name",
        "country",
        "nationality",
    ]

    for field in explicit_fields:
        if skip_region_derived_country_fields and field in {"country_code", "country_name", "country"}:
            # å…¼å®¹å†å²æ•°æ®ï¼šè¿™äº›å­—æ®µå¯èƒ½ç”±æ—§ç‰ˆ "region:search_fallback" è¯¯æ¨æ–­è€Œæ¥
            continue
        code = _normalize_country_code(product.get(field))
        if code:
            return code, f"explicit:{field}"

    extra = product.get("extra")
    if isinstance(extra, dict):
        for field in explicit_fields:
            if skip_region_derived_country_fields and field in {"country_code", "country_name", "country"}:
                continue
            code = _normalize_country_code(extra.get(field))
            if code:
                return code, f"extra:{field}"

    for field in ("country_flag", "company_country_flag", "hq_country_flag"):
        if skip_region_derived_country_fields and field == "country_flag":
            continue
        code = _normalize_country_code(product.get(field))
        if code:
            return code, f"explicit:{field}"

    source = str(product.get("source") or "").strip().lower()
    region_flag = _extract_region_flag(product.get("region"))
    if source == "curated" and region_flag:
        code = FLAG_TO_COUNTRY_CODE.get(region_flag, "")
        if code:
            return code, "curated:region"

    # å‘ç°é˜¶æ®µçš„ region å±äºâ€œæ£€ç´¢å¸‚åœºâ€ä¿¡å·ï¼Œé»˜è®¤ä¸ç›´æ¥å½“å…¬å¸å›½å®¶
    if region_flag and region_flag not in DISCOVERY_REGION_FLAGS:
        code = FLAG_TO_COUNTRY_CODE.get(region_flag, "")
        if code:
            return code, "region:legacy"

    if fallback_region_flag and fallback_region_flag not in DISCOVERY_REGION_FLAGS:
        code = FLAG_TO_COUNTRY_CODE.get(fallback_region_flag, "")
        if code:
            return code, "region:fallback"

    cc_tld_code = _country_code_from_website_tld(product.get("website"))
    if cc_tld_code:
        return cc_tld_code, "website:cc_tld"

    return "", "unknown"


def apply_country_fields(product: Dict[str, Any], fallback_region_flag: str = "") -> None:
    """
    ä¸ºäº§å“å†™å…¥ç»Ÿä¸€å›½å®¶å­—æ®µï¼Œå¹¶ä¿è¯æœªçŸ¥æ—¶ä¸è¾“å‡ºé”™è¯¯å›½æ——ã€‚
    """
    if fallback_region_flag:
        product["source_region"] = fallback_region_flag
    elif not product.get("source_region"):
        existing_region = str(product.get("region") or "").strip()
        if existing_region:
            product["source_region"] = existing_region

    code, country_source = resolve_company_country(product, fallback_region_flag=fallback_region_flag)
    if code:
        country_name = COUNTRY_CODE_TO_NAME.get(code, code)
        country_flag = COUNTRY_CODE_TO_FLAG.get(code, "")
        country_display = f"{country_flag} {country_name}".strip()
        product["country_code"] = code
        product["country_name"] = country_name
        product["country_flag"] = country_flag
        product["country_display"] = country_display
        product["country_source"] = country_source
        product["region"] = country_flag or country_name
        return

    product["country_code"] = UNKNOWN_COUNTRY_CODE
    product["country_name"] = UNKNOWN_COUNTRY_NAME
    product["country_flag"] = ""
    product["country_display"] = UNKNOWN_COUNTRY_DISPLAY
    product["country_source"] = "unknown"
    product["region"] = UNKNOWN_COUNTRY_DISPLAY


# ============================================
# æ¯æ—¥é…é¢ç³»ç»Ÿ
# ============================================
import random

DAILY_QUOTA = {
    "dark_horses": 5,      # 4-5 åˆ†é»‘é©¬äº§å“
    "rising_stars": 10,    # 2-3 åˆ†æ½œåŠ›è‚¡
}

# æ¯åœ°åŒºæœ€å¤§äº§å“æ•°ï¼ˆé˜²æ­¢å•ä¸€åœ°åŒºä¸»å¯¼ï¼‰
REGION_MAX = {
    "us": 6, "cn": 4, "eu": 3, "jp": 2, "kr": 2, "sea": 2
}

MAX_ATTEMPTS = 3  # æœ€å¤§æœç´¢è½®æ•°

# GLM å¹¶å‘/èŠ‚æµé…ç½®ï¼ˆä¸­å›½åŒºï¼‰
GLM_KEYWORD_DELAY = float(os.environ.get('GLM_KEYWORD_DELAY', '3'))  # æ¯ä¸ªå…³é”®è¯ä¹‹é—´çš„é¢å¤–ç­‰å¾…ç§’æ•°
MAX_KEYWORDS_CN = int(os.environ.get('AUTO_DISCOVER_MAX_KEYWORDS_CN', '4'))  # 0=ä¸é™åˆ¶
MAX_KEYWORDS_DEFAULT = int(os.environ.get('AUTO_DISCOVER_MAX_KEYWORDS', '0'))  # 0=ä¸é™åˆ¶

# æˆæœ¬ä¼˜åŒ–é…ç½®
AUTO_DISCOVER_BUDGET_MODE = os.environ.get('AUTO_DISCOVER_BUDGET_MODE', 'adaptive').strip().lower()
if AUTO_DISCOVER_BUDGET_MODE not in {'adaptive', 'legacy'}:
    AUTO_DISCOVER_BUDGET_MODE = 'adaptive'
AUTO_DISCOVER_ROUND1_KEYWORDS = max(1, int(os.environ.get('AUTO_DISCOVER_ROUND1_KEYWORDS', '2')))
AUTO_DISCOVER_ROUND_EXPAND_STEP = max(1, int(os.environ.get('AUTO_DISCOVER_ROUND_EXPAND_STEP', '2')))
AUTO_DISCOVER_ENABLE_ANALYZE_GATE = os.environ.get('AUTO_DISCOVER_ENABLE_ANALYZE_GATE', 'true').lower() == 'true'
AUTO_DISCOVER_QUALITY_FALLBACK = os.environ.get('AUTO_DISCOVER_QUALITY_FALLBACK', 'true').lower() == 'true'
AUTO_DISCOVER_PROMPT_MAX_CHARS = max(1200, int(os.environ.get('AUTO_DISCOVER_PROMPT_MAX_CHARS', '6000')))
AUTO_DISCOVER_RESULT_SNIPPET_MAX_CHARS = max(120, int(os.environ.get('AUTO_DISCOVER_RESULT_SNIPPET_MAX_CHARS', '320')))

# ============================================
# å¤šè¯­è¨€å…³é”®è¯åº“ï¼ˆåŸç”Ÿè¯­è¨€æœç´¢æ•ˆæœæ›´å¥½ï¼‰
# ============================================

# è½¯ä»¶ AI å…³é”®è¯
KEYWORDS_SOFTWARE = {
    "us": [
        "AI startup funding 2026",
        "YC AI companies winter 2026",
        "AI Series A 2026",
        "artificial intelligence company raised funding",
        "AI unicorn startup valuation 2026",
        "AI agent startup funding",
        "generative AI startup Series A",
    ],
    "cn": [
        "AIèèµ„ 2026",
        "äººå·¥æ™ºèƒ½åˆ›ä¸šå…¬å¸",
        "AIGCèèµ„",
        "å¤§æ¨¡å‹åˆ›ä¸š",
        "AIåˆ›ä¸šå…¬å¸ Aè½® Bè½®",
        "äººå·¥æ™ºèƒ½ ç‹¬è§’å…½ ä¼°å€¼",
        "AI Agent åˆ›ä¸šå…¬å¸",
    ],
    "eu": [
        "European AI startup funding 2026",
        "KI Startup Finanzierung",
        "AI Series A Europe",
        "UK France Germany AI startup",
    ],
    "jp": [
        "AI ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— è³‡é‡‘èª¿é” 2026",
        "æ—¥æœ¬ AIä¼æ¥­ ã‚·ãƒªãƒ¼ã‚ºA",
        "äººå·¥çŸ¥èƒ½ ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—",
        "Japan AI startup funding",
    ],
    "kr": [
        "AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì 2026",
        "í•œêµ­ ì¸ê³µì§€ëŠ¥ ê¸°ì—…",
        "AI ì‹œë¦¬ì¦ˆA",
        "Korean AI startup investment",
    ],
    "sea": [
        "Singapore AI startup funding 2026",
        "Southeast Asia AI company",
        "AI startup Indonesia Vietnam",
        "Tech in Asia artificial intelligence",
    ],
}

# ç¡¬ä»¶ AI å…³é”®è¯ï¼ˆä¸“é—¨æœç´¢ç¡¬ä»¶äº§å“ï¼‰
# åˆ†ä¸ºä¸¤ç±»ï¼šä¼ ç»Ÿç¡¬ä»¶ï¼ˆèŠ¯ç‰‡/æœºå™¨äººï¼‰+ åˆ›æ–°å½¢æ€ï¼ˆå¯ç©¿æˆ´/æ–°å½¢æ€ï¼‰
KEYWORDS_HARDWARE = {
    "us": [
        # ä¼ ç»Ÿç¡¬ä»¶ï¼šèŠ¯ç‰‡/æœºå™¨äºº
        "AI chip startup funding 2026",
        "humanoid robot company funding",
        "AI semiconductor startup investment",
        "robotics AI company raised funding",
        # åˆ›æ–°å½¢æ€ç¡¬ä»¶ï¼šå¯ç©¿æˆ´/æ–°å½¢æ€ (Friend Pendant ç±»)
        "AI pendant necklace wearable 2026",
        "AI companion device startup",
        "AI pin badge wearable assistant",
        "AI ring wearable startup",
        "AI glasses startup 2026",
        "AI wearable gadget viral",
        "AI hardware kickstarter indiegogo 2026",
        "AI assistant device form factor innovative",
        "screenless AI device wearable",
    ],
    "cn": [
        # ä¼ ç»Ÿç¡¬ä»¶
        "AIèŠ¯ç‰‡ åˆ›ä¸šå…¬å¸ èèµ„",
        "äººå½¢æœºå™¨äºº åˆ›ä¸šå…¬å¸",
        "å…·èº«æ™ºèƒ½ åˆ›ä¸šå…¬å¸",
        # åˆ›æ–°å½¢æ€ç¡¬ä»¶
        "AIæ™ºèƒ½çœ¼é•œ åˆ›ä¸šå…¬å¸",
        "AIå¯ç©¿æˆ´è®¾å¤‡ åˆ›ä¸šå…¬å¸ 2026",
        "AIé¡¹é“¾ åŠå  æ™ºèƒ½è®¾å¤‡",
        "AIæˆ’æŒ‡ æ™ºèƒ½ç©¿æˆ´",
        "AIç¡¬ä»¶ ä¼—ç­¹ åˆ›æ–°",
        "AIéšèº«è®¾å¤‡ åŠ©æ‰‹",
    ],
    "eu": [
        "European AI chip startup funding",
        "robotics startup Europe funding",
        # åˆ›æ–°å½¢æ€
        "AI wearable startup Europe 2026",
        "AI glasses pendant Europe startup",
    ],
    "jp": [
        "AIåŠå°ä½“ ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— è³‡é‡‘èª¿é”",
        "ãƒ­ãƒœãƒƒãƒˆ AIä¼æ¥­ æ—¥æœ¬",
        # åˆ›æ–°å½¢æ€
        "AIã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ« ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— æ—¥æœ¬",
        "AIãƒ¡ã‚¬ãƒ ãƒ‡ãƒã‚¤ã‚¹ æ—¥æœ¬",
    ],
    "kr": [
        "AI ë°˜ë„ì²´ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì",
        "ë¡œë´‡ AI ê¸°ì—… í•œêµ­",
        # åˆ›æ–°å½¢æ€
        "AI ì›¨ì–´ëŸ¬ë¸” ìŠ¤íƒ€íŠ¸ì—… í•œêµ­",
    ],
    "sea": [
        "AI hardware startup Singapore",
        "robotics company Southeast Asia",
        # åˆ›æ–°å½¢æ€
        "AI wearable device startup Asia 2026",
    ],
}

# å…¼å®¹æ—§ä»£ç çš„åˆ«å
KEYWORDS_BY_REGION = KEYWORDS_SOFTWARE

# ============================================
# ç«™ç‚¹å®šå‘æœç´¢ï¼ˆç›´æ¥æœç´¢ç›®æ ‡åª’ä½“ï¼‰
# ============================================
SITE_SEARCHES = {
    "us": [
        # ç§‘æŠ€åª’ä½“
        "site:techcrunch.com AI startup funding",
        "site:venturebeat.com AI funding",
        "site:wired.com AI hardware device",
        "site:theverge.com AI wearable gadget",
        # äº§å“å‘ç°å¹³å° (åˆ›æ–°å½¢æ€ç¡¬ä»¶é‡ç‚¹)
        "site:producthunt.com AI hardware wearable pendant 2026",
        "site:producthunt.com AI device companion assistant 2026",
        # ä¼—ç­¹å¹³å° (æ—©æœŸåˆ›æ–°ç¡¬ä»¶)
        "site:kickstarter.com AI wearable pendant necklace 2026",
        "site:kickstarter.com AI glasses ring device 2026",
        "site:indiegogo.com AI wearable assistant 2026",
    ],
    "cn": [
        "site:36kr.com AIèèµ„",
        "site:tmtpost.com äººå·¥æ™ºèƒ½",
        "site:jiqizhixin.com èèµ„",
        # ç¡¬ä»¶åˆ›æ–°
        "site:36kr.com AIç¡¬ä»¶ å¯ç©¿æˆ´ æ™ºèƒ½è®¾å¤‡ 2026",
        "site:36kr.com å…·èº«æ™ºèƒ½ äººå½¢æœºå™¨äºº 2026",
        "site:36kr.com AIçœ¼é•œ æ™ºèƒ½ç©¿æˆ´ 2026",
    ],
    "eu": [
        "site:sifted.eu AI funding",
        "site:tech.eu AI startup",
        "site:eu-startups.com AI",
        # åˆ›æ–°ç¡¬ä»¶
        "site:kickstarter.com AI wearable Europe 2026",
    ],
    "jp": [
        "site:thebridge.jp AI startup",
        "site:jp.techcrunch.com AI",
        # åˆ›æ–°ç¡¬ä»¶
        "site:kickstarter.com AI wearable Japan 2026",
    ],
    "kr": [
        "site:platum.kr AI ìŠ¤íƒ€íŠ¸ì—…",
        "site:besuccess.com AI",
    ],
    "sea": [
        "site:e27.co AI startup",
        "site:techinasia.com AI funding",
        "site:kickstarter.com AI wearable Asia 2026",
    ],
}

def get_keywords_for_today(region: str, product_type: str = "mixed") -> list:
    """
    æ ¹æ®æ—¥æœŸè½®æ¢å…³é”®è¯æ± 
    
    Args:
        region: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        product_type: äº§å“ç±»å‹ ("software"/"hardware"/"mixed")

    ç­–ç•¥ï¼š
    - mixed æ¨¡å¼ä¸‹ç¡¬ä»¶:è½¯ä»¶ = 40%:60%
    - æ¯å¤©è½®æ¢ä¸åŒçš„å…³é”®è¯ç»„åˆ
    """
    day = datetime.now().weekday()

    if product_type == "hardware":
        # åªè¿”å›ç¡¬ä»¶å…³é”®è¯
        keywords = KEYWORDS_HARDWARE.get(region, KEYWORDS_HARDWARE["us"])
    elif product_type == "software":
        # åªè¿”å›è½¯ä»¶å…³é”®è¯
        keywords = KEYWORDS_SOFTWARE.get(region, KEYWORDS_SOFTWARE["us"])
    else:
        # mixed æ¨¡å¼ï¼š40% ç¡¬ä»¶ + 60% è½¯ä»¶
        hw_keywords = KEYWORDS_HARDWARE.get(region, KEYWORDS_HARDWARE["us"])
        sw_keywords = KEYWORDS_SOFTWARE.get(region, KEYWORDS_SOFTWARE["us"])
        site_searches = SITE_SEARCHES.get(region, [])
        
        # è®¡ç®—æ•°é‡ï¼šç¡¬ä»¶ 40%ï¼Œè½¯ä»¶ 60%
        hw_count = max(2, len(hw_keywords) * 2 // 5)  # è‡³å°‘ 2 ä¸ªç¡¬ä»¶å…³é”®è¯
        sw_count = max(3, len(sw_keywords) * 3 // 5)  # è‡³å°‘ 3 ä¸ªè½¯ä»¶å…³é”®è¯
        
        # æ ¹æ®æ˜ŸæœŸå‡ è½®æ¢
        hw_start = (day * 2) % max(1, len(hw_keywords))
        sw_start = (day * 2) % max(1, len(sw_keywords))
        
        hw_selected = (hw_keywords[hw_start:] + hw_keywords[:hw_start])[:hw_count]
        sw_selected = (sw_keywords[sw_start:] + sw_keywords[:sw_start])[:sw_count]
        
        keywords = hw_selected + sw_selected + site_searches[:1]

    # éšæœºæ‰“ä¹±é¡ºåº
    shuffled = keywords.copy()
    random.shuffle(shuffled)
    return shuffled


def get_hardware_keywords(region: str) -> list:
    """è·å–ç¡¬ä»¶ä¸“ç”¨å…³é”®è¯"""
    return KEYWORDS_HARDWARE.get(region, KEYWORDS_HARDWARE["us"])


def is_hardware_query_text(query: str) -> bool:
    """åŸºäºå…³é”®è¯åˆ¤æ–­æ˜¯å¦ä¸ºç¡¬ä»¶æŸ¥è¯¢ï¼ˆæ··åˆæ¨¡å¼è·¯ç”±ç”¨ï¼‰"""
    q = query.lower()
    hardware_terms = [
        "hardware", "robot", "robotics", "chip", "semiconductor", "wearable",
        "glasses", "ring", "pendant", "device", "gadget", "embodied", "edge",
        "smart glasses", "kickstarter", "indiegogo", "crowdfunding",
        "ç¡¬ä»¶", "æœºå™¨äºº", "äººå½¢æœºå™¨äºº", "èŠ¯ç‰‡", "åŠå¯¼ä½“", "å…·èº«æ™ºèƒ½", "æ™ºèƒ½çœ¼é•œ",
        "å¯ç©¿æˆ´", "åŠå ", "æˆ’æŒ‡", "è®¾å¤‡", "ä¼—ç­¹",
    ]
    return any(term in q for term in hardware_terms)


def resolve_keyword_type(keyword: str, region_key: str, product_type: str) -> str:
    """æ··åˆæ¨¡å¼ä¸‹æŒ‰å…³é”®è¯è·¯ç”±ç¡¬ä»¶/è½¯ä»¶ prompt"""
    if product_type != "mixed":
        return product_type
    hw_keywords = set(get_hardware_keywords(region_key))
    if keyword in hw_keywords or is_hardware_query_text(keyword):
        return "hardware"
    return "software"

def get_software_keywords(region: str) -> list:
    """è·å–è½¯ä»¶ä¸“ç”¨å…³é”®è¯"""
    return KEYWORDS_SOFTWARE.get(region, KEYWORDS_SOFTWARE["us"])

def get_region_order() -> list:
    """éšæœºåŒ–åœ°åŒºæœç´¢é¡ºåºï¼Œé¿å…å›ºå®šåå·®"""
    regions = list(REGION_CONFIG.keys())
    random.shuffle(regions)
    return regions

# ============================================
# åœ°åŒºé…ç½® (æŒ‰æ¯”ä¾‹åˆ†é…æœç´¢ä»»åŠ¡)
# ============================================
REGION_CONFIG = {
    'us': {
        'name': 'ğŸ‡ºğŸ‡¸ ç¾å›½',
        'weight': 40,  # 40%
        'search_engine': 'bing',
        'keywords': [
            'AI startup funding Series A B 2026',
            'artificial intelligence company raised funding',
            'YC AI startup demo day 2026',
            'AI unicorn startup valuation',
        ],
    },
    'cn': {
        'name': 'ğŸ‡¨ğŸ‡³ ä¸­å›½',
        'weight': 25,  # 25%
        'search_engine': 'sogou',
        'keywords': [
            'AIåˆ›ä¸šå…¬å¸ èèµ„ AIGC å¤§æ¨¡å‹ è·æŠ•',
            'äººå·¥æ™ºèƒ½ åˆåˆ›å…¬å¸ Aè½® Bè½® èèµ„',
            'å¤§æ¨¡å‹ åˆ›ä¸šå…¬å¸ ä¼°å€¼ èèµ„æ–°é—»',
            'AIGC ç‹¬è§’å…½ èèµ„ 2026',
        ],
    },
    'eu': {
        'name': 'ğŸ‡ªğŸ‡º æ¬§æ´²',
        'weight': 15,  # 15%
        'search_engine': 'bing',
        'keywords': [
            'European AI startup funding Sifted',
            'Europe artificial intelligence company raised',
            'UK France Germany AI startup Series A',
        ],
    },
    'jp': {
        'name': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡· æ—¥éŸ©',
        'weight': 10,  # 10%
        'search_engine': 'bing',
        'keywords': [
            'Japan Korea AI startup funding',
            'Japanese artificial intelligence company raised',
            'Korean AI startup investment',
        ],
    },
    'sea': {
        'name': 'ğŸ‡¸ğŸ‡¬ ä¸œå—äºš',
        'weight': 10,  # 10%
        'search_engine': 'bing',
        'keywords': [
            'Southeast Asia AI startup e27 funding',
            'Singapore Indonesia Vietnam AI company raised',
            'Tech in Asia artificial intelligence funding',
        ],
    },
}

# ============================================
# é¡¹ç›®è·¯å¾„è®¾ç½® (å¿…é¡»åœ¨å¯¼å…¥ prompts ä¹‹å‰)
# ============================================
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)
AUTO_DISCOVER_LOCK_FILE = os.environ.get(
    'AUTO_DISCOVER_LOCK_FILE',
    os.path.join(PROJECT_ROOT, 'logs', 'auto_discover.lock')
)
KEYWORD_YIELD_STATS_FILE = os.path.join(PROJECT_ROOT, 'data', 'metrics', 'keyword_yield_stats.json')

try:
    from utils.demand_signals import DemandSignalEngine, apply_demand_guardrail
    HAS_DEMAND_SIGNALS = True
except Exception as e:
    HAS_DEMAND_SIGNALS = False
    print(f"âš ï¸ demand_signals module not available: {e}")


def apply_keyword_limit(region_key: str, keywords: List[str]) -> List[str]:
    keyword_limit = 0
    if region_key == "cn" and MAX_KEYWORDS_CN > 0:
        keyword_limit = MAX_KEYWORDS_CN
    elif MAX_KEYWORDS_DEFAULT > 0:
        keyword_limit = MAX_KEYWORDS_DEFAULT
    if keyword_limit and len(keywords) > keyword_limit:
        return keywords[:keyword_limit]
    return keywords


def build_search_text(search_results: List[dict], snippet_limit: int = AUTO_DISCOVER_RESULT_SNIPPET_MAX_CHARS) -> str:
    blocks = []
    for r in search_results:
        title = str(r.get('title', 'No Title') or 'No Title').strip()
        url = str(r.get('url', 'N/A') or 'N/A').strip()
        date_text = str(r.get('date', '') or '').strip()
        raw_snippet = str(r.get('content', r.get('snippet', '')) or '').strip()
        snippet = raw_snippet[:snippet_limit]
        lines = [f"### {title}", f"URL: {url}"]
        if date_text:
            lines.append(f"Date: {date_text}")
        lines.append(snippet)
        blocks.append("\n".join(lines))
    return "\n\n".join(blocks)


def _extract_domain_from_result(result: dict) -> str:
    raw_url = str(result.get("url", "") or "").strip()
    if not raw_url:
        return ""
    try:
        parsed = urlparse(raw_url)
        host = (parsed.netloc or "").lower()
        if host.startswith("www."):
            host = host[4:]
        return host
    except Exception:
        return ""


def should_analyze_search_results(search_results: List[dict], query: str) -> Tuple[bool, str]:
    query_text = str(query or "").lower()
    if "site:" in query_text:
        return True, "site_query_bypass"
    if len(search_results) < 3:
        return False, "too_few_results"

    domains = {_extract_domain_from_result(r) for r in search_results if _extract_domain_from_result(r)}
    if len(domains) < 2:
        return False, "too_few_domains"

    signal_terms = [
        "raise", "raised", "raises", "funding", "series a", "series b", "series c",
        "launch", "launched", "release", "released", "preview", "beta", "introduc", "unveil",
        "èèµ„", "è·æŠ•", "aè½®", "bè½®", "å‘å¸ƒ", "æ¨å‡º", "ä¸Šçº¿", "ä¼—ç­¹",
    ]
    signal_blob = " ".join(
        f"{(r.get('title') or '').lower()} {(r.get('content', r.get('snippet', '')) or '').lower()}"
        for r in search_results
    )
    if not any(term in signal_blob for term in signal_terms):
        return False, "missing_signal_terms"
    return True, "signal_ok"


def _safe_load_json_dict(path: str) -> Dict[str, Any]:
    if not path or not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            return data
    except Exception:
        pass
    return {}


def _safe_save_json_dict(path: str, payload: Dict[str, Any]) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def load_keyword_yield_stats() -> Dict[str, Any]:
    data = _safe_load_json_dict(KEYWORD_YIELD_STATS_FILE)
    if not data:
        return {
            "meta": {"updated_at": "", "version": "v1"},
            "keywords": {},
        }
    if not isinstance(data.get("keywords"), dict):
        data["keywords"] = {}
    if not isinstance(data.get("meta"), dict):
        data["meta"] = {"updated_at": "", "version": "v1"}
    return data


def rank_keywords_by_yield(region_key: str, keywords: List[str], stats: Dict[str, Any]) -> List[str]:
    if not keywords:
        return []
    region_stats = (stats.get("keywords") or {}).get(region_key, {})
    if not isinstance(region_stats, dict):
        return list(keywords)

    scored = []
    for idx, keyword in enumerate(keywords):
        row = region_stats.get(keyword) if isinstance(region_stats, dict) else None
        if not isinstance(row, dict):
            scored.append((0.0, 0, idx, keyword))
            continue
        saved = int(row.get("saved", 0) or 0)
        dark = int(row.get("dark_horses", 0) or 0)
        searches = int(row.get("searches", 0) or 0)
        precision = (saved / searches) if searches > 0 else 0.0
        dh_boost = (dark / max(saved, 1))
        score = precision * 0.7 + dh_boost * 0.3
        scored.append((score, saved, idx, keyword))

    scored.sort(key=lambda x: (x[0], x[1], -x[2]), reverse=True)
    return [row[3] for row in scored]


def update_keyword_yield_stats(
    stats: Dict[str, Any],
    *,
    region_key: str,
    keyword: str,
    searches: int = 0,
    extracted: int = 0,
    saved: int = 0,
    dark_horses: int = 0,
) -> None:
    if not keyword:
        return
    keywords_bucket = stats.setdefault("keywords", {})
    if not isinstance(keywords_bucket, dict):
        keywords_bucket = {}
        stats["keywords"] = keywords_bucket
    region_bucket = keywords_bucket.setdefault(region_key, {})
    if not isinstance(region_bucket, dict):
        region_bucket = {}
        keywords_bucket[region_key] = region_bucket
    row = region_bucket.setdefault(keyword, {})
    if not isinstance(row, dict):
        row = {}
        region_bucket[keyword] = row

    row["searches"] = int(row.get("searches", 0) or 0) + int(max(searches, 0))
    row["extracted"] = int(row.get("extracted", 0) or 0) + int(max(extracted, 0))
    row["saved"] = int(row.get("saved", 0) or 0) + int(max(saved, 0))
    row["dark_horses"] = int(row.get("dark_horses", 0) or 0) + int(max(dark_horses, 0))
    row["last_seen"] = datetime.utcnow().strftime("%Y-%m-%d")


def flush_keyword_yield_stats(stats: Dict[str, Any]) -> None:
    meta = stats.get("meta")
    if not isinstance(meta, dict):
        meta = {}
        stats["meta"] = meta
    meta["updated_at"] = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
    meta["version"] = "v1"
    _safe_save_json_dict(KEYWORD_YIELD_STATS_FILE, stats)

# ============================================
# Prompt æ¨¡å— (ç‹¬ç«‹ä¼˜åŒ–çš„æœç´¢å’Œåˆ†æ Prompt)
# ============================================

# å¯¼å…¥æ¨¡å—åŒ– Prompt
try:
    from prompts.search_prompts import (
        generate_search_queries,
        generate_discovery_query,
        get_search_params,
        SEARCH_QUERIES_BY_REGION,
    )
    from prompts.analysis_prompts import (
        ANALYSIS_PROMPT_EN,
        ANALYSIS_PROMPT_CN,
        SCORING_PROMPT,
        get_analysis_prompt,
        get_scoring_prompt,
        get_hardware_analysis_prompt,
        validate_hardware_product,
        WELL_KNOWN_PRODUCTS as PROMPT_WELL_KNOWN,
        GENERIC_WHY_MATTERS as PROMPT_GENERIC,
    )
    USE_MODULAR_PROMPTS = True
    print("âœ… Loaded modular prompts from prompts/")
except ImportError as e:
    USE_MODULAR_PROMPTS = False
    print(f"âš ï¸ prompts/ module not found: {e}")

# Fallback: å†…è” Promptï¼ˆå½“æ¨¡å—æœªåŠ è½½æ—¶ä½¿ç”¨ï¼‰
if not USE_MODULAR_PROMPTS:
    # è‹±æ–‡ç‰ˆ Prompt (us/eu/jp/kr/sea)
    ANALYSIS_PROMPT_EN = """You are WeeklyAI's AI Product Analyst. Extract and score AI products from search results.

## Search Results
{search_results}

## STRICT EXCLUSIONS (Never Include):
- Well-Known: ChatGPT, Claude, Gemini, Copilot, DALL-E, Sora, Midjourney, Cursor, Perplexity
- Not Products: LangChain, PyTorch, papers only, tool directories
- Big Tech: Google Gemini, Meta Llama, Microsoft Copilot

## DARK HORSE (4-5) - Must meet â‰¥2:
| growth_anomaly | founder_background | funding_signal | category_innovation | community_buzz |

**5 points**: Funding >$100M OR Top-tier founder OR Category creator
**4 points**: Funding >$30M OR YC/a16z backed OR ARR >$10M

## RISING STAR (2-3) - Need 1:
**3 points**: Funding $1M-$5M OR ProductHunt top 10
**2 points**: Just launched, clear innovation

## CRITICAL: why_matters must have specific numbers!
âœ… GOOD: "Sequoiaé¢†æŠ•$50Mï¼Œ8ä¸ªæœˆARRä»0åˆ°$10M"
âŒ BAD: "This is a promising AI product"

## CRITICAL: Company Country Verification
- `region` is search market only, not company nationality.
- Fill `company_country` using evidence from search results.
- If uncertain, set `company_country` to "unknown" and lower confidence.

## Output (JSON only)
```json
[{{"name": "...", "website": "https://...", "description": "ä¸­æ–‡æè¿°(>20å­—)", "category": "coding|image|video|...", "region": "{region}", "company_country": "US|CN|unknown", "company_country_confidence": 0.8, "funding_total": "$50M", "dark_horse_index": 4, "criteria_met": ["funding_signal"], "why_matters": "å…·ä½“æ•°å­—+å·®å¼‚åŒ–", "source": "...", "confidence": 0.85}}]
```

Quota: Dark Horses: {quota_dark_horses} | Rising Stars: {quota_rising_stars}
Return [] if nothing qualifies."""

    # ä¸­æ–‡ç‰ˆ Prompt (cn)
    ANALYSIS_PROMPT_CN = """ä½ æ˜¯ WeeklyAI çš„ AI äº§å“åˆ†æå¸ˆã€‚ä»æœç´¢ç»“æœä¸­æå–å¹¶è¯„åˆ† AI äº§å“ã€‚

## æœç´¢ç»“æœ
{search_results}

## ä¸¥æ ¼æ’é™¤ï¼š
- å·²çŸ¥å: ChatGPT, Claude, Gemini, Cursor, Kimi, è±†åŒ…, é€šä¹‰åƒé—®, æ–‡å¿ƒä¸€è¨€
- éäº§å“: LangChain, PyTorch, åªæœ‰è®ºæ–‡/demo
- å¤§å‚: Google Gemini, ç™¾åº¦æ–‡å¿ƒ, é˜¿é‡Œé€šä¹‰

## é»‘é©¬ (4-5åˆ†) - æ»¡è¶³â‰¥2æ¡:
| growth_anomaly | founder_background | funding_signal | category_innovation | community_buzz |

**5åˆ†**: èèµ„>$100M æˆ– é¡¶çº§åˆ›å§‹äºº æˆ– å“ç±»å¼€åˆ›è€…
**4åˆ†**: èèµ„>$30M æˆ– YC/a16zèƒŒä¹¦ æˆ– ARR>$10M

## æ½œåŠ›è‚¡ (2-3åˆ†) - æ»¡è¶³1æ¡:
**3åˆ†**: èèµ„$1M-$5M æˆ– ProductHunt Top 10
**2åˆ†**: åˆšå‘å¸ƒä½†æœ‰æ˜æ˜¾åˆ›æ–°

## why_matters å¿…é¡»æœ‰å…·ä½“æ•°å­—!
âœ… GOOD: "Sequoiaé¢†æŠ•$50Mï¼Œ8ä¸ªæœˆARRä»0åˆ°$10M"
âŒ BAD: "è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ½œåŠ›çš„AIäº§å“"

## å…³é”®ï¼šå…¬å¸å›½ç±æ ¡éªŒ
- `region` åªæ˜¯æœç´¢å¸‚åœºï¼Œä¸æ˜¯å…¬å¸å›½ç±ã€‚
- æ ¹æ®æœç´¢ç»“æœè¯æ®å¡«å†™ `company_country`ã€‚
- ä¸ç¡®å®šæ—¶å¿…é¡»å¡« `"company_country": "unknown"` å¹¶é™ä½ç½®ä¿¡åº¦ã€‚

## è¾“å‡º (ä»…JSON)
```json
[{{"name": "äº§å“å", "website": "https://...", "description": "ä¸­æ–‡æè¿°(>20å­—)", "category": "coding|image|video|...", "region": "{region}", "company_country": "US|CN|unknown", "company_country_confidence": 0.8, "funding_total": "$50M", "dark_horse_index": 4, "criteria_met": ["funding_signal"], "why_matters": "å…·ä½“æ•°å­—+å·®å¼‚åŒ–", "source": "...", "confidence": 0.85}}]
```

é…é¢: é»‘é©¬: {quota_dark_horses} | æ½œåŠ›è‚¡: {quota_rising_stars}
æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„è¿”å› []ã€‚"""

    # è¯„åˆ† Prompt
    SCORING_PROMPT = """è¯„ä¼°äº§å“çš„"é»‘é©¬æŒ‡æ•°"(1-5åˆ†)ï¼š

## äº§å“
{product}

## è¯„åˆ†æ ‡å‡†
5åˆ†: èèµ„>$100M æˆ– é¡¶çº§åˆ›å§‹äººèƒŒæ™¯ æˆ– å“ç±»å¼€åˆ›è€…
4åˆ†: èèµ„>$30M æˆ– YC/a16zæŠ•èµ„ æˆ– ARR>$10M
3åˆ†: èèµ„$5M-$30M æˆ– ProductHunt Top 5
2åˆ†: æœ‰åˆ›æ–°ç‚¹ä½†æ•°æ®ä¸è¶³
1åˆ†: è¾¹ç¼˜äº§å“æˆ–å¾…éªŒè¯

## è¿”å›æ ¼å¼ï¼ˆä»…JSONï¼‰
```json
{{"dark_horse_index": 4, "criteria_met": ["funding_signal"], "reason": "è¯„åˆ†ç†ç”±"}}
```"""


def get_extraction_prompt(region_key: str) -> str:
    """
    æ ¹æ®åœ°åŒºé€‰æ‹©åˆé€‚çš„åˆ†æ prompt
    
    Args:
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea)

    Returns:
        å¯¹åº”åœ°åŒºçš„ prompt æ¨¡æ¿
    """
    if region_key == "cn":
        return ANALYSIS_PROMPT_CN
    else:
        return ANALYSIS_PROMPT_EN


# åˆ«åï¼šå…¼å®¹æ—§ä»£ç 
PROMPT_EXTRACTION_EN = ANALYSIS_PROMPT_EN if not USE_MODULAR_PROMPTS else ANALYSIS_PROMPT_EN
PROMPT_EXTRACTION_CN = ANALYSIS_PROMPT_CN if not USE_MODULAR_PROMPTS else ANALYSIS_PROMPT_CN
PROMPT_DARK_HORSE_SCORING = SCORING_PROMPT if not USE_MODULAR_PROMPTS else SCORING_PROMPT

# æ•°æ®æ–‡ä»¶è·¯å¾„
DARK_HORSES_DIR = os.path.join(PROJECT_ROOT, 'data', 'dark_horses')
RISING_STARS_DIR = os.path.join(PROJECT_ROOT, 'data', 'rising_stars')
CANDIDATES_DIR = os.path.join(PROJECT_ROOT, 'data', 'candidates')


def acquire_process_lock(lock_path: str):
    """å•å®ä¾‹é”ï¼Œé¿å…å¹¶å‘è¿è¡Œå¯¼è‡´ API å¹¶å‘è¶…é™"""
    try:
        import fcntl
    except ImportError:
        print("âš ï¸ fcntl not available; skipping process lock")
        return None, True

    os.makedirs(os.path.dirname(lock_path), exist_ok=True)
    lock_file = open(lock_path, 'w')
    try:
        fcntl.flock(lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)
    except BlockingIOError:
        lock_file.close()
        return None, False

    lock_file.write(f"{os.getpid()}\n{datetime.utcnow().isoformat()}Z\n")
    lock_file.flush()
    return lock_file, True

# æ¸ é“é…ç½®
SOURCES = {
    # ç¾å›½æ¸ é“
    'techcrunch': {
        'name': 'TechCrunch',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://techcrunch.com/category/artificial-intelligence/',
        'rss': 'https://techcrunch.com/category/artificial-intelligence/feed/',
        'keywords': ['raises', 'Series A', 'Series B', 'funding', 'AI startup'],
        'tier': 1,
    },
    'producthunt': {
        'name': 'ProductHunt',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://www.producthunt.com/topics/artificial-intelligence',
        'api': 'https://api.producthunt.com/v2/api/graphql',
        'keywords': ['AI', 'machine learning', 'LLM'],
        'tier': 2,
    },
    'ycombinator': {
        'name': 'Y Combinator',
        'region': 'ğŸ‡ºğŸ‡¸',
        'url': 'https://www.ycombinator.com/companies?tags=AI',
        'keywords': ['YC', 'Demo Day'],
        'tier': 1,
    },

    # ä¸­å›½æ¸ é“
    '36kr': {
        'name': '36æ°ª',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://36kr.com/information/AI/',
        'rss': 'https://36kr.com/feed',
        'keywords': ['AIèèµ„', 'äººå·¥æ™ºèƒ½', 'AIGC', 'å¤§æ¨¡å‹', 'è·æŠ•'],
        'tier': 1,
    },
    'itjuzi': {
        'name': 'ITæ¡”å­',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://www.itjuzi.com/investevent',
        'keywords': ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ '],
        'tier': 1,
    },
    'jiqizhixin': {
        'name': 'æœºå™¨ä¹‹å¿ƒ',
        'region': 'ğŸ‡¨ğŸ‡³',
        'url': 'https://www.jiqizhixin.com/',
        'rss': 'https://www.jiqizhixin.com/rss',
        'keywords': ['AI', 'èèµ„', 'åˆ›ä¸š'],
        'tier': 2,
    },

    # æ¬§æ´²æ¸ é“
    'sifted': {
        'name': 'Sifted',
        'region': 'ğŸ‡ªğŸ‡º',
        'url': 'https://sifted.eu/sector/artificial-intelligence',
        'keywords': ['AI', 'funding', 'European startup'],
        'tier': 1,
    },
    'eu_startups': {
        'name': 'EU-Startups',
        'region': 'ğŸ‡ªğŸ‡º',
        'url': 'https://www.eu-startups.com/category/artificial-intelligence/',
        'rss': 'https://www.eu-startups.com/feed/',
        'keywords': ['AI', 'raises', 'funding'],
        'tier': 2,
    },

    # æ—¥éŸ©æ¸ é“
    'bridge': {
        'name': 'Bridge',
        'region': 'ğŸ‡¯ğŸ‡µ',
        'url': 'https://thebridge.jp/en/',
        'keywords': ['AI', 'startup', 'funding', 'Japan'],
        'tier': 1,
    },
    'platum': {
        'name': 'Platum',
        'region': 'ğŸ‡°ğŸ‡·',
        'url': 'https://platum.kr/archives/category/ai',
        'keywords': ['AI', 'startup', 'Korea'],
        'tier': 1,
    },

    # ä¸œå—äºšæ¸ é“
    'e27': {
        'name': 'e27',
        'region': 'ğŸ‡¸ğŸ‡¬',
        'url': 'https://e27.co/tag/artificial-intelligence/',
        'keywords': ['AI', 'Southeast Asia', 'funding'],
        'tier': 1,
    },
    'techinasia': {
        'name': 'Tech in Asia',
        'region': 'ğŸ‡¸ğŸ‡¬',
        'url': 'https://www.techinasia.com/tag/artificial-intelligence',
        'keywords': ['AI', 'Asia', 'startup'],
        'tier': 1,
    },
}


def get_current_week():
    """è·å–å½“å‰å‘¨æ•°"""
    now = datetime.now()
    return f"{now.year}_{now.isocalendar()[1]:02d}"


def load_existing_products():
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„äº§å“åç§°å’Œç½‘å€"""
    existing = set()

    # åŠ è½½é»‘é©¬
    if os.path.exists(DARK_HORSES_DIR):
        for f in os.listdir(DARK_HORSES_DIR):
            if f.endswith('.json'):
                with open(os.path.join(DARK_HORSES_DIR, f), 'r') as file:
                    products = json.load(file)
                    for p in products:
                        existing.add(p.get('name', '').lower())
                        existing.add(p.get('website', '').lower())

    # åŠ è½½æ½œåŠ›è‚¡
    if os.path.exists(RISING_STARS_DIR):
        for f in os.listdir(RISING_STARS_DIR):
            if f.endswith('.json'):
                with open(os.path.join(RISING_STARS_DIR, f), 'r') as file:
                    products = json.load(file)
                    for p in products:
                        existing.add(p.get('name', '').lower())
                        existing.add(p.get('website', '').lower())

    return existing


def is_duplicate(name: str, website: str, existing: set) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦é‡å¤ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰
    
    ä½¿ç”¨åç§°å’Œç½‘ç«™çš„ç²¾ç¡®åŒ¹é…
    """
    return name.lower() in existing or website.lower() in existing


def normalize_url(url: str) -> str:
    """
    æ ‡å‡†åŒ– URLï¼Œæå–ä¸»åŸŸåç”¨äºå»é‡

    "https://www.example.com/page" â†’ "example.com"
    """
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.replace("www.", "")
        return domain.lower()
    except:
        return url.lower()


# ============================================
# å¢å¼ºå»é‡æ£€æŸ¥å™¨ï¼ˆä½¿ç”¨æ–°æ¨¡å—ï¼‰
# ============================================

class EnhancedDuplicateChecker:
    """
    å¢å¼ºçš„å»é‡æ£€æŸ¥å™¨
    
    ç»“åˆæ–°çš„ dedup æ¨¡å—å’Œæ—§çš„é€»è¾‘
    """
    
    def __init__(self, existing_products: list):
        """
        åˆå§‹åŒ–æ£€æŸ¥å™¨
        
        Args:
            existing_products: å·²æœ‰äº§å“åˆ—è¡¨
        """
        self.existing_products = existing_products
        
        # ä½¿ç”¨æ–°æ¨¡å—
        if USE_NEW_DEDUP:
            self.checker = DuplicateChecker(
                existing_products,
                similarity_threshold=0.90,
                check_similarity=True
            )
        else:
            self.checker = None
        
        # æ—§ç´¢å¼•ï¼ˆä½œä¸º fallbackï¼‰
        self.existing_names = set()
        self.existing_domains = set()
        
        for p in existing_products:
            name = p.get('name', '').lower().strip()
            if name:
                self.existing_names.add(name)
            
            website = p.get('website', '')
            if website:
                domain = normalize_url(website)
                if domain:
                    self.existing_domains.add(domain)
    
    def is_duplicate(self, product: dict) -> tuple:
        """
        æ£€æŸ¥äº§å“æ˜¯å¦é‡å¤
        
        Returns:
            (æ˜¯å¦é‡å¤, é‡å¤åŸå› )
        """
        name = product.get('name', '')
        website = product.get('website', '')
        
        # ä¼˜å…ˆä½¿ç”¨æ–°æ¨¡å—
        if self.checker:
            return self.checker.is_duplicate(product)
        
        # Fallback åˆ°æ—§é€»è¾‘
        name_lower = name.lower().strip()
        if name_lower in self.existing_names:
            return True, f"åç§°é‡å¤: {name}"
        
        if website:
            domain = normalize_url(website)
            if domain and domain in self.existing_domains:
                return True, f"åŸŸåé‡å¤: {domain}"
        
        return False, None
    
    def add_product(self, product: dict):
        """æ·»åŠ æ–°äº§å“åˆ°ç´¢å¼•"""
        if self.checker:
            self.checker.add_product(product)
        
        # åŒæ—¶æ›´æ–°æ—§ç´¢å¼•
        name = product.get('name', '').lower().strip()
        if name:
            self.existing_names.add(name)
        
        website = product.get('website', '')
        if website:
            domain = normalize_url(website)
            if domain:
                self.existing_domains.add(domain)


def verify_url_exists(url: str, timeout: int = 5) -> bool:
    """
    éªŒè¯ URL æ˜¯å¦çœŸå®å­˜åœ¨ï¼ˆå¯è®¿é—®ï¼‰
    
    Args:
        url: è¦éªŒè¯çš„ URL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        True å¦‚æœ URL å¯è®¿é—®ï¼ŒFalse å¦åˆ™
    """
    if not url or url.lower() == "unknown":
        return False
    
    try:
        # ç¡®ä¿æœ‰åè®®
        if not url.startswith(("http://", "https://")):
            url = f"https://{url}"
        
        # ç¦ç”¨ SSL è­¦å‘Šï¼ˆLibreSSL ç‰ˆæœ¬é—®é¢˜ï¼‰
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # å‘é€ GET è¯·æ±‚ï¼ˆHEAD æœ‰æ—¶è¢«æ‹’ç»ï¼‰
        response = requests.get(
            url,
            timeout=timeout,
            allow_redirects=True,
            headers={"User-Agent": "Mozilla/5.0 (compatible; WeeklyAI Bot)"},
            verify=False,  # ç¦ç”¨ SSL éªŒè¯ï¼ˆLibreSSL å…¼å®¹æ€§ï¼‰
            stream=True  # ä¸ä¸‹è½½å†…å®¹
        )
        response.close()
        return response.status_code < 400
    except requests.exceptions.RequestException:
        return False


def is_duplicate_domain(product: dict, existing_domains: set) -> bool:
    """æ£€æŸ¥åŸŸåæ˜¯å¦å·²å­˜åœ¨"""
    domain = normalize_url(product.get("website", ""))
    return domain in existing_domains if domain else False


# ============================================
# è´¨é‡è¿‡æ»¤å™¨
# ============================================

# æ³›åŒ–çš„ why_matters é»‘åå•ï¼ˆä¼šè¢«è¿‡æ»¤æ‰ï¼‰
GENERIC_WHY_MATTERS = [
    "å¾ˆæœ‰æ½œåŠ›",
    "å€¼å¾—å…³æ³¨",
    "æœ‰å‰æ™¯",
    "è¡¨ç°ä¸é”™",
    "å›¢é˜ŸèƒŒæ™¯ä¸é”™",
    "èèµ„æƒ…å†µè‰¯å¥½",
    "å¸‚åœºå‰æ™¯å¹¿é˜”",
    "æŠ€æœ¯å®åŠ›å¼º",
    "ç”¨æˆ·åé¦ˆè‰¯å¥½",
    "å¢é•¿è¿…é€Ÿ",
]

# çŸ¥åäº§å“æ’é™¤åå•ï¼ˆä¸æ˜¯é»‘é©¬ï¼‰
WELL_KNOWN_PRODUCTS = {
    # å›½é™…çŸ¥å AI äº§å“
    "chatgpt", "openai", "claude", "anthropic", "gemini", "bard",
    "copilot", "github copilot", "dall-e", "dall-e 3", "sora",
    "midjourney", "stable diffusion", "stability ai",
    "cursor", "perplexity", "elevenlabs", "eleven labs",
    "synthesia", "runway", "runway ml", "pika", "pika labs",
    "bolt.new", "bolt", "v0.dev", "v0", "replit", "together ai", "groq",
    "character.ai", "character ai", "jasper", "jasper ai",
    "notion ai", "grammarly", "copy.ai", "writesonic",
    "huggingface", "hugging face", "langchain", "llamaindex",
    # ä¸­å›½çŸ¥å AI äº§å“
    "kimi", "æœˆä¹‹æš—é¢", "moonshot", "doubao", "è±†åŒ…", "å­—èŠ‚è·³åŠ¨",
    "tongyi", "é€šä¹‰åƒé—®", "é€šä¹‰", "qwen", "wenxin", "æ–‡å¿ƒä¸€è¨€", "æ–‡å¿ƒ",
    "ernie", "ç™¾åº¦", "baidu",
    "è®¯é£æ˜Ÿç«", "æ˜Ÿç«", "spark", "minimax", "abab",
    # å¤§å‚äº§å“
    "google gemini", "google bard", "meta llama", "llama",
    "microsoft copilot", "bing chat", "amazon q", "aws bedrock",
}

# â”€â”€ ä¸å¯ä¿¡ source é»‘åå• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
UNTRUSTED_SOURCES = {
    # é›¶å”®å¹³å°
    "æ¥½å¤©å¸‚å ´", "rakuten", "çœ¼é¡å¸‚å ´", "amazon", "taobao", "æ·˜å®",
    "äº¬ä¸œ", "jd.com", "å¤©çŒ«", "aliexpress", "ebay",
    # è§†é¢‘å¹³å°
    "youtube", "bilibili", "tiktok", "æŠ–éŸ³", "å¿«æ‰‹",
    # ç¤¾äº¤åª’ä½“
    "twitter", "x.com", "å¾®åš", "weibo", "çŸ¥ä¹", "zhihu",
    "reddit", "facebook", "instagram",
}

# source_url åŸŸåé»‘åå•ï¼ˆé˜² source å­—æ®µè¢«æ¨¡å‹ä¼ªé€ ï¼‰
UNTRUSTED_SOURCE_DOMAINS = {
    # é›¶å”®/ç”µå•†
    "rakuten.co.jp", "item.rakuten.co.jp", "amazon.com", "amazon.co.jp",
    "taobao.com", "tmall.com", "jd.com", "aliexpress.com", "ebay.com",
    # è§†é¢‘/çŸ­è§†é¢‘
    "youtube.com", "youtu.be", "bilibili.com", "tiktok.com", "douyin.com", "kuaishou.com",
    # ç¤¾äº¤/ç¤¾åŒº
    "x.com", "twitter.com", "weibo.com", "zhihu.com", "reddit.com", "facebook.com", "instagram.com",
}

PLACEHOLDER_SOURCE_DOMAINS = {
    "example.com", "example.org", "example.net", "localhost", "127.0.0.1",
}

# â”€â”€ åšå®¢æ ‡é¢˜ç‰¹å¾è¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BLOG_TITLE_MARKERS = [
    "ï¼š", "ï¼Ÿ", "ï¼", "å¦‚ä½•", "ä»€ä¹ˆæ˜¯", "ä¸ºä»€ä¹ˆ", "çš„ä¸‹ä¸€ä¸ª",
    "é£å£", "è¶‹åŠ¿", "æœªæ¥", "ç›˜ç‚¹", "åˆé›†", "Top",
]

# â”€â”€ é€šç”¨æ¦‚å¿µåï¼ˆä¸æ˜¯äº§å“åï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GENERIC_CONCEPT_NAMES = [
    "AIéšèº«è®¾å¤‡", "AIæ™ºèƒ½åŠ©æ‰‹", "æ™ºèƒ½ç©¿æˆ´è®¾å¤‡", "AIç¡¬ä»¶",
    "AIçœ¼é•œ", "AIåŠ©æ‰‹", "æ™ºèƒ½ç¡¬ä»¶", "AIå¯ç©¿æˆ´",
    "AI wearable", "AI device", "AI hardware", "smart glasses",
]


def validate_source(product: dict) -> tuple[bool, str]:
    """éªŒè¯äº§å“æ¥æºæ˜¯å¦å¯ä¿¡"""
    source = product.get("source", "").strip().lower()
    source_url = product.get("source_url", "").strip().lower()

    if not source:
        source = ""

    for untrusted in UNTRUSTED_SOURCES:
        if source and untrusted.lower() in source:
            return False, f"untrusted source: {source}"

    if source_url:
        domain = normalize_url(source_url)
        if domain in PLACEHOLDER_SOURCE_DOMAINS:
            return False, f"placeholder source_url domain: {domain}"
        for blocked in UNTRUSTED_SOURCE_DOMAINS:
            if blocked in domain:
                return False, f"untrusted source_url domain: {domain}"

    return True, "source ok"


def validate_product_name(name: str) -> tuple[bool, str]:
    """éªŒè¯äº§å“åæ˜¯å¦åƒçœŸæ­£çš„äº§å“åï¼ˆä¸æ˜¯åšå®¢æ ‡é¢˜æˆ–é€šç”¨æ¦‚å¿µï¼‰"""
    if not name:
        return False, "empty name"

    # æ£€æŸ¥åšå®¢æ ‡é¢˜ç‰¹å¾
    if len(name) > 10:
        matching_markers = [m for m in BLOG_TITLE_MARKERS if m in name]
        if len(matching_markers) >= 1:
            return False, f"name looks like blog title (markers: {matching_markers})"

    # æ£€æŸ¥é€šç”¨æ¦‚å¿µå
    name_lower = name.lower().strip()
    for concept in GENERIC_CONCEPT_NAMES:
        if name_lower == concept.lower() or name_lower.startswith(concept.lower()):
            return False, f"name is generic concept: {concept}"

    return True, "name ok"


def validate_against_search_results(
    product: dict, search_results: list
) -> tuple[bool, str]:
    """
    äº¤å‰éªŒè¯ï¼šäº§å“åæ˜¯å¦åœ¨æœç´¢ç»“æœä¸­å‡ºç°è¿‡ã€‚
    è¿™æ˜¯é˜²æ­¢ LLM å¹»è§‰çš„æœ€é‡è¦æ£€æŸ¥ã€‚

    Args:
        product: æå–çš„äº§å“
        search_results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨ (SearchResult å¯¹è±¡æˆ–å­—å…¸)

    Returns:
        (æ˜¯å¦é€šè¿‡, åŸå› )
    """
    if not search_results:
        return True, "no search results to cross-check"

    name = product.get("name", "").strip()
    if not name:
        return False, "no name"

    # å°†æ‰€æœ‰æœç´¢ç»“æœçš„æ–‡æœ¬åˆå¹¶
    search_text_combined = ""
    for r in search_results:
        if isinstance(r, dict):
            search_text_combined += " " + (r.get("title", "") + " " +
                                           r.get("content", "") + " " +
                                           r.get("snippet", "") + " " +
                                           r.get("url", ""))
        elif hasattr(r, 'title'):
            search_text_combined += " " + (r.title + " " + r.snippet + " " + r.url)

    search_text_lower = search_text_combined.lower()

    # æ£€æŸ¥äº§å“åï¼ˆæˆ–å…¶ä¸»è¦éƒ¨åˆ†ï¼‰æ˜¯å¦åœ¨æœç´¢ç»“æœä¸­å‡ºç°
    name_lower = name.lower()

    # ç²¾ç¡®åŒ¹é…
    if name_lower in search_text_lower:
        return True, "exact match in search results"

    # éƒ¨åˆ†åŒ¹é…ï¼ˆå¯¹äºä¸­æ–‡åå¯èƒ½éœ€è¦ï¼‰: è‡³å°‘ 3 ä¸ªå­—ç¬¦çš„å­ä¸²
    name_parts = name.split()
    for part in name_parts:
        if len(part) >= 3 and part.lower() in search_text_lower:
            return True, f"partial match: {part}"

    # å¯¹äºä¸­æ–‡åï¼Œæ£€æŸ¥è¿ç»­ 3+ å­—ç¬¦
    if any('\u4e00' <= c <= '\u9fff' for c in name):
        for i in range(len(name) - 2):
            chunk = name[i:i+3]
            if chunk.lower() in search_text_lower:
                return True, f"chinese partial match: {chunk}"

    # source_url æ£€æŸ¥ï¼šå¦‚æœäº§å“çš„ source_url åœ¨æœç´¢ç»“æœä¸­
    source_url = product.get("source_url", "")
    if source_url:
        for r in search_results:
            r_url = r.get("url", "") if isinstance(r, dict) else getattr(r, 'url', '')
            if source_url == r_url:
                return True, "source_url matches search result"

    return False, f"product '{name}' not found in search results (possible hallucination)"


def validate_product(product: dict) -> tuple[bool, str]:
    """
    éªŒè¯äº§å“è´¨é‡ï¼Œè¿”å› (æ˜¯å¦é€šè¿‡, åŸå› )

    è¿‡æ»¤æ¡ä»¶:
    1. å¿…é¡»æœ‰æœ‰æ•ˆçš„ website URL
    2. description å¿…é¡» >20 å­—ç¬¦
    3. why_matters ä¸èƒ½æ˜¯æ³›åŒ–æè¿°
    4. name ä¸èƒ½æ˜¯æ–°é—»æ ‡é¢˜
    5. çŸ¥åäº§å“æ’é™¤ï¼ˆä½¿ç”¨ WELL_KNOWN_PRODUCTSï¼‰
    6. é»‘é©¬(4-5åˆ†)å¿…é¡»æ»¡è¶³è‡³å°‘2æ¡æ ‡å‡† (criteria_met)
    7. ç½®ä¿¡åº¦æ£€æŸ¥ (confidence >= 0.6)
    """
    name = product.get("name", "").strip()
    website = product.get("website", "").strip()
    description = product.get("description", "").strip()
    why_matters = product.get("why_matters", "").strip()

    # 0a. æ£€æŸ¥äº§å“åæ˜¯å¦åˆæ³•ï¼ˆé˜²åšå®¢æ ‡é¢˜/é€šç”¨æ¦‚å¿µï¼‰
    name_valid, name_reason = validate_product_name(name)
    if not name_valid:
        return False, name_reason

    # 0b. æ£€æŸ¥æ¥æºæ˜¯å¦å¯ä¿¡ï¼ˆé˜²é›¶å”®/è§†é¢‘/ç¤¾äº¤å¹³å°ï¼‰
    source_valid, source_reason = validate_source(product)
    if not source_valid:
        return False, source_reason

    # 1. æ£€æŸ¥å¿…å¡«å­—æ®µ
    if not name:
        return False, "missing name"
    if not description:
        return False, "missing description"
    if not why_matters:
        return False, "missing why_matters"

    # 2. æ£€æŸ¥ website
    if not website:
        return False, "missing website"
    if website.lower() == "unknown":
        return False, "unknown website not allowed"
    
    # ä¿®å¤ç¼ºå°‘åè®®çš„ URL
    if not website.startswith(("http://", "https://")) and "." in website:
        website = f"https://{website}"
        product["website"] = website
    
    if website.lower() == "unknown":
        return False, "unknown website not allowed"
    elif not website.startswith(("http://", "https://")):
        return False, "invalid website URL"

    # 3. æ£€æŸ¥ description é•¿åº¦
    if len(description) < 20:
        return False, f"description too short ({len(description)} chars)"

    # 4. æ£€æŸ¥ why_matters æ˜¯å¦å¤ªæ³›åŒ–
    #    Fix: use OR â€” reject if contains generic phrase OR is too short.
    #    Previous AND logic allowed very short generic texts through.
    why_lower = why_matters.lower()
    for generic in GENERIC_WHY_MATTERS:
        if generic in why_lower or len(why_matters) < 30:
            return False, f"generic why_matters: contains '{generic}' or too short ({len(why_matters)} chars)"

    # 5. æ£€æŸ¥ why_matters æ˜¯å¦åŒ…å«å…·ä½“æ•°å­—ï¼ˆèèµ„/ARR/ç”¨æˆ·æ•°ï¼‰
    has_number = bool(re.search(r'[\$Â¥â‚¬]\d+|ARR|\d+[MBKä¸‡äº¿]|\d+%', why_matters))
    has_specific = any(kw in why_matters for kw in [
        'é¢†æŠ•', 'èèµ„', 'ä¼°å€¼', 'ç”¨æˆ·', 'å¢é•¿', 'ARR', 'é¦–åˆ›', 'é¦–ä¸ª',
        'å‰OpenAI', 'å‰Google', 'å‰Meta', 'YC', 'a16z', 'Sequoia',
    ])
    if not has_number and not has_specific:
        return False, "why_matters lacks specific details"

    # 6. æ£€æŸ¥ name æ˜¯å¦åƒæ–°é—»æ ‡é¢˜ï¼ˆä¸­æ–‡åŒºæ›´å®¹æ˜“æŠŠæ ‡é¢˜å½“äº§å“åï¼‰
    news_patterns = [
        'èèµ„', 'å®£å¸ƒ', 'å‘å¸ƒ', 'è·å¾—', 'å®Œæˆ', 'æ¨å‡º', 'ä¸Šçº¿',
        'æŠ•èµ„', 'é¢†æŠ•', 'å‚æŠ•', 'è¢«æŠ•', 'æ”¶è´­', 'ä¼°å€¼',
        'ç‹¬å®¶', 'çˆ†æ–™', 'æŠ¥é“', 'æ›å…‰', 'ä¼ å‡º', 'æ¶ˆæ¯', 'ä¼ é—»',
    ]
    if any(p in name for p in news_patterns) and len(name) >= 8:
        return False, "name looks like news headline"

    # 7. æ£€æŸ¥æ˜¯å¦æ˜¯çŸ¥åäº§å“
    name_lower = name.lower()
    if name_lower in WELL_KNOWN_PRODUCTS:
        return False, f"well-known product: {name}"
    # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆä¾‹å¦‚ "ChatGPT Plus" åŒ…å« "chatgpt"ï¼‰
    for known in WELL_KNOWN_PRODUCTS:
        if known in name_lower or name_lower in known:
            return False, f"well-known product match: {known}"

    # 8. æ£€æŸ¥é»‘é©¬(4-5åˆ†)æ˜¯å¦æ»¡è¶³è‡³å°‘1æ¡æ ‡å‡†ï¼ˆæ”¾å®½è¦æ±‚ï¼‰
    # æ³¨ï¼šåŸæ¥è¦æ±‚ â‰¥2 æ¡æ ‡å‡†å¤ªä¸¥æ ¼ï¼Œå¯¼è‡´äº§å‡ºå¤ªå°‘
    score = product.get("dark_horse_index", 0)
    if isinstance(score, str):
        try:
            score = int(float(score))
        except ValueError:
            score = 0
    criteria = product.get("criteria_met", [])
    if not isinstance(criteria, list):
        criteria = [criteria] if criteria else []
    if score >= 5 and len(criteria) < 2:
        # 5åˆ†é»‘é©¬éœ€è¦ â‰¥2 æ¡æ ‡å‡†
        return False, f"5-star dark_horse needs â‰¥2 criteria (has {len(criteria)})"
    if score == 4 and len(criteria) < 1:
        # 4åˆ†é»‘é©¬åªéœ€è¦ â‰¥1 æ¡æ ‡å‡†
        return False, f"4-star dark_horse needs â‰¥1 criteria (has {len(criteria)})"

    # 9. æ£€æŸ¥ç½®ä¿¡åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
    confidence = product.get("confidence", 1.0)
    if confidence < 0.6:
        return False, f"low confidence ({confidence:.2f})"

    # 10. Default missing categories to ["other"]
    cats = product.get("categories")
    if not cats or not isinstance(cats, list) or len(cats) == 0:
        product["categories"] = ["other"]

    # 11. Default missing/null region
    region = product.get("region")
    if not region or not isinstance(region, str) or not region.strip():
        product["region"] = UNKNOWN_COUNTRY_DISPLAY

    return True, "passed"


def load_existing_domains() -> set:
    """åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„äº§å“åŸŸå"""
    domains = set()

    for dir_path in [DARK_HORSES_DIR, RISING_STARS_DIR]:
        if os.path.exists(dir_path):
            for f in os.listdir(dir_path):
                if f.endswith('.json'):
                    try:
                        with open(os.path.join(dir_path, f), 'r') as file:
                            products = json.load(file)
                            for p in products:
                                domain = normalize_url(p.get('website', ''))
                                if domain:
                                    domains.add(domain)
                    except:
                        pass

    return domains


def get_perplexity_client():
    """
    è·å– Perplexity å®¢æˆ·ç«¯

    Returns:
        PerplexityClient å®ä¾‹æˆ– None
    """
    if not PERPLEXITY_API_KEY:
        print("  âš ï¸ PERPLEXITY_API_KEY not set")
        return None

    try:
        from utils.perplexity_client import PerplexityClient
        client = PerplexityClient(api_key=PERPLEXITY_API_KEY)
        if client.is_available():
            return client
        return None
    except ImportError as e:
        print(f"  âš ï¸ perplexity_client module not found: {e}")
        return None


def get_glm_client():
    """
    è·å– GLM (æ™ºè°±) å®¢æˆ·ç«¯

    Returns:
        GLMClient å®ä¾‹æˆ– None
    """
    if not ZHIPU_API_KEY:
        print("  âš ï¸ ZHIPU_API_KEY not set")
        return None

    try:
        from utils.glm_client import GLMClient
        client = GLMClient(api_key=ZHIPU_API_KEY)
        if client.is_available():
            return client
        return None
    except ImportError as e:
        print(f"  âš ï¸ glm_client module not found: {e}")
        return None


def get_provider_for_region(region_key: str) -> str:
    """
    æ ¹æ®åœ°åŒºè¿”å›æœç´¢ provider

    è·¯ç”±è§„åˆ™:
    - cn (ä¸­å›½) â†’ GLM (å¦‚æœå¯ç”¨ä¸”å¯ç”¨)
    - å…¶ä»–åœ°åŒº â†’ Perplexity

    Args:
        region_key: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)

    Returns:
        provider åç§° ("glm" æˆ– "perplexity")
    """
    if region_key == "cn" and ZHIPU_API_KEY and USE_GLM_FOR_CN:
        return "glm"
    return "perplexity"


def perplexity_search(
    query: str,
    count: int = 10,
    region: Optional[str] = None,
    domain_filter: Optional[list] = None
) -> list:
    """
    ä½¿ç”¨ Perplexity Search API è¿›è¡Œå®æ—¶ Web æœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        count: ç»“æœæ•°é‡
        region: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        domain_filter: åŸŸåè¿‡æ»¤ (["techcrunch.com", "-reddit.com"] ç­‰)
    
    Returns:
        [{"title": "", "url": "", "content": ""}, ...]
    """
    client = get_perplexity_client()
    if not client:
        return []
    
    try:
        if region:
            results = client.search_by_region(
                query,
                region=region,
                max_results=count
            )
        else:
            results = client.search(
                query,
                max_results=count,
                domain_filter=domain_filter
            )
        return [r.to_dict() for r in results]
    
    except Exception as e:
        print(f"  âŒ Perplexity Search Error: {e}")
        return []


def analyze_with_perplexity(content: str, task: str = "extract", region: str = "ğŸ‡ºğŸ‡¸",
                            quota_remaining: dict = None, region_key: str = "us",
                            product_type: str = "mixed") -> dict:
    """
    ä½¿ç”¨ Perplexity Sonar æ¨¡å‹åˆ†æå†…å®¹

    ç”¨äºäº§å“æå–å’Œè¯„åˆ†ã€‚

    Args:
        content: è¦åˆ†æçš„å†…å®¹ï¼ˆæœç´¢ç»“æœæ–‡æœ¬ï¼‰
        task: ä»»åŠ¡ç±»å‹ (extract/score)
        region: åœ°åŒºæ ‡è¯† (emoji flag)
        quota_remaining: å‰©ä½™é…é¢ {"dark_horses": n, "rising_stars": m}
        region_key: åœ°åŒºä»£ç  (cn/us/eu/jp/kr/sea) ç”¨äºé€‰æ‹© prompt è¯­è¨€

    Returns:
        è§£æåçš„ JSONï¼ˆäº§å“åˆ—è¡¨æˆ–è¯„åˆ†ç»“æœï¼‰
    """
    client = get_perplexity_client()
    if not client:
        return {}

    if quota_remaining is None:
        quota_remaining = DAILY_QUOTA.copy()

    # æ„å»º prompt
    if task == "extract":
        if USE_MODULAR_PROMPTS and product_type == "hardware":
            prompt = get_hardware_analysis_prompt(
                search_results=content[:AUTO_DISCOVER_PROMPT_MAX_CHARS],
                region=region,
                quota_dark_horses=quota_remaining.get("dark_horses", 5),
                quota_rising_stars=quota_remaining.get("rising_stars", 10)
            )
        elif USE_MODULAR_PROMPTS:
            prompt = get_analysis_prompt(
                region_key=region_key,
                search_results=content[:AUTO_DISCOVER_PROMPT_MAX_CHARS],
                quota_dark_horses=quota_remaining.get("dark_horses", 5),
                quota_rising_stars=quota_remaining.get("rising_stars", 10),
                region_flag=region
            )
        else:
            prompt_template = get_extraction_prompt(region_key)
            prompt = prompt_template.format(
                search_results=content[:AUTO_DISCOVER_PROMPT_MAX_CHARS],
                region=region,
                quota_dark_horses=quota_remaining.get("dark_horses", 5),
                quota_rising_stars=quota_remaining.get("rising_stars", 10)
            )
    elif task == "score":
        prompt = SCORING_PROMPT.format(
            product=json.dumps(content, ensure_ascii=False, indent=2)
        ) if 'SCORING_PROMPT' in dir() else f"Score this product: {content}"
    else:
        return {}

    try:
        # ä½¿ç”¨ analyze æ–¹æ³• (Sonar Chat Completions)
        result = client.analyze(
            prompt=prompt,
            temperature=0.3,  # ä½æ¸©åº¦è·å¾—æ›´ç¨³å®šè¾“å‡º
            max_tokens=4096
        )
        return result if isinstance(result, (dict, list)) else {}

    except Exception as e:
        print(f"  âŒ Perplexity Analysis Error: {e}")
        return {}


# ============================================
# GLM (æ™ºè°±) æœç´¢å’Œåˆ†æå‡½æ•° (ä¸­å›½åŒº)
# ============================================

def glm_search(
    query: str,
    count: int = 10,
    region: Optional[str] = None,
) -> list:
    """
    ä½¿ç”¨ GLM è”ç½‘æœç´¢ API è¿›è¡Œä¸­å›½åŒºæœç´¢

    Args:
        query: æœç´¢æŸ¥è¯¢
        count: ç»“æœæ•°é‡
        region: åœ°åŒºä»£ç  (ä¸»è¦ç”¨äº cn)

    Returns:
        [{"title": "", "url": "", "content": ""}, ...]
    """
    client = get_glm_client()
    if not client:
        return []

    try:
        results = client.search_by_region(
            query,
            region=region or "cn",
            max_results=count
        )
        return [r.to_dict() for r in results]

    except Exception as e:
        print(f"  âŒ GLM Search Error: {e}")
        return []


def analyze_with_glm(content: str, task: str = "extract", region: str = "ğŸ‡¨ğŸ‡³",
                     quota_remaining: dict = None, region_key: str = "cn",
                     product_type: str = "mixed") -> dict:
    """
    ä½¿ç”¨ GLM æ¨¡å‹åˆ†æå†…å®¹ (ä¸­å›½åŒº)

    Args:
        content: è¦åˆ†æçš„å†…å®¹ï¼ˆæœç´¢ç»“æœæ–‡æœ¬ï¼‰
        task: ä»»åŠ¡ç±»å‹ (extract/score)
        region: åœ°åŒºæ ‡è¯† (emoji flag)
        quota_remaining: å‰©ä½™é…é¢ {"dark_horses": n, "rising_stars": m}
        region_key: åœ°åŒºä»£ç 

    Returns:
        è§£æåçš„ JSONï¼ˆäº§å“åˆ—è¡¨æˆ–è¯„åˆ†ç»“æœï¼‰
    """
    client = get_glm_client()
    if not client:
        return {}

    if quota_remaining is None:
        quota_remaining = DAILY_QUOTA.copy()

    # æ„å»º prompt (ä¸­å›½åŒºä½¿ç”¨ä¸­æ–‡ prompt)
    if task == "extract":
        if USE_MODULAR_PROMPTS and product_type == "hardware":
            prompt = get_hardware_analysis_prompt(
                search_results=content[:AUTO_DISCOVER_PROMPT_MAX_CHARS],
                region=region,
                quota_dark_horses=quota_remaining.get("dark_horses", 5),
                quota_rising_stars=quota_remaining.get("rising_stars", 10)
            )
        elif USE_MODULAR_PROMPTS:
            prompt = get_analysis_prompt(
                region_key=region_key,
                search_results=content[:AUTO_DISCOVER_PROMPT_MAX_CHARS],
                quota_dark_horses=quota_remaining.get("dark_horses", 5),
                quota_rising_stars=quota_remaining.get("rising_stars", 10),
                region_flag=region
            )
        else:
            prompt_template = get_extraction_prompt("cn")
            prompt = prompt_template.format(
                search_results=content[:AUTO_DISCOVER_PROMPT_MAX_CHARS],
                region=region,
                quota_dark_horses=quota_remaining.get("dark_horses", 5),
                quota_rising_stars=quota_remaining.get("rising_stars", 10)
            )

        # GLM is more likely to hallucinate websites / output headline-like names.
        # Add strict guardrails to keep results traceable and reduce junk entries.
        prompt += """

## GLM é¢å¤–è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼Œè¿åä»»ä½•ä¸€æ¡åˆ™ä¸è¾“å‡ºè¯¥äº§å“ï¼‰

### åå¹»è§‰è§„åˆ™ï¼ˆæœ€é‡è¦ï¼ï¼‰

1. **åªæå–æœç´¢ç»“æœä¸­æ˜ç¡®æåˆ°çš„äº§å“**ã€‚
   - å¦‚æœæœç´¢ç»“æœä¸­æ²¡æœ‰æåˆ°æŸä¸ªäº§å“çš„åå­—ï¼Œç»å¯¹ä¸è¦è¾“å‡ºå®ƒã€‚
   - ä¸è¦ä»ä½ çš„è®­ç»ƒçŸ¥è¯†ä¸­"è¡¥å……"äº§å“ã€‚æœç´¢ç»“æœé‡Œæ²¡æœ‰çš„ = ä¸å­˜åœ¨ã€‚
   - è¾“å‡ºäº§å“æ•°é‡ä¸èƒ½è¶…è¿‡æœç´¢ç»“æœä¸­å®é™…æåˆ°çš„ä¸åŒäº§å“æ•°é‡ã€‚

2. `source_url` å¿…é¡»ç²¾ç¡®å¤åˆ¶è‡ªä¸Šæ–¹æœç´¢ç»“æœä¸­çš„ `Source URL:` è¡Œã€‚
   - æ‰¾ä¸åˆ°å¯å¯¹åº”çš„ URLï¼Œå°±ä¸è¦è¾“å‡ºè¯¥äº§å“ã€‚
   - ä¸å…è®¸ç¼–é€  source_urlï¼Œä¹Ÿä¸å…è®¸ç•™ç©ºã€‚

3. `website` åªæœ‰åœ¨æœç´¢ç»“æœæ–‡æœ¬é‡Œã€Œæ˜ç¡®å‡ºç°å®˜ç½‘åŸŸåã€æ—¶æ‰å¡«å†™ã€‚
   - æ— æ³•ç¡®è®¤çœŸå®å®˜ç½‘æ—¶ï¼š**ä¸è¦è¾“å‡ºè¯¥äº§å“**ï¼ˆä¸è¦å†™ unknownï¼‰ã€‚
   - ä¸è¦å‡­æ„Ÿè§‰çŒœæµ‹å®˜ç½‘ï¼ˆå¦‚æŠŠå…¬å¸åæ‹¼æˆ .com/.aiï¼‰ã€‚

### äº§å“åç§°è§„åˆ™

4. `name` å¿…é¡»æ˜¯ä¸€ä¸ªæ˜ç¡®çš„ã€Œäº§å“/å…¬å¸åã€ï¼Œä¸èƒ½æ˜¯ï¼š
   - æ–°é—»æ ‡é¢˜æˆ–æè¿°å¥ï¼ˆç¦æ­¢åŒ…å«ï¼šæŠ•èµ„/é¢†æŠ•/èèµ„/ç‹¬å®¶/çˆ†æ–™/æŠ¥é“/æ›å…‰/æ¶ˆæ¯/ä¼ é—»/å¦‚ä½•/ä»€ä¹ˆæ˜¯/é£å£/è¶‹åŠ¿ï¼‰
   - é€šç”¨æ¦‚å¿µï¼ˆå¦‚"AIéšèº«è®¾å¤‡"ã€"AIæ™ºèƒ½åŠ©æ‰‹"ã€"æ™ºèƒ½ç©¿æˆ´è®¾å¤‡"ï¼‰
   - åšå®¢æ–‡ç« æ ‡é¢˜ï¼ˆå«"ï¼š""ï¼Ÿ""ï¼"ç­‰æ ‡ç‚¹çš„é•¿å¥ï¼‰

### æ¥æºå¯ä¿¡åº¦è§„åˆ™

5. `source` å¿…é¡»æ˜¯æƒå¨åª’ä½“æˆ–äº§å“å¹³å°ï¼Œä»¥ä¸‹æ¥æºä¸å¯ä¿¡ï¼Œä¸è¦ä½¿ç”¨ï¼š
   - é›¶å”®å¹³å°ï¼šæ¥½å¤©å¸‚å ´ã€çœ¼é¡å¸‚å ´ã€Amazonã€æ·˜å®ã€äº¬ä¸œ
   - è§†é¢‘å¹³å°ï¼šYouTubeã€Bilibiliã€TikTok
   - ç¤¾äº¤åª’ä½“ï¼šTwitter/Xã€å¾®åšã€çŸ¥ä¹
   - å¦‚æœæœç´¢ç»“æœå…¨éƒ¨æ¥è‡ªä»¥ä¸Šä¸å¯ä¿¡æ¥æºï¼Œè¿”å›ç©ºæ•°ç»„ `[]`
"""
    elif task == "score":
        prompt = SCORING_PROMPT.format(
            product=json.dumps(content, ensure_ascii=False, indent=2)
        ) if 'SCORING_PROMPT' in dir() else f"è¯„åˆ†äº§å“: {content}"
    else:
        return {}

    try:
        result = client.analyze(
            prompt=prompt,
            temperature=0.3,
            max_tokens=4096
        )
        return result if isinstance(result, (dict, list)) else {}

    except Exception as e:
        print(f"  âŒ GLM Analysis Error: {e}")
        return {}


# ============================================
# Provider Routing Functions
# ============================================

def search_with_provider(query: str, region_key: str, search_engine: str = "bing") -> list:
    """
    æ ¹æ®åœ°åŒºè·¯ç”±æœç´¢è¯·æ±‚

    è·¯ç”±è§„åˆ™:
    - cn (ä¸­å›½) â†’ GLM è”ç½‘æœç´¢ (å¦‚æœå¯ç”¨)
    - å…¶ä»–åœ°åŒº â†’ Perplexity Search API

    Args:
        query: æœç´¢æŸ¥è¯¢
        region_key: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        search_engine: æœç´¢å¼•æ“ (å·²å¼ƒç”¨ï¼Œä¿ç•™å…¼å®¹)

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    provider = get_provider_for_region(region_key)

    if provider == "glm":
        print(f"    ğŸ” Using GLM for {region_key}")
        return glm_search(query, region=region_key)
    else:
        print(f"    ğŸ” Using Perplexity for {region_key}")
        return perplexity_search(query, region=region_key)


def analyze_with_provider(content, task: str, region_key: str, region_flag: str = "ğŸ‡ºğŸ‡¸",
                          quota_remaining: dict = None, product_type: str = "mixed"):
    """
    æ ¹æ®åœ°åŒºè·¯ç”±åˆ†æè¯·æ±‚

    è·¯ç”±è§„åˆ™:
    - cn (ä¸­å›½) â†’ GLM æ¨¡å‹åˆ†æ (å¦‚æœå¯ç”¨)
    - å…¶ä»–åœ°åŒº â†’ Perplexity Sonar åˆ†æ

    Args:
        content: è¦åˆ†æçš„å†…å®¹
        task: ä»»åŠ¡ç±»å‹ (extract/score)
        region_key: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        region_flag: åœ°åŒºæ ‡è¯† (emoji)
        quota_remaining: å‰©ä½™é…é¢

    Returns:
        åˆ†æç»“æœ
    """
    provider = get_provider_for_region(region_key)

    if provider == "glm":
        return analyze_with_glm(content, task, region_flag, quota_remaining, region_key, product_type)
    else:
        return analyze_with_perplexity(content, task, region_flag, quota_remaining, region_key, product_type)


def fetch_url_content(url: str) -> str:
    """æŠ“å– URL å†…å®¹"""
    try:
        import urllib.request
        req = urllib.request.Request(
            url,
            headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
        )
        with urllib.request.urlopen(req, timeout=15) as response:
            content = response.read().decode('utf-8', errors='ignore')

            # ç®€å•æå–æ­£æ–‡ï¼ˆå»é™¤ HTML æ ‡ç­¾ï¼‰
            content = re.sub(r'<script[^>]*>[\s\S]*?</script>', '', content)
            content = re.sub(r'<style[^>]*>[\s\S]*?</style>', '', content)
            content = re.sub(r'<[^>]+>', ' ', content)
            content = re.sub(r'\s+', ' ', content)
            return content[:15000]  # é™åˆ¶é•¿åº¦
    except Exception as e:
        print(f"  Fetch error: {e}")
        return ""


def _normalize_match_text(text: str) -> str:
    if not text:
        return ""
    return re.sub(r'\s+', '', text.lower())


def _score_search_result_for_name(name: str, result: dict) -> int:
    name_norm = _normalize_match_text(name)
    if not name_norm or len(name_norm) < 3:
        return -1

    title = _normalize_match_text(result.get('title', ''))
    content = _normalize_match_text(result.get('content') or result.get('snippet', ''))
    url = (result.get('url', '') or '').lower()

    score = 0
    if name_norm in title:
        score += 5
    if name_norm in content:
        score += 3
    if name_norm in url:
        score += 2

    tokens = re.findall(r'[a-z0-9]{3,}', name.lower())
    for token in tokens:
        if token in title:
            score += 2
        elif token in content:
            score += 1
        elif token in url:
            score += 1

    return score


def attach_source_url(product: dict, search_results: list, min_score: int = 4) -> None:
    """ä¸ºäº§å“åŒ¹é…æœç´¢ç»“æœ URL (ç”¨äºåç»­å®˜ç½‘è§£æ)"""
    if product.get('source_url'):
        return

    name = product.get('name', '').strip()
    if not name or not search_results:
        return

    best_result = None
    best_score = -1
    for result in search_results:
        score = _score_search_result_for_name(name, result)
        if score > best_score:
            best_score = score
            best_result = result

    if best_result and best_score >= min_score:
        url = best_result.get('url', '')
        if url:
            product['source_url'] = url
        title = best_result.get('title', '')
        if title:
            product['source_title'] = title


def fetch_with_provider(source_config: dict, limit: int = 10) -> list:
    """
    ä½¿ç”¨ Provider è·¯ç”±åˆ†ææ¥æºé¡µé¢å†…å®¹

    ç­–ç•¥ï¼š
    1. å…ˆå°è¯•æŠ“å–ç½‘é¡µ
    2. æ ¹æ®åœ°åŒºé€‰æ‹© Provider (cnâ†’GLM, å…¶ä»–â†’Perplexity) æå–å¹¶è¯„åˆ†
    """
    source_name = source_config['name']
    region_flag = source_config['region']
    url = source_config.get('url', '')

    region_key_map = {
        'ğŸ‡ºğŸ‡¸': 'us', 'ğŸ‡¨ğŸ‡³': 'cn', 'ğŸ‡ªğŸ‡º': 'eu',
        'ğŸ‡¯ğŸ‡µ': 'jp', 'ğŸ‡°ğŸ‡·': 'kr', 'ğŸ‡¸ğŸ‡¬': 'sea'
    }
    region_key = region_key_map.get(region_flag, 'us')
    provider = get_provider_for_region(region_key)

    print(f"  Fetching: {url}")

    # æŠ“å–ç½‘é¡µå†…å®¹
    content = fetch_url_content(url)
    products = []

    if content and len(content) > 500:
        print(f"  Analyzing page content with {provider}...")
        products = analyze_with_provider(content, task="extract", region_key=region_key, region_flag=region_flag)
        if not isinstance(products, list):
            products = []

    print(f"  Found {len(products)} potential products")

    # è¡¥å……ä¿¡æ¯å¹¶è¯„åˆ†
    result = []
    for p in products[:limit]:
        # æ·»åŠ æ¥æºä¿¡æ¯
        p['source'] = source_name
        p['source_region'] = region_flag
        p['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')
        if url and not p.get('source_url'):
            p['source_url'] = url
        apply_country_fields(p, fallback_region_flag=region_flag)

        score_result = analyze_with_provider(p, task="score", region_key=region_key, region_flag=region_flag)
        if isinstance(score_result, dict) and score_result:
            p['dark_horse_index'] = score_result.get('score', p.get('dark_horse_index', 2))
            if 'reason' in score_result:
                p['score_reason'] = score_result['reason']

        if 'dark_horse_index' not in p:
            p = analyze_and_score(p)

        result.append(p)

    return result


# ä¿æŒå‘åå…¼å®¹çš„åˆ«å
fetch_with_perplexity = fetch_with_provider


def analyze_and_score(product: dict) -> dict:
    """
    ä½¿ç”¨ AI åˆ†æäº§å“å¹¶è¯„åˆ†

    è¯„åˆ†æ ‡å‡†ï¼š
    - 5åˆ†: èèµ„ >$100M æˆ– é¡¶çº§åˆ›å§‹äºº æˆ– å“ç±»å¼€åˆ›è€…
    - 4åˆ†: èèµ„ >$30M æˆ– YC/é¡¶çº§VC
    - 3åˆ†: èèµ„ >$5M æˆ– ProductHunt Top 5
    - 2åˆ†: æœ‰æ½œåŠ›ä½†æ•°æ®ä¸è¶³
    - 1åˆ†: è¾¹ç¼˜
    """
    funding = product.get('funding_total', '')
    source = product.get('source', '')

    # ç®€å•çš„è§„åˆ™è¯„åˆ†ï¼ˆå¯ä»¥æ›¿æ¢ä¸º AI è¯„åˆ†ï¼‰
    score = 2  # é»˜è®¤

    # è§£æèèµ„é‡‘é¢
    funding_amount = 0
    if funding:
        match = re.search(r'\$?([\d.]+)\s*([BMK])?', funding, re.I)
        if match:
            amount = float(match.group(1))
            unit = (match.group(2) or '').upper()
            if unit == 'B':
                funding_amount = amount * 1000
            elif unit == 'M':
                funding_amount = amount
            elif unit == 'K':
                funding_amount = amount / 1000
            else:
                funding_amount = amount

    # è¯„åˆ†é€»è¾‘
    if funding_amount >= 100:
        score = 5
    elif funding_amount >= 30:
        score = 4
    elif funding_amount >= 5:
        score = 3
    elif source in ['Y Combinator', 'ProductHunt']:
        score = 3

    product['dark_horse_index'] = score
    return product


def _coerce_score(value, default: int = 2) -> int:
    try:
        score = int(float(str(value)))
    except Exception:
        score = default
    return max(1, min(5, score))


def _ensure_criteria_list(product: dict) -> list:
    criteria = product.get('criteria_met', [])
    if isinstance(criteria, list):
        out = [str(c).strip() for c in criteria if str(c).strip()]
    elif criteria:
        out = [str(criteria).strip()]
    else:
        out = []
    product['criteria_met'] = out
    return out


def _add_criteria(product: dict, tag: str) -> None:
    tag = str(tag or '').strip()
    if not tag:
        return
    criteria = _ensure_criteria_list(product)
    if tag not in criteria:
        criteria.append(tag)
        product['criteria_met'] = criteria


def _parse_funding_amount_musd(funding_text: str) -> float:
    text = str(funding_text or '').strip()
    if not text:
        return 0.0
    match = re.search(r'\$?\s*([\d,.]+)\s*([BMK]?)', text, re.IGNORECASE)
    if not match:
        return 0.0
    try:
        amount = float(match.group(1).replace(',', ''))
    except Exception:
        return 0.0
    unit = (match.group(2) or '').upper()
    if unit == 'B':
        amount *= 1000.0
    elif unit == 'K':
        amount /= 1000.0
    return amount


def _has_strong_supply_signal(product: dict) -> bool:
    funding = _parse_funding_amount_musd(product.get('funding_total', ''))
    if funding >= 30.0:
        return True

    criteria = [str(c).lower() for c in _ensure_criteria_list(product)]
    if any(k in criteria for k in ['funding_signal', 'top_vc_backing', 'founder_background', 'category_creator']):
        return True

    why_matters = str(product.get('why_matters', '')).lower()
    strong_markers = ['sequoia', 'a16z', 'benchmark', 'accel', 'greylock', 'yc', 'y combinator', 'top-tier']
    return any(m in why_matters for m in strong_markers)


def _apply_demand_signals_and_guardrail(
    product: dict,
    *,
    demand_engine,
    llm_score: int,
    override_mode: str,
) -> tuple[int, str]:
    """
    Enrich product with demand signals and apply scoring guardrail.

    Returns:
        (new_score, guardrail_applied)
    """
    if not demand_engine:
        return llm_score, "none"

    result = demand_engine.collect_for_product(product)
    demand_payload = result.get('demand') or {}
    community_verdict = result.get('community_verdict')
    criteria_tags = result.get('criteria_tags') or []

    extra = product.get('extra')
    if not isinstance(extra, dict):
        extra = {}
    extra['demand'] = demand_payload
    product['extra'] = extra

    if isinstance(community_verdict, dict):
        product['community_verdict'] = community_verdict

    for tag in criteria_tags:
        _add_criteria(product, tag)

    has_supply = _has_strong_supply_signal(product)
    new_score, applied, reason = apply_demand_guardrail(
        llm_score=llm_score,
        demand_payload=demand_payload,
        has_strong_supply_signal=has_supply,
        mode=override_mode,
    )

    demand_payload['guardrail_applied'] = applied
    demand_payload['guardrail_reason'] = reason
    extra['demand'] = demand_payload
    product['extra'] = extra

    if applied == 'upgraded':
        _add_criteria(product, 'demand_guardrail_upgraded')
    elif applied == 'downgraded':
        _add_criteria(product, 'demand_guardrail_downgraded')

    return new_score, applied


def save_product(product: dict, dry_run: bool = False):
    """ä¿å­˜äº§å“åˆ°ç›¸åº”ç›®å½•"""
    score = product.get('dark_horse_index', 2)
    week = get_current_week()

    if score >= 4:
        # é»‘é©¬
        target_dir = DARK_HORSES_DIR
        target_file = os.path.join(target_dir, f'week_{week}.json')
    else:
        # æ½œåŠ›è‚¡
        target_dir = RISING_STARS_DIR
        target_file = os.path.join(target_dir, f'global_{week}.json')

    if dry_run:
        print(f"  [DRY RUN] Would save to: {target_file}")
        print(f"  {json.dumps(product, ensure_ascii=False, indent=2)}")
        return

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(target_dir, exist_ok=True)

    # åŠ è½½ç°æœ‰æ•°æ®
    if os.path.exists(target_file):
        with open(target_file, 'r', encoding='utf-8') as f:
            products = json.load(f)
    else:
        products = []

    # æ·»åŠ æ–°äº§å“
    products.append(product)

    # ä¿å­˜åˆ°åˆ†ç±»æ–‡ä»¶
    with open(target_file, 'w', encoding='utf-8') as f:
        json.dump(products, f, ensure_ascii=False, indent=2)

    print(f"  Saved to: {target_file}")
    
    # åŒæ—¶åŒæ­¥åˆ° products_featured.jsonï¼ˆå‰ç«¯æ•°æ®æºï¼‰
    sync_to_featured(product)


def sync_to_featured(product: dict):
    """
    åŒæ­¥äº§å“åˆ° products_featured.jsonï¼ˆå‰ç«¯æ•°æ®æºï¼‰
    
    è¿™æ ·å‘ç°çš„äº§å“å¯ä»¥ç›´æ¥åœ¨å‰ç«¯æ˜¾ç¤º
    """
    if product.get('dark_horse_index', 0) < 2:
        print(f"  â­ï¸ Skip featured (score < 2): {product.get('name')}")
        return
    featured_file = os.path.join(PROJECT_ROOT, 'data', 'products_featured.json')
    
    try:
        # åŠ è½½ç°æœ‰æ•°æ®
        if os.path.exists(featured_file):
            with open(featured_file, 'r', encoding='utf-8') as f:
                featured = json.load(f)
        else:
            featured = []
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆwebsite ä¼˜å…ˆï¼Œå…¶æ¬¡ nameï¼‰
        existing_websites = {normalize_url(p.get('website', '')) for p in featured}
        def _safe_name_key(value: str) -> str:
            if not value:
                return ""
            try:
                return normalize_name(value) if callable(globals().get("normalize_name")) else "".join(
                    ch for ch in value.lower() if ch.isalnum()
                )
            except Exception:
                return "".join(ch for ch in value.lower() if ch.isalnum())

        existing_names = {_safe_name_key(p.get('name', '')) for p in featured}
        product_domain = normalize_url(product.get('website', ''))
        product_name_key = _safe_name_key(product.get('name', ''))

        if product_domain and product_domain in existing_websites:
            print(f"  ğŸ“‹ Already in featured (domain): {product.get('name')}")
            return
        if (not product_domain) and product_name_key and product_name_key in existing_names:
            print(f"  ğŸ“‹ Already in featured (name): {product.get('name')}")
            return
        
        # è½¬æ¢å­—æ®µæ ¼å¼ï¼ˆé€‚é…å‰ç«¯ï¼‰
        apply_country_fields(product, fallback_region_flag=str(product.get('source_region') or product.get('region') or '').strip())
        featured_product = {
            'name': product.get('name'),
            'description': product.get('description'),
            'website': product.get('website'),
            'logo_url': product.get('logo_url') or product.get('logo', ''),
            'categories': [product.get('category', 'other')],
            'dark_horse_index': product.get('dark_horse_index', 2),
            'why_matters': product.get('why_matters', ''),
            'funding_total': product.get('funding_total', ''),
            'region': product.get('region', UNKNOWN_COUNTRY_DISPLAY),
            'country_code': product.get('country_code', UNKNOWN_COUNTRY_CODE),
            'country_name': product.get('country_name', UNKNOWN_COUNTRY_NAME),
            'country_flag': product.get('country_flag', ''),
            'country_display': product.get('country_display', UNKNOWN_COUNTRY_DISPLAY),
            'country_source': product.get('country_source', 'unknown'),
            'source_region': product.get('source_region', ''),
            'source': product.get('source', 'auto_discover'),
            'source_url': product.get('source_url', ''),
            'source_title': product.get('source_title', ''),
            'website_source': product.get('website_source', ''),
            'community_verdict': product.get('community_verdict'),
            'extra': product.get('extra', {}) if isinstance(product.get('extra'), dict) else {},
            'discovered_at': product.get('discovered_at', datetime.utcnow().strftime('%Y-%m-%d')),
            'first_seen': datetime.utcnow().isoformat() + 'Z',
            # è®¡ç®—åˆ†æ•°ï¼ˆç”¨äºæ’åºï¼‰
            'final_score': product.get('dark_horse_index', 2) * 20,
            'trending_score': product.get('dark_horse_index', 2) * 18,
        }
        
        # æ·»åŠ åˆ°åˆ—è¡¨å¼€å¤´ï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
        featured.insert(0, featured_product)
        
        # ä¿å­˜
        with open(featured_file, 'w', encoding='utf-8') as f:
            json.dump(featured, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… Synced to featured: {product.get('name')}")
        
    except Exception as e:
        print(f"  âš ï¸ Failed to sync to featured: {e}")


def discover_from_source(source_key: str, dry_run: bool = False):
    """ä»å•ä¸ªæ¸ é“å‘ç°äº§å“"""
    if source_key not in SOURCES:
        print(f"Unknown source: {source_key}")
        return

    config = SOURCES[source_key]
    print(f"\n{'='*50}")
    print(f"  Discovering from: {config['name']} {config['region']}")
    print(f"{'='*50}")

    existing = load_existing_products()

    # ä½¿ç”¨ Perplexity å‘ç°äº§å“
    products = fetch_with_perplexity(config)

    new_count = 0
    for product in products:
        if is_duplicate(product.get('name', ''), product.get('website', ''), existing):
            print(f"  Skip duplicate: {product.get('name')}")
            continue

        # å¦‚æœè¯„åˆ†ç¼ºå¤±ï¼Œä½¿ç”¨è§„åˆ™è¯„åˆ†
        if 'dark_horse_index' not in product:
            product = analyze_and_score(product)

        save_product(product, dry_run)
        new_count += 1
        existing.add(product.get('name', '').lower())

    print(f"\n  Found {new_count} new products from {config['name']}")


def discover_all(dry_run: bool = False, tier: int = None):
    """ä»æ‰€æœ‰æ¸ é“å‘ç°äº§å“"""
    for source_key, config in SOURCES.items():
        if tier and config.get('tier', 1) > tier:
            continue
        discover_from_source(source_key, dry_run)


# ============================================
# æ–°å¢ï¼šåŸºäºåœ°åŒºçš„ Perplexity æœç´¢å‘ç°
# ============================================

def discover_by_region(region_key: str, dry_run: bool = False, product_type: str = "mixed") -> dict:
    """
    ä½¿ç”¨ Perplexity Search API æŒ‰åœ°åŒºå‘ç° AI äº§å“ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦è´¨é‡è¿‡æ»¤å’Œå…³é”®è¯è½®æ¢ï¼‰

    Args:
        region_key: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
        dry_run: é¢„è§ˆæ¨¡å¼
        product_type: äº§å“ç±»å‹ (software/hardware/mixed)

    Returns:
        ç»Ÿè®¡ä¿¡æ¯
    """
    if region_key not in REGION_CONFIG:
        print(f"âŒ Unknown region: {region_key}")
        print(f"   Available: {', '.join(REGION_CONFIG.keys())}")
        return {"error": f"Unknown region: {region_key}"}

    config = REGION_CONFIG[region_key]
    region_name = config['name']
    search_engine = config['search_engine']
    current_provider = get_provider_for_region(region_key)

    # ä½¿ç”¨å…³é”®è¯è½®æ¢ï¼ˆæ”¯æŒäº§å“ç±»å‹ï¼‰
    keyword_stats = load_keyword_yield_stats()
    keywords = get_keywords_for_today(region_key, product_type)
    keywords = apply_keyword_limit(region_key, keywords)
    if AUTO_DISCOVER_BUDGET_MODE == "adaptive":
        keywords = rank_keywords_by_yield(region_key, keywords, keyword_stats)

    keyword_limit = 0
    if region_key == "cn" and MAX_KEYWORDS_CN > 0:
        keyword_limit = MAX_KEYWORDS_CN
    elif MAX_KEYWORDS_DEFAULT > 0:
        keyword_limit = MAX_KEYWORDS_DEFAULT

    type_label = {"software": "ğŸ’» è½¯ä»¶", "hardware": "ğŸ”§ ç¡¬ä»¶", "mixed": "ğŸ“Š æ··åˆ(40%ç¡¬ä»¶+60%è½¯ä»¶)"}.get(product_type, "æ··åˆ")

    print(f"\n{'='*60}")
    print(f"  ğŸŒ Discovering AI Products: {region_name}")
    print(f"  ğŸ“¡ Search Engine: {search_engine}")
    print(f"  ğŸ¤– Provider: {current_provider}")
    print(f"  ğŸ“¦ Product Type: {type_label}")
    print(f"  ğŸ’¸ Budget Mode: {AUTO_DISCOVER_BUDGET_MODE}")
    print(f"  ğŸ§ª Analyze Gate: {'on' if AUTO_DISCOVER_ENABLE_ANALYZE_GATE else 'off'}")
    print(f"  ğŸ”‘ Keywords: {len(keywords)} queries (day {datetime.now().weekday()})")
    if keyword_limit:
        print(f"  ğŸ§¯ Keyword limit: {keyword_limit}")
    print(f"{'='*60}")

    # ä½¿ç”¨å¢å¼ºå»é‡æ£€æŸ¥å™¨
    featured_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "products_featured.json")
    existing_products = []
    if os.path.exists(featured_path):
        with open(featured_path, 'r', encoding='utf-8') as f:
            existing_products = json.load(f)
    
    dedup_checker = EnhancedDuplicateChecker(existing_products)
    all_products = []
    quality_rejections = []
    demand_engine = None
    demand_processed = 0
    demand_upgraded = 0
    demand_downgraded = 0

    if ENABLE_DEMAND_SIGNALS and HAS_DEMAND_SIGNALS:
        demand_engine = DemandSignalEngine(
            window_days=DEMAND_WINDOW_DAYS,
            strict_x_official=True,
            official_handles_path=PRODUCT_OFFICIAL_HANDLES_FILE,
            perplexity_api_key=PERPLEXITY_API_KEY,
            github_token=GITHUB_TOKEN,
            github_max_star_pages=DEMAND_GITHUB_MAX_STAR_PAGES,
        )

    stats = {
        "region": region_key,
        "region_name": region_name,
        "search_results": 0,
        "products_found": 0,
        "products_saved": 0,
        "dark_horses": 0,
        "rising_stars": 0,
        "duplicates_skipped": 0,
        "quality_rejections": 0,
        "demand_processed": 0,
        "demand_upgraded": 0,
        "demand_downgraded": 0,
    }

    deferred_keywords: List[Tuple[str, str, List[dict], str]] = []
    region_flag_map = {
        'us': 'ğŸ‡ºğŸ‡¸', 'cn': 'ğŸ‡¨ğŸ‡³', 'eu': 'ğŸ‡ªğŸ‡º',
        'jp': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·', 'kr': 'ğŸ‡°ğŸ‡·', 'sea': 'ğŸ‡¸ğŸ‡¬'
    }
    region_flag = region_flag_map.get(region_key, 'ğŸŒ')

    def _run_extract_for_keyword(
        keyword: str,
        keyword_type: str,
        search_results: List[dict],
        *,
        bypass_gate: bool = False,
    ) -> Tuple[int, int, int]:
        nonlocal demand_processed, demand_upgraded, demand_downgraded
        keyword_saved = 0
        keyword_dark_horses = 0
        current_provider = get_provider_for_region(region_key)

        if AUTO_DISCOVER_ENABLE_ANALYZE_GATE and not bypass_gate:
            analyze_ok, analyze_reason = should_analyze_search_results(search_results, keyword)
            if not analyze_ok:
                deferred_keywords.append((keyword, keyword_type, search_results, analyze_reason))
                print(f"    â­ï¸ Analyze gate skipped: {analyze_reason}")
                return 0, 0, 0

        search_text = build_search_text(search_results)
        if not search_text.strip():
            return 0, 0, 0

        print(f"    ğŸ“Š Extracting products with {current_provider}...")
        products = analyze_with_provider(
            search_text,
            "extract",
            region_key,
            region_flag,
            product_type=keyword_type
        )
        if not isinstance(products, list):
            products = []

        print(f"    âœ… Extracted {len(products)} products")
        stats["products_found"] += len(products)

        for product in products:
            name = product.get('name', '')
            if not name:
                continue

            is_dup, dup_reason = dedup_checker.is_duplicate(product)
            if is_dup:
                stats["duplicates_skipped"] += 1
                print(f"    â­ï¸ Skip duplicate: {dup_reason}")
                continue

            attach_source_url(product, search_results)

            if current_provider == "glm":
                xref_valid, xref_reason = validate_against_search_results(
                    product, search_results
                )
                if not xref_valid:
                    stats["quality_rejections"] += 1
                    quality_rejections.append({"name": name, "reason": xref_reason})
                    print(f"    ğŸš« Hallucination filter: {name} ({xref_reason})")
                    continue

            is_hardware = (
                keyword_type == "hardware" or
                product.get("category") == "hardware" or
                product.get("is_hardware", False)
            )
            if is_hardware:
                product.setdefault("category", "hardware")
                product["is_hardware"] = True
            if is_hardware and USE_MODULAR_PROMPTS:
                is_valid, reason = validate_hardware_product(product)
            else:
                is_valid, reason = validate_product(product)
            if not is_valid:
                stats["quality_rejections"] += 1
                quality_rejections.append({"name": name, "reason": reason})
                print(f"    âŒ Quality fail: {name} ({reason})")
                continue

            product['source_region'] = region_flag
            product['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')
            product['discovery_method'] = f'{current_provider}_search'
            product['search_keyword'] = keyword
            apply_country_fields(product, fallback_region_flag=region_flag)

            score = product.get('dark_horse_index')
            if score is None:
                print(f"    ğŸ¯ Fallback scoring: {product.get('name')}...")
                product = analyze_and_score(product)
                score = product.get('dark_horse_index', 2)
            score = _coerce_score(score, default=2)
            product['dark_horse_index'] = score

            guardrail_applied = "none"
            if demand_engine and demand_processed < max(DEMAND_MAX_PRODUCTS_PER_RUN, 0):
                try:
                    score, guardrail_applied = _apply_demand_signals_and_guardrail(
                        product,
                        demand_engine=demand_engine,
                        llm_score=score,
                        override_mode=DEMAND_OVERRIDE_MODE,
                    )
                    demand_processed += 1
                    if guardrail_applied == 'upgraded':
                        demand_upgraded += 1
                    elif guardrail_applied == 'downgraded':
                        demand_downgraded += 1
                    product['dark_horse_index'] = score
                    print(f"    ğŸ§­ Demand guardrail: {guardrail_applied} (score={score})")
                except Exception as e:
                    print(f"    âš ï¸ Demand signal failed: {e}")
            elif demand_engine and DEMAND_MAX_PRODUCTS_PER_RUN >= 0 and demand_processed >= DEMAND_MAX_PRODUCTS_PER_RUN:
                print("    â­ï¸ Demand skipped: per-run limit reached")

            criteria = product.get('criteria_met', [])
            print(f"    ğŸ“ˆ Score: {score}/5 | Criteria: {criteria}")

            website = product.get('website', '')
            if not dry_run and website and website.lower() != 'unknown':
                if not verify_url_exists(website, timeout=5):
                    print(f"    âš ï¸ URL not accessible: {website}")
                    product['needs_verification'] = True

            save_product(product, dry_run)
            stats["products_saved"] += 1
            keyword_saved += 1

            if score >= 4:
                stats["dark_horses"] += 1
                keyword_dark_horses += 1
            else:
                stats["rising_stars"] += 1

            dedup_checker.add_product(product)
            all_products.append(product)

        return keyword_saved, keyword_dark_horses, len(products)

    # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
    for i, keyword in enumerate(keywords, 1):
        print(f"\n  [{i}/{len(keywords)}] Searching: {keyword[:50]}...")
        keyword_type = resolve_keyword_type(keyword, region_key, product_type)

        search_results = search_with_provider(keyword, region_key, search_engine)
        stats["search_results"] += len(search_results)
        if not search_results:
            update_keyword_yield_stats(
                keyword_stats,
                region_key=region_key,
                keyword=keyword,
                searches=1,
            )
            continue

        saved_count, dark_count, extracted_count = _run_extract_for_keyword(
            keyword,
            keyword_type,
            search_results,
            bypass_gate=False,
        )
        update_keyword_yield_stats(
            keyword_stats,
            region_key=region_key,
            keyword=keyword,
            searches=1,
            extracted=extracted_count,
            saved=saved_count,
            dark_horses=dark_count,
        )

        current_provider = get_provider_for_region(region_key)
        if current_provider == "glm" and GLM_KEYWORD_DELAY > 0 and i < len(keywords):
            print(f"  â³ GLM cooldown: sleeping {GLM_KEYWORD_DELAY:.1f}s")
            time.sleep(GLM_KEYWORD_DELAY)

    # Analyze gate ä¿åº•å›æ”¾ï¼šå½“æœ¬è½®äº§å‡ºåä½æ—¶ï¼Œå›æ”¾è¢« gate æ‹¦æˆªçš„å…³é”®è¯
    min_expected_saves = max(1, len(keywords) // 4)
    if AUTO_DISCOVER_ENABLE_ANALYZE_GATE and deferred_keywords and stats["products_saved"] < min_expected_saves:
        print(f"\n  â™»ï¸ Replaying deferred keywords due to low yield ({stats['products_saved']} < {min_expected_saves})")
        for keyword, keyword_type, search_results, reason in deferred_keywords:
            print(f"  â†© Replaying: {keyword[:50]}... (gate reason: {reason})")
            saved_count, dark_count, extracted_count = _run_extract_for_keyword(
                keyword,
                keyword_type,
                search_results,
                bypass_gate=True,
            )
            update_keyword_yield_stats(
                keyword_stats,
                region_key=region_key,
                keyword=keyword,
                searches=0,
                extracted=extracted_count,
                saved=saved_count,
                dark_horses=dark_count,
            )

    flush_keyword_yield_stats(keyword_stats)

    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"  ğŸ“Š Summary for {region_name}")
    print(f"{'='*60}")
    print(f"  Search Results: {stats['search_results']}")
    print(f"  Products Found: {stats['products_found']}")
    print(f"  Products Saved: {stats['products_saved']}")
    print(f"  ğŸ‡ Dark Horses (4-5): {stats['dark_horses']}")
    print(f"  â­ Rising Stars (2-3): {stats['rising_stars']}")
    print(f"  Duplicates Skipped: {stats['duplicates_skipped']}")
    print(f"  Quality Rejections: {stats['quality_rejections']}")
    if demand_engine:
        stats["demand_processed"] = demand_processed
        stats["demand_upgraded"] = demand_upgraded
        stats["demand_downgraded"] = demand_downgraded
        print(
            "  Demand Signals: "
            f"processed={demand_processed}, upgraded={demand_upgraded}, downgraded={demand_downgraded}"
        )

    if quality_rejections:
        print(f"\n  Top rejection reasons:")
        reason_counts = {}
        for rej in quality_rejections:
            reason = rej['reason']
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
        for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1])[:3]:
            print(f"    - {reason}: {count}")

    return stats


def discover_all_regions(dry_run: bool = False, product_type: str = "mixed") -> dict:
    """
    å¸¦é…é¢ç³»ç»Ÿçš„å…¨çƒ AI äº§å“å‘ç°

    ç›®æ ‡é…é¢ï¼š
    - é»‘é©¬ (4-5åˆ†): 5 ä¸ª/å¤©
    - æ½œåŠ›è‚¡ (2-3åˆ†): 10 ä¸ª/å¤©
    
    Args:
        dry_run: é¢„è§ˆæ¨¡å¼
        product_type: äº§å“ç±»å‹ (software/hardware/mixed)

    Returns:
        è¯¦ç»†çš„å‘ç°æŠ¥å‘Š
    """
    start_time = datetime.now()
    today_str = start_time.strftime('%Y-%m-%d')

    type_label = {"software": "ğŸ’» è½¯ä»¶", "hardware": "ğŸ”§ ç¡¬ä»¶", "mixed": "ğŸ“Š æ··åˆ(40%ç¡¬ä»¶+60%è½¯ä»¶)"}.get(product_type, "æ··åˆ")
    
    print("\n" + "â•"*70)
    print(f"  ğŸŒ Daily AI Product Discovery - {today_str}")
    print("â•"*70)
    print(f"  ğŸ“Š Quota: {DAILY_QUOTA['dark_horses']} Dark Horses + {DAILY_QUOTA['rising_stars']} Rising Stars")
    print(f"  ğŸ“¦ Product Type: {type_label}")
    print(f"  ğŸ”„ Max Attempts: {MAX_ATTEMPTS} rounds")
    print(f"  ğŸ’¸ Budget Mode: {AUTO_DISCOVER_BUDGET_MODE}")
    print(f"  ğŸ§ª Analyze Gate: {'on' if AUTO_DISCOVER_ENABLE_ANALYZE_GATE else 'off'}")
    print(f"  ğŸ›Ÿ Quality Fallback: {'on' if AUTO_DISCOVER_QUALITY_FALLBACK else 'off'}")
    print(f"  ğŸ“… Keyword Pool: Day {datetime.now().weekday()} (0=Mon)")
    glm_status = 'enabled' if (ZHIPU_API_KEY and USE_GLM_FOR_CN) else 'disabled'
    pplx_status = 'enabled' if PERPLEXITY_API_KEY else 'missing key'
    print(f"  ğŸ¤– Provider: Perplexity ({pplx_status}) | GLM-cn ({glm_status})")
    print("â•"*70)

    # åˆå§‹åŒ–è·Ÿè¸ª
    found = {"dark_horses": 0, "rising_stars": 0}
    region_yield = {k: 0 for k in REGION_CONFIG.keys()}
    provider_stats = {"perplexity": 0, "glm": 0}
    duplicates_skipped = 0
    quality_rejections = []
    attempts = 0
    unique_domains = set()
    demand_processed = 0
    demand_upgraded = 0
    demand_downgraded = 0
    demand_engine = None
    keyword_stats = load_keyword_yield_stats()

    featured_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "products_featured.json")
    existing_products = []
    if os.path.exists(featured_path):
        with open(featured_path, 'r', encoding='utf-8') as f:
            existing_products = json.load(f)

    dedup_checker = EnhancedDuplicateChecker(existing_products)
    if ENABLE_DEMAND_SIGNALS and HAS_DEMAND_SIGNALS:
        demand_engine = DemandSignalEngine(
            window_days=DEMAND_WINDOW_DAYS,
            strict_x_official=True,
            official_handles_path=PRODUCT_OFFICIAL_HANDLES_FILE,
            perplexity_api_key=PERPLEXITY_API_KEY,
            github_token=GITHUB_TOKEN,
            github_max_star_pages=DEMAND_GITHUB_MAX_STAR_PAGES,
        )

    region_flag_map = {
        'us': 'ğŸ‡ºğŸ‡¸', 'cn': 'ğŸ‡¨ğŸ‡³', 'eu': 'ğŸ‡ªğŸ‡º',
        'jp': 'ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·', 'kr': 'ğŸ‡°ğŸ‡·', 'sea': 'ğŸ‡¸ğŸ‡¬'
    }
    keyword_pools: Dict[str, List[str]] = {}
    keyword_cursors = {k: 0 for k in REGION_CONFIG.keys()}
    prev_round_region_saved = {k: 1 for k in REGION_CONFIG.keys()}

    def quotas_met():
        return (found["dark_horses"] >= DAILY_QUOTA["dark_horses"] and
                found["rising_stars"] >= DAILY_QUOTA["rising_stars"])

    def get_category(score):
        return "dark_horses" if score >= 4 else "rising_stars"

    def get_keyword_pool(region_key: str) -> List[str]:
        if region_key in keyword_pools:
            return keyword_pools[region_key]
        pool = get_keywords_for_today(region_key, product_type)
        pool = apply_keyword_limit(region_key, pool)
        if AUTO_DISCOVER_BUDGET_MODE == "adaptive":
            pool = rank_keywords_by_yield(region_key, pool, keyword_stats)
        keyword_pools[region_key] = pool
        return pool

    def select_keywords_for_round(region_key: str, attempt: int) -> List[str]:
        pool = get_keyword_pool(region_key)
        if AUTO_DISCOVER_BUDGET_MODE == "legacy":
            return pool[:2] if attempt > 1 else list(pool)

        cursor = keyword_cursors.get(region_key, 0)
        if cursor >= len(pool):
            return []
        if attempt == 1:
            take = AUTO_DISCOVER_ROUND1_KEYWORDS
        else:
            if prev_round_region_saved.get(region_key, 0) <= 0:
                return []
            take = AUTO_DISCOVER_ROUND_EXPAND_STEP
        end = min(len(pool), cursor + max(1, take))
        selected = pool[cursor:end]
        keyword_cursors[region_key] = end
        return selected

    def process_keyword(
        *,
        region_key: str,
        search_engine: str,
        keyword: str,
        keyword_type: str,
        quota_remaining: Dict[str, int],
        deferred_queue: List[Tuple[str, str, List[dict], str]],
        search_results_override: Optional[List[dict]] = None,
        bypass_gate: bool = False,
    ) -> int:
        nonlocal duplicates_skipped, demand_processed, demand_upgraded, demand_downgraded

        search_requests = 0
        if search_results_override is None:
            search_results = search_with_provider(keyword, region_key, search_engine)
            search_requests = 1
        else:
            search_results = list(search_results_override)

        if not search_results:
            update_keyword_yield_stats(
                keyword_stats,
                region_key=region_key,
                keyword=keyword,
                searches=search_requests,
            )
            return 0

        if AUTO_DISCOVER_ENABLE_ANALYZE_GATE and not bypass_gate:
            analyze_ok, analyze_reason = should_analyze_search_results(search_results, keyword)
            if not analyze_ok:
                deferred_queue.append((keyword, keyword_type, search_results, analyze_reason))
                print(f"    â­ï¸ Analyze gate skipped: {analyze_reason}")
                update_keyword_yield_stats(
                    keyword_stats,
                    region_key=region_key,
                    keyword=keyword,
                    searches=search_requests,
                )
                return 0

        search_text = build_search_text(search_results)
        if not search_text.strip():
            update_keyword_yield_stats(
                keyword_stats,
                region_key=region_key,
                keyword=keyword,
                searches=search_requests,
            )
            return 0

        region_flag = region_flag_map.get(region_key, 'ğŸŒ')
        products = analyze_with_provider(
            search_text,
            "extract",
            region_key,
            region_flag,
            quota_remaining,
            product_type=keyword_type
        )
        if not isinstance(products, list):
            products = []

        print(f"    ğŸ“¦ Extracted: {len(products)} candidates")
        saved_count = 0
        dark_count = 0
        current_provider = get_provider_for_region(region_key)

        for product in products:
            if quotas_met():
                break

            name = product.get('name', '')
            if not name:
                continue

            is_dup, dup_reason = dedup_checker.is_duplicate(product)
            if is_dup:
                duplicates_skipped += 1
                print(f"    â­ï¸ Skip: {dup_reason}")
                continue

            attach_source_url(product, search_results)

            if current_provider == "glm":
                xref_valid, xref_reason = validate_against_search_results(product, search_results)
                if not xref_valid:
                    quality_rejections.append({"name": name, "reason": xref_reason})
                    print(f"    ğŸš« Hallucination filter: {name} ({xref_reason})")
                    continue

            is_hardware = (
                keyword_type == "hardware" or
                product.get("category") == "hardware" or
                product.get("is_hardware", False)
            )
            if is_hardware:
                product.setdefault("category", "hardware")
                product["is_hardware"] = True
            if is_hardware and USE_MODULAR_PROMPTS:
                is_valid, reason = validate_hardware_product(product)
            else:
                is_valid, reason = validate_product(product)
            if not is_valid:
                quality_rejections.append({"name": name, "reason": reason})
                print(f"    âŒ Quality fail: {name} ({reason})")
                continue

            score = product.get('dark_horse_index')
            if score is None:
                product = analyze_and_score(product)
                score = product.get('dark_horse_index', 2)
            score = _coerce_score(score, default=2)
            product['dark_horse_index'] = score

            guardrail_applied = "none"
            if demand_engine and demand_processed < max(DEMAND_MAX_PRODUCTS_PER_RUN, 0):
                try:
                    score, guardrail_applied = _apply_demand_signals_and_guardrail(
                        product,
                        demand_engine=demand_engine,
                        llm_score=score,
                        override_mode=DEMAND_OVERRIDE_MODE,
                    )
                    demand_processed += 1
                    if guardrail_applied == 'upgraded':
                        demand_upgraded += 1
                    elif guardrail_applied == 'downgraded':
                        demand_downgraded += 1
                    product['dark_horse_index'] = score
                    print(f"    ğŸ§­ Demand guardrail: {guardrail_applied} (score={score})")
                except Exception as e:
                    print(f"    âš ï¸ Demand signal failed: {e}")
            elif demand_engine and DEMAND_MAX_PRODUCTS_PER_RUN >= 0 and demand_processed >= DEMAND_MAX_PRODUCTS_PER_RUN:
                print("    â­ï¸ Demand skipped: per-run limit reached")

            category = get_category(score)
            if found[category] >= DAILY_QUOTA[category]:
                print(f"    â­ï¸ {category} quota full, skip: {name}")
                continue
            if region_yield[region_key] >= REGION_MAX.get(region_key, 3):
                print(f"    â­ï¸ Region max reached, skip: {name}")
                continue

            product['source_region'] = region_flag
            product['discovered_at'] = datetime.utcnow().strftime('%Y-%m-%d')
            product['discovery_method'] = f'{current_provider}_search'
            product['search_keyword'] = keyword
            apply_country_fields(product, fallback_region_flag=region_flag)
            save_product(product, dry_run)

            found[category] += 1
            region_yield[region_key] += 1
            provider_stats[current_provider] = provider_stats.get(current_provider, 0) + 1
            dedup_checker.add_product(product)
            saved_count += 1
            if category == "dark_horses":
                dark_count += 1

            website = product.get('website', '')
            if website:
                unique_domains.add(normalize_url(website))

            status_icon = "ğŸ¦„" if category == "dark_horses" else "â­"
            print(f"    {status_icon} SAVED: {name} (score={score}, {category}, {current_provider})")

        update_keyword_yield_stats(
            keyword_stats,
            region_key=region_key,
            keyword=keyword,
            searches=search_requests,
            extracted=len(products),
            saved=saved_count,
            dark_horses=dark_count,
        )
        return saved_count

    # ä¸»å‘ç°å¾ªç¯
    while not quotas_met() and attempts < MAX_ATTEMPTS:
        attempts += 1
        print(f"\n{'â”€'*70}")
        print(f"  ğŸ”„ Round {attempts}/{MAX_ATTEMPTS}")
        print(f"  Progress: DH {found['dark_horses']}/{DAILY_QUOTA['dark_horses']} | RS {found['rising_stars']}/{DAILY_QUOTA['rising_stars']}")
        print(f"{'â”€'*70}")

        round_region_saved = {k: 0 for k in REGION_CONFIG.keys()}
        region_order = get_region_order()

        for region_key in region_order:
            if region_yield[region_key] >= REGION_MAX.get(region_key, 3):
                print(f"\n  â­ï¸ Skip {region_key}: region max reached ({region_yield[region_key]})")
                continue
            if quotas_met():
                break

            config = REGION_CONFIG[region_key]
            region_name = config['name']
            search_engine = config['search_engine']
            current_provider = get_provider_for_region(region_key)
            keywords_this_round = select_keywords_for_round(region_key, attempts)
            if not keywords_this_round:
                print(f"\n  â­ï¸ {region_name} | no keywords in this round")
                continue

            print(f"\n  ğŸ“ {region_name} | Provider: {current_provider} | Keywords: {len(keywords_this_round)}")
            quota_remaining = {
                "dark_horses": DAILY_QUOTA["dark_horses"] - found["dark_horses"],
                "rising_stars": DAILY_QUOTA["rising_stars"] - found["rising_stars"],
            }
            deferred_keywords: List[Tuple[str, str, List[dict], str]] = []

            for idx, keyword in enumerate(keywords_this_round, 1):
                if quotas_met():
                    break
                print(f"\n    ğŸ” Searching: {keyword[:50]}...")
                keyword_type = resolve_keyword_type(keyword, region_key, product_type)
                saved = process_keyword(
                    region_key=region_key,
                    search_engine=search_engine,
                    keyword=keyword,
                    keyword_type=keyword_type,
                    quota_remaining=quota_remaining,
                    deferred_queue=deferred_keywords,
                )
                round_region_saved[region_key] += saved

                if current_provider == "glm" and GLM_KEYWORD_DELAY > 0 and idx < len(keywords_this_round):
                    print(f"    â³ GLM cooldown: sleeping {GLM_KEYWORD_DELAY:.1f}s")
                    time.sleep(GLM_KEYWORD_DELAY)

            if AUTO_DISCOVER_ENABLE_ANALYZE_GATE and deferred_keywords and round_region_saved[region_key] == 0 and not quotas_met():
                print(f"    â™»ï¸ Replaying deferred keywords for {region_key} (no saves in round)")
                for keyword, keyword_type, cached_results, reason in deferred_keywords:
                    if quotas_met():
                        break
                    print(f"    â†© Replaying: {keyword[:50]}... (gate reason: {reason})")
                    saved = process_keyword(
                        region_key=region_key,
                        search_engine=search_engine,
                        keyword=keyword,
                        keyword_type=keyword_type,
                        quota_remaining=quota_remaining,
                        deferred_queue=[],
                        search_results_override=cached_results,
                        bypass_gate=True,
                    )
                    round_region_saved[region_key] += saved

        prev_round_region_saved = round_region_saved

    if (
        AUTO_DISCOVER_BUDGET_MODE == "adaptive"
        and AUTO_DISCOVER_QUALITY_FALLBACK
        and not quotas_met()
    ):
        print("\n  ğŸ›Ÿ Quality fallback: quotas unmet after adaptive rounds, replaying remaining keywords in legacy mode")
        for region_key in get_region_order():
            if quotas_met():
                break
            if region_yield[region_key] >= REGION_MAX.get(region_key, 3):
                continue
            config = REGION_CONFIG[region_key]
            search_engine = config['search_engine']
            pool = get_keyword_pool(region_key)
            cursor = keyword_cursors.get(region_key, 0)
            remaining = pool[cursor:]
            if not remaining:
                continue
            quota_remaining = {
                "dark_horses": DAILY_QUOTA["dark_horses"] - found["dark_horses"],
                "rising_stars": DAILY_QUOTA["rising_stars"] - found["rising_stars"],
            }
            print(f"  â†ª {region_key}: fallback keywords={len(remaining)}")
            for keyword in remaining:
                if quotas_met():
                    break
                keyword_type = resolve_keyword_type(keyword, region_key, product_type)
                process_keyword(
                    region_key=region_key,
                    search_engine=search_engine,
                    keyword=keyword,
                    keyword_type=keyword_type,
                    quota_remaining=quota_remaining,
                    deferred_queue=[],
                )
            keyword_cursors[region_key] = len(pool)

    flush_keyword_yield_stats(keyword_stats)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    dh_status = "âœ…" if found["dark_horses"] >= DAILY_QUOTA["dark_horses"] else "âš ï¸"
    rs_status = "âœ…" if found["rising_stars"] >= DAILY_QUOTA["rising_stars"] else "âš ï¸"

    print("\n" + "â•"*70)
    print(f"  Daily Discovery Report - {today_str}")
    print("â•"*70)
    print(f"  Quotas:     Dark Horses: {found['dark_horses']}/{DAILY_QUOTA['dark_horses']} {dh_status}  Rising Stars: {found['rising_stars']}/{DAILY_QUOTA['rising_stars']} {rs_status}")
    print(f"  Attempts:   {attempts} rounds")
    print(f"  Duration:   {duration:.1f} seconds")
    print(f"  Regions:    {', '.join(f'{k}: {v}' for k, v in region_yield.items() if v > 0)}")
    print(f"  Providers:  {', '.join(f'{k}: {v}' for k, v in provider_stats.items() if v > 0)}")
    print(f"  Total saved: {found['dark_horses'] + found['rising_stars']}")
    print(f"  Duplicates skipped: {duplicates_skipped}")
    print(f"  Quality rejections: {len(quality_rejections)}")
    if demand_engine:
        print(
            "  Demand signals: "
            f"processed={demand_processed}, upgraded={demand_upgraded}, downgraded={demand_downgraded}"
        )

    if quality_rejections:
        print("\n  Quality rejection reasons:")
        reason_counts = {}
        for rej in quality_rejections:
            reason = rej['reason']
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
        for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1])[:5]:
            print(f"    - {reason}: {count}")

    print("â•"*70)

    # è¿”å›æŠ¥å‘Šæ•°æ®
    return {
        "date": today_str,
        "found": found,
        "quota": DAILY_QUOTA,
        "attempts": attempts,
        "region_yield": region_yield,
        "provider_stats": provider_stats,
        "unique_domains": len(unique_domains),
        "duplicates_skipped": duplicates_skipped,
        "quality_rejections": len(quality_rejections),
        "demand_processed": demand_processed,
        "demand_upgraded": demand_upgraded,
        "demand_downgraded": demand_downgraded,
        "duration_seconds": duration,
        "quotas_met": quotas_met(),
    }


def test_perplexity():
    """æµ‹è¯• Perplexity Search API è¿æ¥"""
    print("\n" + "="*60)
    print("  ğŸ” Testing Perplexity Search API")
    print("="*60)

    # æ£€æŸ¥ API Key
    if not PERPLEXITY_API_KEY:
        print("\n  âŒ PERPLEXITY_API_KEY not set")
        print("  Set it with: export PERPLEXITY_API_KEY=pplx_xxx")
        return

    print(f"  API Key: {PERPLEXITY_API_KEY[:12]}...")
    print(f"  Model: {PERPLEXITY_MODEL}")
    # å°è¯•å¯¼å…¥æ–°æ¨¡å—
    try:
        from utils.perplexity_client import PerplexityClient
        client = PerplexityClient()
        print(f"  Client Status: {client.get_status()}")
    except ImportError as e:
        print(f"  âš ï¸ SDK not installed: {e}")
        print("  Install with: pip install perplexityai")

    # æµ‹è¯•æœç´¢
    test_queries = [
        ("us", "AI startup funding 2026"),
        ("cn", "AIèèµ„ 2026"),
    ]

    for region, query in test_queries:
        print(f"\n  ğŸ“ Testing region={region}: {query}")
        results = perplexity_search(query, count=3, region=region)

        if results:
            print(f"  âœ… Found {len(results)} results")
            for i, r in enumerate(results[:2], 1):
                title = r.get('title', 'No Title')[:50]
                url = r.get('url', 'N/A')[:60]
                print(f"    {i}. {title}...")
                print(f"       URL: {url}")
        else:
            print(f"  âš ï¸ No results")

    print("\n  âœ… Perplexity test completed!")


def test_glm():
    """æµ‹è¯• GLM (æ™ºè°±) è”ç½‘æœç´¢ API è¿æ¥"""
    print("\n" + "="*60)
    print("  ğŸ” Testing GLM (æ™ºè°±) Web Search API")
    print("="*60)

    # æ£€æŸ¥ API Key
    if not ZHIPU_API_KEY:
        print("\n  âŒ ZHIPU_API_KEY not set")
        print("  Set it with: export ZHIPU_API_KEY=your-api-key")
        return

    print(f"  API Key: {ZHIPU_API_KEY[:12]}...")
    print(f"  Model: {GLM_MODEL}")
    print(f"  Search Engine: {GLM_SEARCH_ENGINE}")
    print(f"  USE_GLM_FOR_CN: {USE_GLM_FOR_CN}")

    # å°è¯•å¯¼å…¥æ¨¡å—
    try:
        from utils.glm_client import GLMClient
        client = GLMClient()
        print(f"  Client Status: {client.get_status()}")
    except ImportError as e:
        print(f"  âš ï¸ glm_client module not found: {e}")
        print("  Make sure utils/glm_client.py exists")
        print("  Install SDK with: pip install zhipuai")
        return

    if not client.is_available():
        print("\n  âŒ GLM client not available")
        print("  Install SDK with: pip install zhipuai")
        return

    # æµ‹è¯•æœç´¢
    test_queries = [
        "AIåˆ›ä¸šå…¬å¸ èèµ„ 2026",
        "AIèŠ¯ç‰‡ ç‹¬è§’å…½",
    ]

    for query in test_queries:
        print(f"\n  ğŸ“ Testing: {query}")
        results = glm_search(query, count=3)

        if results:
            print(f"  âœ… Found {len(results)} results")
            for i, r in enumerate(results[:2], 1):
                title = r.get('title', 'No Title')[:50]
                url = r.get('url', 'N/A')[:60]
                print(f"    {i}. {title}...")
                print(f"       URL: {url}")
        else:
            print(f"  âš ï¸ No results")

    print("\n  âœ… GLM test completed!")


def test_provider_routing():
    """æµ‹è¯• Provider è·¯ç”±é€»è¾‘"""
    print("\n" + "="*60)
    print("  ğŸ”€ Testing Provider Routing")
    print("="*60)

    regions = ['us', 'cn', 'eu', 'jp', 'kr', 'sea']

    print("\n  Provider routing results:")
    print(f"  ZHIPU_API_KEY set: {bool(ZHIPU_API_KEY)}")
    print(f"  USE_GLM_FOR_CN: {USE_GLM_FOR_CN}")
    print()

    for region in regions:
        provider = get_provider_for_region(region)
        icon = "ğŸ‡¨ğŸ‡³" if provider == "glm" else "ğŸŒ"
        print(f"    {region:5} â†’ {provider:12} {icon}")

    print("\n  âœ… Routing test completed!")


def setup_schedule():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆmacOS/Linuxï¼‰"""
    script_path = os.path.abspath(__file__)

    # ç”Ÿæˆ cron ä»»åŠ¡
    cron_line = f"0 9 * * * cd {PROJECT_ROOT} && /usr/bin/python3 {script_path} >> /tmp/auto_discover.log 2>&1"

    print("\nè®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©æ—©ä¸Š9ç‚¹è¿è¡Œï¼‰ï¼š")
    print("-" * 50)
    print("è¿è¡Œä»¥ä¸‹å‘½ä»¤æ·»åŠ  cron ä»»åŠ¡ï¼š")
    print(f"\n  (crontab -l 2>/dev/null; echo \"{cron_line}\") | crontab -")
    print("\næˆ–è€…ä½¿ç”¨ launchd (macOS)ï¼š")
    print(f"  åˆ›å»º ~/Library/LaunchAgents/com.weeklyai.autodiscover.plist")


def main():
    parser = argparse.ArgumentParser(
        description='è‡ªåŠ¨å‘ç°å…¨çƒ AI äº§å“ (v2.0 - Perplexity Search)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  # æŒ‰åœ°åŒºæœç´¢ï¼ˆæ¨èï¼Œä½¿ç”¨ Perplexity Searchï¼‰
  python tools/auto_discover.py --region us      # æœç´¢ç¾å›½ AI äº§å“
  python tools/auto_discover.py --region cn      # æœç´¢ä¸­å›½ AI äº§å“
  python tools/auto_discover.py --region eu      # æœç´¢æ¬§æ´² AI äº§å“
  python tools/auto_discover.py --region jp      # æœç´¢æ—¥éŸ© AI äº§å“
  python tools/auto_discover.py --region sea     # æœç´¢ä¸œå—äºš AI äº§å“
  python tools/auto_discover.py --region all     # æœç´¢æ‰€æœ‰åœ°åŒº

  # æŒ‰æ¸ é“æœç´¢ï¼ˆæ—§æ–¹å¼ï¼‰
  python tools/auto_discover.py --source 36kr    # ä» 36æ°ª å‘ç°
  python tools/auto_discover.py --source producthunt

  # å…¶ä»–é€‰é¡¹
  python tools/auto_discover.py --dry-run        # é¢„è§ˆä¸ä¿å­˜
"""
    )

    # æ–°å¢ï¼šåœ°åŒºå‚æ•°
    parser.add_argument('--region', '-r',
                        choices=['us', 'cn', 'eu', 'jp', 'sea', 'all'],
                        help='æŒ‰åœ°åŒºæœç´¢ (us/cn/eu/jp/sea/all)')
    
    # æ–°å¢ï¼šäº§å“ç±»å‹å‚æ•°
    parser.add_argument('--type', '-T',
                        choices=['software', 'hardware', 'mixed'],
                        default='mixed',
                        help='äº§å“ç±»å‹ (software/hardware/mixedï¼Œé»˜è®¤ mixed=40%%ç¡¬ä»¶+60%%è½¯ä»¶)')

    # åŸæœ‰å‚æ•°
    parser.add_argument('--source', '-s', help='æŒ‡å®šæ¸ é“ (e.g., 36kr, producthunt)')
    parser.add_argument('--tier', '-t', type=int, choices=[1, 2, 3], help='åªè¿è¡ŒæŒ‡å®šçº§åˆ«çš„æ¸ é“')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸ä¿å­˜')
    parser.add_argument('--schedule', action='store_true', help='è®¾ç½®å®šæ—¶ä»»åŠ¡')
    parser.add_argument('--list-sources', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æ¸ é“')
    parser.add_argument('--list-regions', action='store_true', help='åˆ—å‡ºæ‰€æœ‰åœ°åŒº')
    parser.add_argument('--list-keywords', action='store_true', help='åˆ—å‡ºå…³é”®è¯ï¼ˆæŒ‰ç±»å‹ï¼‰')
    parser.add_argument('--test-perplexity', action='store_true', help='æµ‹è¯• Perplexity Search API')
    parser.add_argument('--test-glm', action='store_true', help='æµ‹è¯• GLM (æ™ºè°±) è”ç½‘æœç´¢ API')
    parser.add_argument('--test-routing', action='store_true', help='æµ‹è¯• Provider è·¯ç”±é€»è¾‘')
    parser.add_argument('--no-lock', action='store_true', help='ç¦ç”¨å•å®ä¾‹é”ï¼ˆä¸å»ºè®®ï¼‰')

    args = parser.parse_args()

    # æµ‹è¯•åŠŸèƒ½
    if args.test_perplexity:
        test_perplexity()
        return

    if args.test_glm:
        test_glm()
        return

    if args.test_routing:
        test_provider_routing()
        return

    # åˆ—è¡¨åŠŸèƒ½
    if args.list_sources:
        print("\nå¯ç”¨æ¸ é“ï¼š")
        print("-" * 60)
        for key, config in SOURCES.items():
            print(f"  {key:15} {config['region']} {config['name']:20} Tier {config.get('tier', 1)}")
        return

    if args.list_regions:
        print("\nå¯ç”¨åœ°åŒºï¼š")
        print("-" * 60)
        for key, config in REGION_CONFIG.items():
            print(f"  {key:5} {config['name']:15} æƒé‡:{config['weight']:2}% æœç´¢å¼•æ“:{config['search_engine']}")
        return
    
    if args.list_keywords:
        region = args.region or 'us'
        print(f"\nå…³é”®è¯åˆ—è¡¨ (åœ°åŒº: {region})ï¼š")
        print("-" * 60)
        print("\nğŸ”§ ç¡¬ä»¶å…³é”®è¯:")
        for kw in get_hardware_keywords(region):
            print(f"  - {kw}")
        print("\nğŸ’» è½¯ä»¶å…³é”®è¯:")
        for kw in get_software_keywords(region):
            print(f"  - {kw}")
        print(f"\nğŸ“Š Mixed æ¨¡å¼å…³é”®è¯ (40%ç¡¬ä»¶ + 60%è½¯ä»¶):")
        for kw in get_keywords_for_today(region, "mixed"):
            print(f"  - {kw}")
        return

    if args.schedule:
        setup_schedule()
        return

    should_lock = not (
        args.list_sources or args.list_regions or args.list_keywords or
        args.test_perplexity or args.test_glm or args.test_routing or
        args.schedule
    )
    if should_lock and not args.no_lock:
        _lock_handle, acquired = acquire_process_lock(AUTO_DISCOVER_LOCK_FILE)
        if not acquired:
            print(f"\nâ›” Another auto_discover process is running.")
            print(f"   Lock file: {AUTO_DISCOVER_LOCK_FILE}")
            print("   If you are sure it's stale, delete the lock file and retry.")
            return

    # å‘ç°åŠŸèƒ½
    if args.region:
        # æ–°æ–¹å¼ï¼šæŒ‰åœ°åŒºæœç´¢
        product_type = getattr(args, 'type', 'mixed')
        if args.region == 'all':
            discover_all_regions(args.dry_run, product_type)
        else:
            discover_by_region(args.region, args.dry_run, product_type)
    elif args.source:
        # æ—§æ–¹å¼ï¼šæŒ‰æ¸ é“æœç´¢
        discover_from_source(args.source, args.dry_run)
    else:
        # é»˜è®¤ï¼šè¿è¡Œæ‰€æœ‰åœ°åŒºçš„ Perplexity Search
        print("\nğŸ’¡ æç¤ºï¼šä½¿ç”¨ --region å‚æ•°è¿›è¡Œåœ°åŒºæœç´¢ï¼ˆæ¨èï¼‰")
        print("   ç¤ºä¾‹: python tools/auto_discover.py --region us")
        print("   æˆ–è€…: python tools/auto_discover.py --region all")
        print("\n   ä½¿ç”¨ --source å‚æ•°è¿›è¡Œæ—§æ¸ é“æœç´¢")
        print("   ç¤ºä¾‹: python tools/auto_discover.py --source 36kr")
        print("\nè¿è¡Œ --help æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹")


if __name__ == '__main__':
    main()
