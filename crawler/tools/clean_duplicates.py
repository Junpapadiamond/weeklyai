#!/usr/bin/env python3
"""
äº§å“æ•°æ®æ¸…ç†å·¥å…·

åŠŸèƒ½ï¼š
1. æ£€æµ‹å¹¶ç§»é™¤é‡å¤äº§å“
2. ä¿®å¤ç¼ºå¤±çš„ slug
3. æ ‡è®°ç¼ºå¤± website çš„äº§å“
4. ç”Ÿæˆæ¸…ç†æŠ¥å‘Š
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.dedup import (
    DuplicateChecker,
    deduplicate_products,
    fix_missing_fields,
    generate_slug,
    normalize_domain,
    normalize_name,
    name_similarity,
    get_domain_key,
)


def load_products(filepath: str) -> List[Dict]:
    """åŠ è½½äº§å“æ•°æ®"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_products(products: List[Dict], filepath: str):
    """ä¿å­˜äº§å“æ•°æ®"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(products, f, ensure_ascii=False, indent=2)


def analyze_duplicates(products: List[Dict], similarity_threshold: float = 0.90):
    """åˆ†æžé‡å¤æƒ…å†µ"""
    print("\n" + "=" * 60)
    print("é‡å¤åˆ†æžæŠ¥å‘Š")
    print("=" * 60)
    
    # 1. åŸŸåé‡å¤ï¼ˆä½¿ç”¨ domain_keyï¼ŒåŒºåˆ†åŒä¸€å…¬å¸çš„ä¸åŒäº§å“ï¼‰
    domain_map: Dict[str, List[Dict]] = {}
    for p in products:
        domain_key = get_domain_key(p.get('website', ''))
        if domain_key:
            if domain_key not in domain_map:
                domain_map[domain_key] = []
            domain_map[domain_key].append(p)
    
    domain_dups = {k: v for k, v in domain_map.items() if len(v) > 1}
    
    print(f"\nðŸ“ åŸŸåé‡å¤ (åŒä¸€ URL è·¯å¾„): {len(domain_dups)} ä¸ª")
    for domain_key, items in domain_dups.items():
        print(f"\n  {domain_key}:")
        for item in items:
            print(f"    - {item.get('name')} (score: {item.get('dark_horse_index', '?')})")
    
    # 2. åç§°é‡å¤
    name_map: Dict[str, List[Dict]] = {}
    for p in products:
        name = normalize_name(p.get('name', ''))
        if name:
            if name not in name_map:
                name_map[name] = []
            name_map[name].append(p)
    
    name_dups = {k: v for k, v in name_map.items() if len(v) > 1}
    
    print(f"\nðŸ“› è§„èŒƒåŒ–åç§°é‡å¤: {len(name_dups)} ä¸ª")
    for name, items in name_dups.items():
        print(f"\n  [{name}]:")
        for item in items:
            print(f"    - {item.get('name')} ({item.get('website', 'no url')})")
    
    # 3. ç›¸ä¼¼åç§°
    print(f"\nðŸ” ç›¸ä¼¼åç§°æ£€æµ‹ (é˜ˆå€¼: {similarity_threshold:.0%}):")
    seen_pairs = set()
    similar_count = 0
    
    for i, p1 in enumerate(products):
        name1 = p1.get('name', '')
        if not name1:
            continue
        
        for p2 in products[i+1:]:
            name2 = p2.get('name', '')
            if not name2:
                continue
            
            sim = name_similarity(name1, name2)
            if sim >= similarity_threshold and sim < 1.0:
                pair = tuple(sorted([name1, name2]))
                if pair not in seen_pairs:
                    seen_pairs.add(pair)
                    similar_count += 1
                    print(f"\n  [{sim:.0%}] {name1}")
                    print(f"       â†” {name2}")
                    print(f"       URLs: {p1.get('website', 'N/A')} vs {p2.get('website', 'N/A')}")
    
    if similar_count == 0:
        print("  (æ— ç›¸ä¼¼åç§°)")
    
    # 4. ç¼ºå¤±å­—æ®µ
    missing_website = [p for p in products if not p.get('website')]
    missing_slug = [p for p in products if not p.get('slug')]
    
    print(f"\nâš ï¸  ç¼ºå¤±å­—æ®µ:")
    print(f"  - æ—  website: {len(missing_website)} ä¸ª")
    print(f"  - æ—  slug: {len(missing_slug)} ä¸ª")
    
    if missing_website:
        print("\n  æ—  website çš„äº§å“:")
        for p in missing_website[:10]:
            print(f"    - {p.get('name', 'N/A')}")
        if len(missing_website) > 10:
            print(f"    ... è¿˜æœ‰ {len(missing_website) - 10} ä¸ª")
    
    return {
        "domain_duplicates": len(domain_dups),
        "name_duplicates": len(name_dups),
        "similar_names": similar_count,
        "missing_website": len(missing_website),
        "missing_slug": len(missing_slug),
    }


def clean_products(
    products: List[Dict],
    similarity_threshold: float = 0.85,
    fix_slugs: bool = True,
    remove_duplicates: bool = True,
    keep: str = "best"
) -> tuple:
    """
    æ¸…ç†äº§å“æ•°æ®
    
    Returns:
        (cleaned_products, removed_products, stats)
    """
    stats = {
        "original_count": len(products),
        "duplicates_removed": 0,
        "slugs_fixed": 0,
        "missing_website": 0,
    }
    
    # 1. ä¿®å¤ç¼ºå¤±çš„ slug
    if fix_slugs:
        for p in products:
            if not p.get('slug'):
                name = p.get('name', '')
                if name:
                    p['slug'] = generate_slug(name)
                    stats["slugs_fixed"] += 1
    
    # 2. åŽ»é‡
    removed = []
    if remove_duplicates:
        products, removed = deduplicate_products(
            products,
            similarity_threshold=similarity_threshold,
            keep=keep
        )
        stats["duplicates_removed"] = len(removed)
    
    # 3. ç»Ÿè®¡ç¼ºå¤± website
    stats["missing_website"] = len([p for p in products if not p.get('website')])
    stats["final_count"] = len(products)
    
    return products, removed, stats


def main():
    parser = argparse.ArgumentParser(description="äº§å“æ•°æ®æ¸…ç†å·¥å…·")
    parser.add_argument(
        "--input", "-i",
        default="data/products_featured.json",
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output", "-o",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è¦†ç›–è¾“å…¥æ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "--backup", "-b",
        action="store_true",
        help="æ¸…ç†å‰å¤‡ä»½åŽŸæ–‡ä»¶"
    )
    parser.add_argument(
        "--analyze-only", "-a",
        action="store_true",
        help="ä»…åˆ†æžï¼Œä¸ä¿®æ”¹æ–‡ä»¶"
    )
    parser.add_argument(
        "--threshold", "-t",
        type=float,
        default=0.85,
        help="åç§°ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤: 0.85)"
    )
    parser.add_argument(
        "--keep",
        choices=["first", "best"],
        default="best",
        help="ä¿ç•™ç­–ç•¥: first=ä¿ç•™ç¬¬ä¸€ä¸ª, best=ä¿ç•™è¯„åˆ†æœ€é«˜çš„ (é»˜è®¤: best)"
    )
    parser.add_argument(
        "--no-fix-slugs",
        action="store_true",
        help="ä¸ä¿®å¤ç¼ºå¤±çš„ slug"
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šæ–‡ä»¶è·¯å¾„
    script_dir = Path(__file__).parent.parent
    input_path = script_dir / args.input
    output_path = Path(args.output) if args.output else input_path
    
    # åŠ è½½æ•°æ®
    print(f"ðŸ“‚ åŠ è½½æ•°æ®: {input_path}")
    products = load_products(input_path)
    print(f"   å…± {len(products)} ä¸ªäº§å“")
    
    # åˆ†æž
    analysis = analyze_duplicates(products, args.threshold)
    
    if args.analyze_only:
        print("\nâœ… ä»…åˆ†æžæ¨¡å¼ï¼Œæœªä¿®æ”¹æ–‡ä»¶")
        return
    
    # æ¸…ç†
    print("\n" + "=" * 60)
    print("å¼€å§‹æ¸…ç†...")
    print("=" * 60)
    
    cleaned, removed, stats = clean_products(
        products,
        similarity_threshold=args.threshold,
        fix_slugs=not args.no_fix_slugs,
        keep=args.keep
    )
    
    # å¤‡ä»½
    if args.backup:
        backup_path = input_path.with_suffix(f'.backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
        save_products(products, backup_path)
        print(f"\nðŸ’¾ å·²å¤‡ä»½åˆ°: {backup_path}")
    
    # ä¿å­˜
    save_products(cleaned, output_path)
    print(f"\nðŸ’¾ å·²ä¿å­˜åˆ°: {output_path}")
    
    # ä¿å­˜è¢«ç§»é™¤çš„é‡å¤é¡¹
    if removed:
        removed_path = script_dir / "data" / "duplicates_removed.json"
        save_products(removed, removed_path)
        print(f"ðŸ—‘ï¸  é‡å¤é¡¹å·²ä¿å­˜åˆ°: {removed_path}")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("æ¸…ç†ç»Ÿè®¡")
    print("=" * 60)
    print(f"  åŽŸå§‹æ•°é‡: {stats['original_count']}")
    print(f"  ç§»é™¤é‡å¤: {stats['duplicates_removed']}")
    print(f"  ä¿®å¤ slug: {stats['slugs_fixed']}")
    print(f"  æœ€ç»ˆæ•°é‡: {stats['final_count']}")
    print(f"  ä»ç¼º website: {stats['missing_website']}")
    
    if removed:
        print(f"\nðŸ—‘ï¸  è¢«ç§»é™¤çš„é‡å¤é¡¹:")
        for p in removed[:10]:
            reason = p.get('_duplicate_reason', 'unknown')
            print(f"    - {p.get('name')}: {reason}")
        if len(removed) > 10:
            print(f"    ... è¿˜æœ‰ {len(removed) - 10} ä¸ª")


if __name__ == "__main__":
    main()
