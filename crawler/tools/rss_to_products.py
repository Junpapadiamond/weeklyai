#!/usr/bin/env python3
"""
RSS æ–°é—» â†’ äº§å“æ•°æ®è½¬æ¢æ¨¡å—

æµç¨‹:
1. è¯»å– RSS æ–°é—»æ–‡ç«  (blogs_news.json)
2. ç­›é€‰åŒ…å«äº§å“/èèµ„ä¿¡æ¯çš„æ–‡ç« 
3. ç”¨ LLM æå–äº§å“ä¿¡æ¯
4. è¯„ä¼°æ˜¯å¦ç¬¦åˆé»‘é©¬æ ‡å‡†
5. è¾“å‡ºåˆ°å€™é€‰æ±  (candidates/)

ä½¿ç”¨:
    python tools/rss_to_products.py                    # å¤„ç†æ‰€æœ‰æ–°é—»
    python tools/rss_to_products.py --limit 10         # åªå¤„ç† 10 ç¯‡
    python tools/rss_to_products.py --dry-run          # æµ‹è¯•æ¨¡å¼
    python tools/rss_to_products.py --source TechCrunch # æŒ‡å®šæ¥æº
"""

import json
import os
import sys
import re
import time
from datetime import datetime
from typing import List, Dict, Optional, Any
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

from dotenv import load_dotenv
load_dotenv(os.path.join(PROJECT_ROOT, '.env'))

# ============================================
# é…ç½®
# ============================================

DATA_DIR = os.path.join(PROJECT_ROOT, 'data')
BLOGS_NEWS_FILE = os.path.join(DATA_DIR, 'blogs_news.json')
CANDIDATES_DIR = os.path.join(DATA_DIR, 'candidates')
PRODUCTS_FEATURED_FILE = os.path.join(DATA_DIR, 'products_featured.json')

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(CANDIDATES_DIR, exist_ok=True)

# äº§å“æåŠå…³é”®è¯ (ç”¨äºåˆç­›)
PRODUCT_KEYWORDS = [
    # èèµ„ç›¸å…³
    "raises", "raised", "funding", "Series A", "Series B", "Series C", "seed round",
    "valuation", "unicorn", "investment", "æŠ•èµ„", "èèµ„", "ä¼°å€¼", "Aè½®", "Bè½®",
    # äº§å“å‘å¸ƒ
    "launches", "launched", "announces", "announced", "releases", "released",
    "introduces", "unveiled", "å‘å¸ƒ", "æ¨å‡º", "ä¸Šçº¿",
    # å…¬å¸åŠ¨æ€
    "startup", "founded", "founded by", "åˆ›ä¸š", "åˆ›å§‹äºº",
    # æ’é™¤è¯ (å¤§å…¬å¸åŠ¨æ€)
    # "OpenAI", "Google", "Microsoft", "Meta", "Apple", "Amazon",
]

# å¤§å…¬å¸åå• (ç¡¬ç¼–ç ï¼Œç”¨äºåå¤„ç†æ£€æµ‹)
BIG_COMPANY_KEYWORDS = {
    # å…¬å¸å -> æ˜¾ç¤ºå
    "openai": "OpenAI",
    "chatgpt": "OpenAI",
    "gpt-": "OpenAI",
    "dall-e": "OpenAI",
    "sora": "OpenAI",
    "google": "Google",
    "gemini": "Google",
    "deepmind": "Google",
    "veo": "Google",
    "imagen": "Google",
    "google flow": "Google",  # Google Labs Flow (ç²¾ç¡®åŒ¹é…é¿å…è¯¯åˆ¤)
    "anthropic": "Anthropic",
    "claude": "Anthropic",
    "microsoft": "Microsoft",
    "copilot": "Microsoft",
    "meta": "Meta",
    "llama": "Meta",
    "apple": "Apple",
    "amazon": "Amazon",
    "alexa": "Amazon",
    "nvidia": "Nvidia",
    "tesla": "Tesla",
    "alibaba": "Alibaba",
    "qwen": "Alibaba",
    "tencent": "Tencent",
    "baidu": "Baidu",
    "ernie": "Baidu",
    "bytedance": "ByteDance",
    "doubao": "ByteDance",
}

# æ’é™¤å¤§å…¬å¸åŠå…¶äº§å“ (Focus: é»‘é©¬åˆ›ä¸šå…¬å¸)
EXCLUDE_BIG_COMPANY_PRODUCTS = True  # è®¾ä¸º False å¯æ”¶å½•å¤§å…¬å¸äº§å“

# çº¯å…¬å¸åæ’é™¤
EXCLUDE_TERMS = {
    "openai", "google", "microsoft", "meta", "apple", "amazon", 
    "nvidia", "anthropic", "alibaba", "tencent", "baidu", "bytedance",
}

# ============================================
# LLM å®¢æˆ·ç«¯
# ============================================

def get_llm_client():
    """è·å– LLM å®¢æˆ·ç«¯ (ä¼˜å…ˆ Perplexityï¼Œå…¶æ¬¡ GLM)"""
    
    # å°è¯• Perplexity
    perplexity_key = os.getenv('PERPLEXITY_API_KEY')
    if perplexity_key:
        try:
            from utils.perplexity_client import PerplexityClient
            client = PerplexityClient(api_key=perplexity_key)
            if client.is_available():
                return ("perplexity", client)
        except Exception as e:
            print(f"  âš ï¸ Perplexity åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # å°è¯• GLM
    glm_key = os.getenv('ZHIPU_API_KEY')
    if glm_key:
        try:
            from zhipuai import ZhipuAI
            client = ZhipuAI(api_key=glm_key)
            return ("glm", client)
        except Exception as e:
            print(f"  âš ï¸ GLM åˆå§‹åŒ–å¤±è´¥: {e}")
    
    return (None, None)


# ============================================
# äº§å“æå– Prompt
# ============================================

EXTRACTION_PROMPT = """åˆ†æä»¥ä¸‹æ–°é—»æ–‡ç« ï¼Œæå–å…¶ä¸­æåˆ°çš„ AI äº§å“æˆ–åˆ›ä¸šå…¬å¸ä¿¡æ¯ã€‚

æ–‡ç« æ ‡é¢˜: {title}
æ–‡ç« æ¥æº: {source}
æ–‡ç« å†…å®¹: {content}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœæ–‡ç« ä¸­æ²¡æœ‰æåˆ°å…·ä½“äº§å“/å…¬å¸ï¼Œè¿”å›ç©º JSONï¼‰ï¼š

{{
  "has_product": true/false,  // æ˜¯å¦åŒ…å«å¯æ”¶å½•çš„äº§å“ä¿¡æ¯
  "products": [
    {{
      "name": "äº§å“/å…¬å¸åç§°",
      "website": "å®˜ç½‘ URL (å¦‚æœæ–‡ç« æåˆ°)",
      "description": "ä¸€å¥è¯äº§å“æè¿° (50å­—ä»¥å†…)",
      "category": "ç±»åˆ«: coding/image/video/voice/writing/agent/hardware/finance/education/healthcare/other",
      "is_hardware": false,  // æ˜¯å¦æ˜¯ç¡¬ä»¶äº§å“
      "hardware_category": "",  // å¦‚æœæ˜¯ç¡¬ä»¶: ai_chip/robotics/smart_glasses/wearables/drone/edge_ai
      "funding_total": "èèµ„é‡‘é¢ (å¦‚ $50M, $1.2B)",
      "funding_stage": "èèµ„é˜¶æ®µ (Seed/Series A/B/C)",
      "founded_date": "æˆç«‹å¹´ä»½",
      "region": "åœ°åŒº: ğŸ‡ºğŸ‡¸/ğŸ‡¨ğŸ‡³/ğŸ‡ªğŸ‡º/ğŸ‡¯ğŸ‡µ/ğŸ‡°ğŸ‡·/ğŸ‡¸ğŸ‡¬",
      "why_matters": "ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ (ä¸€å¥è¯ï¼Œè¦å…·ä½“ï¼ŒåŒ…å«æ•°æ®)",
      "dark_horse_score": 1-5,  // é»‘é©¬è¯„åˆ†
      "score_reason": "è¯„åˆ†ç†ç”±"
    }}
  ]
}}

ã€é‡è¦ã€‘æˆ‘ä»¬çš„æ ¸å¿ƒç›®æ ‡æ˜¯å‘ç°ã€Œé»‘é©¬ã€å’Œã€Œæ½œåŠ›æ–°äººã€ï¼š
- **ä¼˜å…ˆæå–åˆ›ä¸šå…¬å¸** - èèµ„æ–°é—»ã€æ–°äº§å“å‘å¸ƒã€å¿«é€Ÿå¢é•¿çš„å°å…¬å¸
- **å¤§å…¬å¸äº§å“æ¬¡è¦** - åªæœ‰éå¸¸åˆ›æ–°çš„æ–°äº§å“æ‰å€¼å¾—æ”¶å½•

è¯„åˆ†æ ‡å‡†:
- 5åˆ†: åˆ›ä¸šå…¬å¸èèµ„>$100M / å“ç±»å¼€åˆ›è€… / å¢é•¿å¼‚å¸¸å¿«
- 4åˆ†: åˆ›ä¸šå…¬å¸èèµ„>$30M / ARR>$10M / é¡¶çº§VCèƒŒä¹¦
- 3åˆ†: èèµ„$1M-$30M / ProductHuntä¸Šæ¦œ / æœ‰æ˜æ˜¾å¢é•¿
- 2åˆ†: åˆšå‘å¸ƒ/æ•°æ®ä¸è¶³ ä½†æœ‰åˆ›æ–°ç‚¹
- 1åˆ†: ä¿¡æ¯ä¸è¶³ / æ™®é€šäº§å“
- å¤§å…¬å¸æ–°äº§å“: éå¸¸åˆ›æ–°å¯ç»™4-5åˆ†ï¼Œæ™®é€šæ›´æ–°1-2åˆ†

æ³¨æ„:
1. åªæå–æ˜ç¡®çš„äº§å“/å…¬å¸ï¼Œä¸è¦çŒœæµ‹
2. çº¯å…¬å¸åä¸ç®—äº§å“ (å¦‚ "OpenAI" ä¸æ˜¯äº§å“ï¼Œä½† "ChatGPT Health" æ˜¯äº§å“)
3. why_matters å¿…é¡»å…·ä½“ï¼ŒåŒ…å«æ•°å­—/äº‹å®ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
4. å¦‚æœæ–‡ç« åªæ˜¯è¡Œä¸šåˆ†æ/è§‚ç‚¹ï¼Œhas_product è®¾ä¸º false

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""


# ============================================
# LLM è°ƒç”¨
# ============================================

def extract_products_with_llm(article: Dict, llm_type: str, llm_client: Any) -> List[Dict]:
    """ä½¿ç”¨ LLM ä»æ–‡ç« ä¸­æå–äº§å“ä¿¡æ¯"""
    
    title = article.get('title', '')
    source = article.get('source', '')
    content = article.get('summary', '')
    
    prompt = EXTRACTION_PROMPT.format(
        title=title,
        source=source,
        content=content
    )
    
    try:
        if llm_type == "perplexity":
            response = llm_client.analyze(prompt=prompt)
            # analyze è¿”å›è§£æåçš„ JSON æˆ–å­—ç¬¦ä¸²
            if isinstance(response, dict):
                result_text = json.dumps(response)
            elif isinstance(response, list):
                result_text = json.dumps({"has_product": True, "products": response})
            else:
                result_text = str(response)
        
        elif llm_type == "glm":
            response = llm_client.chat.completions.create(
                model="glm-4-flash",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª AI äº§å“åˆ†æå¸ˆï¼Œä¸“é—¨ä»æ–°é—»ä¸­æå–äº§å“ä¿¡æ¯ã€‚åªè¿”å› JSON æ ¼å¼ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
            )
            result_text = response.choices[0].message.content
        
        else:
            return []
        
        # è§£æ JSON
        json_match = re.search(r'\{[\s\S]*\}', result_text)
        if json_match:
            result = json.loads(json_match.group())
            if result.get('has_product') and result.get('products'):
                return result['products']
        
        return []
    
    except Exception as e:
        print(f"    âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        return []


# ============================================
# äº§å“éªŒè¯å’Œæ ‡å‡†åŒ–
# ============================================

def search_website(name: str, category: str, llm_type: str, llm_client: Any) -> str:
    """æœç´¢äº§å“å®˜ç½‘"""
    if llm_type != "perplexity":
        return ""
    
    try:
        query = f"{name} {category} official website"
        results = llm_client.search(query=query, max_results=3)
        
        if results:
            # ä¼˜å…ˆé€‰æ‹©äº§å“å®˜ç½‘
            for r in results:
                url = r.url.lower()
                name_clean = name.lower().replace(' ', '').replace('-', '')
                if name_clean[:4] in url or any(domain in url for domain in ['.com', '.ai', '.io']):
                    return r.url
            return results[0].url
        return ""
    except Exception:
        return ""


def validate_product(product: Dict, article: Dict, llm_type: str = None, llm_client: Any = None) -> Optional[Dict]:
    """éªŒè¯å’Œæ ‡å‡†åŒ–äº§å“æ•°æ®"""
    
    name = product.get('name', '').strip()
    if not name or len(name) < 2:
        return None
    
    # æ’é™¤çº¯å…¬å¸å (ä¸æ˜¯å…·ä½“äº§å“)
    if name.lower() in EXCLUDE_TERMS:
        return None
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    score = product.get('dark_horse_score', 0)
    if score < 2:
        return None  # è¯„åˆ†å¤ªä½ï¼Œä¸æ”¶å½•
    
    why_matters = product.get('why_matters', '')
    if not why_matters or len(why_matters) < 10:
        return None  # æ²¡æœ‰è¯´æ˜ä¸ºä»€ä¹ˆé‡è¦
    
    # è·å–ç½‘ç«™ (å…ˆä»äº§å“æå–ï¼Œæ²¡æœ‰åˆ™æœç´¢è¡¥å……)
    website = product.get('website', '')
    if not website and llm_client:
        website = search_website(name, product.get('category', ''), llm_type, llm_client)
    
    # æ£€æµ‹å¹¶æ’é™¤å¤§å…¬å¸äº§å“ (Focus: é»‘é©¬å’Œåˆ›ä¸šå…¬å¸)
    name_lower = name.lower()
    website_lower = website.lower() if website else ""
    
    # æ£€æŸ¥äº§å“åæ˜¯å¦åŒ…å«å¤§å…¬å¸å…³é”®è¯
    for keyword in BIG_COMPANY_KEYWORDS.keys():
        if keyword in name_lower:
            return None  # æ’é™¤å¤§å…¬å¸äº§å“
    
    # æ£€æŸ¥ç½‘ç«™æ˜¯å¦å±äºå¤§å…¬å¸
    big_company_domains = [
        "openai.com", "anthropic.com", "claude.ai", "claude.com",
        "google.com", "labs.google", "deepmind.google",
        "microsoft.com", "meta.com", "apple.com", "amazon.com",
        "nvidia.com", "alibaba.com", "tencent.com", "baidu.com", "bytedance.com"
    ]
    for domain in big_company_domains:
        if domain in website_lower:
            return None  # æ’é™¤å¤§å…¬å¸äº§å“
    
    is_big_company = False
    parent_company = ""
    
    # æ ‡å‡†åŒ–æ•°æ®
    standardized = {
        "name": name,
        "slug": name.lower().replace(' ', '-').replace('.', '-'),
        "website": website,
        "description": product.get('description', '')[:200],
        "category": product.get('category', 'other'),
        "is_hardware": product.get('is_hardware', False),
        "hardware_category": product.get('hardware_category', ''),
        "is_big_company": is_big_company,  # æ ‡è®°å¤§å…¬å¸äº§å“
        "parent_company": parent_company,  # æ¯å…¬å¸åç§°
        "funding_total": product.get('funding_total', ''),
        "funding_stage": product.get('funding_stage', ''),
        "founded_date": product.get('founded_date', ''),
        "region": product.get('region', 'ğŸ‡ºğŸ‡¸'),
        "dark_horse_index": min(5, max(1, int(score))),
        "why_matters": why_matters[:300],
        "criteria_met": [product.get('score_reason', '')],
        "discovered_at": datetime.now().strftime('%Y-%m-%d'),
        "source": article.get('source', ''),
        "source_url": article.get('link', ''),
        "source_title": article.get('title', ''),
    }
    
    return standardized


def is_duplicate(product: Dict, existing_products: List[Dict]) -> bool:
    """æ£€æŸ¥äº§å“æ˜¯å¦é‡å¤"""
    name = product.get('name', '').lower().replace(' ', '')
    website = product.get('website', '').lower()
    
    for existing in existing_products:
        existing_name = existing.get('name', '').lower().replace(' ', '')
        existing_website = existing.get('website', '').lower()
        
        if name == existing_name:
            return True
        if website and existing_website and website in existing_website:
            return True
    
    return False


# ============================================
# ä¸»æµç¨‹
# ============================================

def load_existing_products() -> List[Dict]:
    """åŠ è½½å·²æœ‰äº§å“ (ç”¨äºå»é‡)"""
    products = []
    
    # åŠ è½½ featured äº§å“
    if os.path.exists(PRODUCTS_FEATURED_FILE):
        with open(PRODUCTS_FEATURED_FILE, 'r') as f:
            products.extend(json.load(f))
    
    # åŠ è½½å€™é€‰æ± 
    for filename in os.listdir(CANDIDATES_DIR):
        if filename.endswith('.json'):
            filepath = os.path.join(CANDIDATES_DIR, filename)
            with open(filepath, 'r') as f:
                data = json.load(f)
                if isinstance(data, list):
                    products.extend(data)
    
    return products


def filter_articles(articles: List[Dict], source_filter: str = None) -> List[Dict]:
    """ç­›é€‰åŒ…å«äº§å“ä¿¡æ¯çš„æ–‡ç« """
    filtered = []
    
    for article in articles:
        # æ¥æºç­›é€‰
        if source_filter and source_filter.lower() not in article.get('source', '').lower():
            continue
        
        # å…³é”®è¯ç­›é€‰
        text = f"{article.get('title', '')} {article.get('summary', '')}".lower()
        has_keyword = any(kw.lower() in text for kw in PRODUCT_KEYWORDS)
        
        if has_keyword or article.get('has_product_mention'):
            filtered.append(article)
    
    return filtered


def process_articles(
    articles: List[Dict],
    llm_type: str,
    llm_client: Any,
    existing_products: List[Dict],
    dry_run: bool = False
) -> List[Dict]:
    """å¤„ç†æ–‡ç« ï¼Œæå–äº§å“"""
    
    extracted_products = []
    
    for i, article in enumerate(articles):
        title = article.get('title', '')[:60]
        source = article.get('source', '')
        
        print(f"\n[{i+1}/{len(articles)}] {source}")
        print(f"  ğŸ“° {title}...")
        
        # è°ƒç”¨ LLM æå–
        products = extract_products_with_llm(article, llm_type, llm_client)
        
        if not products:
            print(f"  â­ï¸ æ— äº§å“ä¿¡æ¯")
            continue
        
        for product in products:
            # éªŒè¯ (ä¼ é€’ LLM å®¢æˆ·ç«¯ç”¨äºæœç´¢ç½‘ç«™)
            validated = validate_product(product, article, llm_type, llm_client)
            if not validated:
                print(f"  â­ï¸ {product.get('name', '?')} - éªŒè¯æœªé€šè¿‡")
                continue
            
            # å»é‡
            if is_duplicate(validated, existing_products + extracted_products):
                print(f"  â­ï¸ {validated['name']} - å·²å­˜åœ¨")
                continue
            
            score = validated['dark_horse_index']
            print(f"  âœ… {validated['name']} ({score}åˆ†) - {validated['why_matters'][:40]}...")
            extracted_products.append(validated)
        
        # é€Ÿç‡é™åˆ¶
        time.sleep(1)
    
    return extracted_products


def save_products(products: List[Dict], dry_run: bool = False):
    """ä¿å­˜äº§å“åˆ°å€™é€‰æ± """
    if not products:
        print("\nğŸ“­ æ²¡æœ‰æ–°äº§å“éœ€è¦ä¿å­˜")
        return
    
    if dry_run:
        print(f"\nğŸ§ª [DRY RUN] å°†ä¿å­˜ {len(products)} ä¸ªäº§å“")
        return
    
    # æŒ‰æ—¥æœŸä¿å­˜
    today = datetime.now().strftime('%Y%m%d')
    filename = f"rss_candidates_{today}.json"
    filepath = os.path.join(CANDIDATES_DIR, filename)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ 
    existing = []
    if os.path.exists(filepath):
        with open(filepath, 'r') as f:
            existing = json.load(f)
    
    # åˆå¹¶å¹¶å»é‡
    all_products = existing + products
    seen = set()
    unique = []
    for p in all_products:
        key = p.get('name', '').lower()
        if key not in seen:
            seen.add(key)
            unique.append(p)
    
    with open(filepath, 'w') as f:
        json.dump(unique, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ä¿å­˜ {len(products)} ä¸ªæ–°äº§å“åˆ° {filepath}")


# ============================================
# CLI
# ============================================

def main():
    parser = argparse.ArgumentParser(description="RSS æ–°é—» â†’ äº§å“æ•°æ®è½¬æ¢")
    parser.add_argument("--limit", type=int, default=50, help="å¤„ç†æ–‡ç« æ•°é‡ä¸Šé™")
    parser.add_argument("--source", type=str, help="åªå¤„ç†æŒ‡å®šæ¥æºçš„æ–‡ç« ")
    parser.add_argument("--dry-run", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼Œä¸ä¿å­˜")
    parser.add_argument("--input", type=str, help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    print("ğŸ”„ RSS æ–°é—» â†’ äº§å“æ•°æ®è½¬æ¢")
    print("=" * 50)
    
    # è¯»å–æ–°é—»
    input_file = args.input or BLOGS_NEWS_FILE
    if not os.path.exists(input_file):
        print(f"âŒ æ–°é—»æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print("è¯·å…ˆè¿è¡Œ: python tools/rss_feeds.py")
        return
    
    with open(input_file, 'r') as f:
        articles = json.load(f)
    
    print(f"ğŸ“° è¯»å– {len(articles)} ç¯‡æ–°é—»")
    
    # ç­›é€‰æ–‡ç« 
    filtered = filter_articles(articles, args.source)
    print(f"ğŸ” ç­›é€‰å‡º {len(filtered)} ç¯‡åŒ…å«äº§å“ä¿¡æ¯çš„æ–‡ç« ")
    
    if not filtered:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ…å«äº§å“ä¿¡æ¯çš„æ–‡ç« ")
        return
    
    # é™åˆ¶æ•°é‡
    filtered = filtered[:args.limit]
    
    # è·å– LLM å®¢æˆ·ç«¯
    print("\nğŸ¤– åˆå§‹åŒ– LLM...")
    llm_type, llm_client = get_llm_client()
    
    if not llm_client:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ LLM å®¢æˆ·ç«¯")
        print("è¯·é…ç½® PERPLEXITY_API_KEY æˆ– ZHIPU_API_KEY")
        return
    
    print(f"  âœ… ä½¿ç”¨ {llm_type}")
    
    # åŠ è½½å·²æœ‰äº§å“
    existing = load_existing_products()
    print(f"ğŸ“¦ å·²æœ‰ {len(existing)} ä¸ªäº§å“")
    
    # å¤„ç†æ–‡ç« 
    print(f"\nğŸ”„ å¼€å§‹å¤„ç† {len(filtered)} ç¯‡æ–‡ç« ...")
    products = process_articles(
        filtered,
        llm_type,
        llm_client,
        existing,
        args.dry_run
    )
    
    # ä¿å­˜
    save_products(products, args.dry_run)
    
    # ç»Ÿè®¡
    print("\nğŸ“Š ç»Ÿè®¡:")
    print(f"  - å¤„ç†æ–‡ç« : {len(filtered)}")
    print(f"  - æå–äº§å“: {len(products)}")
    
    if products:
        scores = [p.get('dark_horse_index', 0) for p in products]
        print(f"  - 5åˆ†äº§å“: {scores.count(5)}")
        print(f"  - 4åˆ†äº§å“: {scores.count(4)}")
        print(f"  - 3åˆ†äº§å“: {scores.count(3)}")
        print(f"  - 2åˆ†äº§å“: {scores.count(2)}")


if __name__ == "__main__":
    main()
