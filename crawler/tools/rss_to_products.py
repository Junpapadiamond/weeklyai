#!/usr/bin/env python3
"""
RSS æ–°é—» â†’ äº§å“æ•°æ®è½¬æ¢æ¨¡å—

æµç¨‹:
1. è¯»å– RSS æ–°é—»æ–‡ç«  (blogs_news.json)
2. ç­›é€‰åŒ…å«äº§å“/èèµ„ä¿¡æ¯çš„æ–‡ç« 
3. ç”¨ LLM æå–äº§å“ä¿¡æ¯
4. è¯„ä¼°æ˜¯å¦ç¬¦åˆé»‘é©¬æ ‡å‡†
5. è¾“å‡ºåˆ°å€™é€‰æ±  (candidates/)

ä½¿ç”¨:
    python tools/rss_to_products.py                    # å¤„ç†æ‰€æœ‰æ–°é—»
    python tools/rss_to_products.py --limit 10         # åªå¤„ç† 10 ç¯‡
    python tools/rss_to_products.py --dry-run          # æµ‹è¯•æ¨¡å¼
    python tools/rss_to_products.py --source TechCrunch # æŒ‡å®šæ¥æº
"""

import json
import os
import sys
import re
import time
from datetime import datetime, timezone
from typing import List, Dict, Optional, Any, Set, Tuple
import argparse
from urllib.parse import urlparse

from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

REPO_ROOT = os.path.dirname(PROJECT_ROOT)
load_dotenv(os.path.join(REPO_ROOT, '.env'))
load_dotenv(os.path.join(PROJECT_ROOT, '.env'))

# ============================================
# é…ç½®
# ============================================

DATA_DIR = os.path.join(PROJECT_ROOT, 'data')
BLOGS_NEWS_FILE = os.path.join(DATA_DIR, 'blogs_news.json')
CANDIDATES_DIR = os.path.join(DATA_DIR, 'candidates')
PRODUCTS_FEATURED_FILE = os.path.join(DATA_DIR, 'products_featured.json')
INDUSTRY_LEADERS_FILE = os.path.join(DATA_DIR, 'industry_leaders.json')
PENDING_REVIEW_FILE = os.path.join(CANDIDATES_DIR, 'pending_review.json')
DEFAULT_CACHE_FILE = os.path.join(CANDIDATES_DIR, 'rss_to_products_cache.json')

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(CANDIDATES_DIR, exist_ok=True)

def safe_load_json(path: str, default):
    if not path or not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default


def safe_save_json(path: str, payload) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def normalize_domain(url: str) -> str:
    if not url:
        return ""
    url = url.strip()
    if not url:
        return ""
    if not url.startswith(("http://", "https://")) and "." in url:
        url = f"https://{url}"
    try:
        parsed = urlparse(url)
    except Exception:
        return url.lower()
    host = (parsed.netloc or "").lower()
    if host.startswith("www."):
        host = host[4:]
    return host or url.lower()


def normalize_name(value: str) -> str:
    return re.sub(r"[^a-z0-9]", "", (value or "").lower())


def normalize_name_key(value: str) -> str:
    """Strict normalized name key used for safe featured enrich fallback."""
    key = normalize_name(value)
    return key if len(key) >= 4 else ""


_INDUSTRY_LEADERS_CACHE: Optional[Tuple[Set[str], Set[str]]] = None


def load_industry_leader_index() -> Tuple[Set[str], Set[str]]:
    """
    Load industry leader products (name + domain) for exclusion.

    Source: crawler/data/industry_leaders.json
    """
    global _INDUSTRY_LEADERS_CACHE
    if _INDUSTRY_LEADERS_CACHE is not None:
        return _INDUSTRY_LEADERS_CACHE

    data = safe_load_json(INDUSTRY_LEADERS_FILE, {}) or {}
    names: Set[str] = set()
    domains: Set[str] = set()

    try:
        categories = (data.get("categories") or {}) if isinstance(data, dict) else {}
        if isinstance(categories, dict):
            for cat in categories.values():
                products = (cat or {}).get("products") if isinstance(cat, dict) else None
                if not isinstance(products, list):
                    continue
                for p in products:
                    if not isinstance(p, dict):
                        continue
                    n = normalize_name(p.get("name", ""))
                    if n:
                        names.add(n)
                    d = normalize_domain(p.get("website", ""))
                    if d:
                        domains.add(d)
    except Exception:
        # Best-effort; on error treat as empty.
        names = set()
        domains = set()

    _INDUSTRY_LEADERS_CACHE = (names, domains)
    return _INDUSTRY_LEADERS_CACHE


def is_industry_leader(name: str, website: str) -> bool:
    """Return True if the product matches an industry leader by normalized name or website domain."""
    names, domains = load_industry_leader_index()
    name_norm = normalize_name(name)
    if name_norm and name_norm in names:
        return True
    domain = normalize_domain(website)
    if domain and domain in domains:
        return True
    return False


def to_iso(dt: datetime) -> str:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.isoformat().replace("+00:00", "Z")


def parse_date(value: str) -> Optional[datetime]:
    if not value:
        return None
    try:
        cleaned = value.replace("Z", "+00:00")
        return datetime.fromisoformat(cleaned).astimezone(timezone.utc)
    except Exception:
        pass
    try:
        return datetime.strptime(value[:10], "%Y-%m-%d").replace(tzinfo=timezone.utc)
    except Exception:
        return None


def normalize_article(raw: Dict[str, Any]) -> Dict[str, Any]:
    """Normalize different news/blog schemas into a single article shape."""
    extra = raw.get("extra") or {}
    if not isinstance(extra, dict):
        extra = {}

    title = (raw.get("title") or raw.get("name") or "").strip()
    summary = (raw.get("summary") or raw.get("description") or raw.get("snippet") or "").strip()
    link = (raw.get("link") or raw.get("website") or raw.get("source_url") or "").strip()
    source = (raw.get("source") or "").strip()

    published_at = (
        raw.get("published_at")
        or raw.get("published")
        or raw.get("first_seen")
        or raw.get("discovered_at")
        or ""
    )

    return {
        "title": title,
        "summary": summary,
        "link": link,
        "source": source,
        "published_at": published_at,
        "extra": extra,
        "has_product_mention": bool(raw.get("has_product_mention")),
    }


def article_key(article: Dict[str, Any]) -> str:
    link = (article.get("link") or "").strip()
    if link:
        return link
    return f"{article.get('source', '')}:{normalize_name(article.get('title', ''))}"


def build_signal(article: Dict[str, Any]) -> Dict[str, Any]:
    source = (article.get("source") or "").lower().strip()
    title = (article.get("title") or "").strip()
    link = (article.get("link") or "").strip()
    summary = (article.get("summary") or "").strip()
    published_at = (article.get("published_at") or "").strip()
    extra = article.get("extra") or {}
    if not isinstance(extra, dict):
        extra = {}

    author = ""
    if source == "youtube":
        author = (extra.get("channel") or extra.get("author") or "").strip()
    elif source == "x":
        handle = (extra.get("author_handle") or extra.get("author") or "").strip()
        if handle and not handle.startswith("@"):
            handle = f"@{handle}"
        author = handle

    signal = {
        "platform": source,
        "url": link,
        "title": title[:140],
        "published_at": published_at[:40],
        "snippet": summary[:280],
        "author": author[:80],
    }
    return {k: v for k, v in signal.items() if v}

# äº§å“æåŠå…³é”®è¯ (ç”¨äºåˆç­›)
PRODUCT_KEYWORDS = [
    # èèµ„ç›¸å…³
    "raises", "raised", "funding", "Series A", "Series B", "Series C", "seed round",
    "valuation", "unicorn", "investment", "æŠ•èµ„", "èèµ„", "ä¼°å€¼", "Aè½®", "Bè½®",
    # äº§å“å‘å¸ƒ
    "launches", "launched", "announces", "announced", "releases", "released",
    "introduces", "unveiled", "å‘å¸ƒ", "æ¨å‡º", "ä¸Šçº¿",
    # å…¬å¸åŠ¨æ€
    "startup", "founded", "founded by", "åˆ›ä¸š", "åˆ›å§‹äºº",
    # æ’é™¤è¯ (å¤§å…¬å¸åŠ¨æ€)
    # "OpenAI", "Google", "Microsoft", "Meta", "Apple", "Amazon",
]

# å¤§å…¬å¸åå• (ç¡¬ç¼–ç ï¼Œç”¨äºåå¤„ç†æ£€æµ‹)
BIG_COMPANY_KEYWORDS = {
    # å…¬å¸å -> æ˜¾ç¤ºå
    "openai": "OpenAI",
    "chatgpt": "OpenAI",
    "gpt-": "OpenAI",
    "dall-e": "OpenAI",
    "sora": "OpenAI",
    "google": "Google",
    "gemini": "Google",
    "deepmind": "Google",
    "veo": "Google",
    "imagen": "Google",
    "google flow": "Google",  # Google Labs Flow (ç²¾ç¡®åŒ¹é…é¿å…è¯¯åˆ¤)
    "anthropic": "Anthropic",
    "claude": "Anthropic",
    "microsoft": "Microsoft",
    "copilot": "Microsoft",
    "meta": "Meta",
    "llama": "Meta",
    "apple": "Apple",
    "amazon": "Amazon",
    "alexa": "Amazon",
    "nvidia": "Nvidia",
    "tesla": "Tesla",
    "alibaba": "Alibaba",
    "qwen": "Alibaba",
    "tencent": "Tencent",
    "baidu": "Baidu",
    "ernie": "Baidu",
    "bytedance": "ByteDance",
    "doubao": "ByteDance",
}

# æ’é™¤å¤§å…¬å¸åŠå…¶äº§å“ (Focus: é»‘é©¬åˆ›ä¸šå…¬å¸)
EXCLUDE_BIG_COMPANY_PRODUCTS = True  # è®¾ä¸º False å¯æ”¶å½•å¤§å…¬å¸äº§å“

# çº¯å…¬å¸åæ’é™¤
EXCLUDE_TERMS = {
    "openai", "google", "microsoft", "meta", "apple", "amazon", 
    "nvidia", "anthropic", "alibaba", "tencent", "baidu", "bytedance",
}

# ============================================
# LLM å®¢æˆ·ç«¯
# ============================================

def get_llm_client():
    """è·å– LLM å®¢æˆ·ç«¯ (Perplexity)"""
    perplexity_key = os.getenv('PERPLEXITY_API_KEY')
    if not perplexity_key:
        return (None, None)
    try:
        from utils.perplexity_client import PerplexityClient
        client = PerplexityClient(api_key=perplexity_key)
        if client.is_available():
            return ("perplexity", client)
    except Exception as e:
        print(f"  âš ï¸ Perplexity åˆå§‹åŒ–å¤±è´¥: {e}")
    return (None, None)


# ============================================
# äº§å“æå– Prompt
# ============================================

EXTRACTION_PROMPT = """åˆ†æä»¥ä¸‹æ–°é—»æ–‡ç« ï¼Œæå–å…¶ä¸­æåˆ°çš„ AI äº§å“æˆ–åˆ›ä¸šå…¬å¸ä¿¡æ¯ã€‚

æ–‡ç« æ ‡é¢˜: {title}
æ–‡ç« æ¥æº: {source}
æ–‡ç« å†…å®¹: {content}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœæ–‡ç« ä¸­æ²¡æœ‰æåˆ°å…·ä½“äº§å“/å…¬å¸ï¼Œè¿”å›ç©º JSONï¼‰ï¼š

{{
  "has_product": true/false,  // æ˜¯å¦åŒ…å«å¯æ”¶å½•çš„äº§å“ä¿¡æ¯
  "products": [
    {{
      "name": "äº§å“/å…¬å¸åç§°",
      "website": "å®˜ç½‘ URL (å¦‚æœæ–‡ç« æåˆ°)",
      "description": "ä¸€å¥è¯äº§å“æè¿° (50å­—ä»¥å†…)",
      "category": "ç±»åˆ«: coding/image/video/voice/writing/agent/hardware/finance/education/healthcare/other",
      "is_hardware": false,  // æ˜¯å¦æ˜¯ç¡¬ä»¶äº§å“
      "hardware_category": "",  // å¦‚æœæ˜¯ç¡¬ä»¶: ai_chip/robotics/smart_glasses/wearables/drone/edge_ai
      "funding_total": "èèµ„é‡‘é¢ (å¦‚ $50M, $1.2B)",
      "funding_stage": "èèµ„é˜¶æ®µ (Seed/Series A/B/C)",
      "founded_date": "æˆç«‹å¹´ä»½",
      "region": "åœ°åŒº: ğŸ‡ºğŸ‡¸/ğŸ‡¨ğŸ‡³/ğŸ‡ªğŸ‡º/ğŸ‡¯ğŸ‡µ/ğŸ‡°ğŸ‡·/ğŸ‡¸ğŸ‡¬",
      "why_matters": "ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ (ä¸€å¥è¯ï¼Œè¦å…·ä½“ï¼ŒåŒ…å«æ•°æ®)",
      "dark_horse_score": 1-5,  // é»‘é©¬è¯„åˆ†
      "score_reason": "è¯„åˆ†ç†ç”±"
    }}
  ]
}}

ã€é‡è¦ã€‘æˆ‘ä»¬çš„æ ¸å¿ƒç›®æ ‡æ˜¯å‘ç°ã€Œé»‘é©¬ã€å’Œã€Œæ½œåŠ›æ–°äººã€ï¼š
- **ä¼˜å…ˆæå–åˆ›ä¸šå…¬å¸** - èèµ„æ–°é—»ã€æ–°äº§å“å‘å¸ƒã€å¿«é€Ÿå¢é•¿çš„å°å…¬å¸
- **å¤§å…¬å¸äº§å“æ¬¡è¦** - åªæœ‰éå¸¸åˆ›æ–°çš„æ–°äº§å“æ‰å€¼å¾—æ”¶å½•

è¯„åˆ†æ ‡å‡†:
- 5åˆ†: åˆ›ä¸šå…¬å¸èèµ„>$100M / å“ç±»å¼€åˆ›è€… / å¢é•¿å¼‚å¸¸å¿«
- 4åˆ†: åˆ›ä¸šå…¬å¸èèµ„>$30M / ARR>$10M / é¡¶çº§VCèƒŒä¹¦
- 3åˆ†: èèµ„$1M-$30M / ProductHuntä¸Šæ¦œ / æœ‰æ˜æ˜¾å¢é•¿
- 2åˆ†: åˆšå‘å¸ƒ/æ•°æ®ä¸è¶³ ä½†æœ‰åˆ›æ–°ç‚¹
- 1åˆ†: ä¿¡æ¯ä¸è¶³ / æ™®é€šäº§å“
- å¤§å…¬å¸æ–°äº§å“: éå¸¸åˆ›æ–°å¯ç»™4-5åˆ†ï¼Œæ™®é€šæ›´æ–°1-2åˆ†

æ³¨æ„:
1. åªæå–æ˜ç¡®çš„äº§å“/å…¬å¸ï¼Œä¸è¦çŒœæµ‹
2. çº¯å…¬å¸åä¸ç®—äº§å“ (å¦‚ "OpenAI" ä¸æ˜¯äº§å“ï¼Œä½† "ChatGPT Health" æ˜¯äº§å“)
3. why_matters å¿…é¡»å…·ä½“ï¼šä¼˜å…ˆä½¿ç”¨åŸæ–‡ä¸­å‡ºç°çš„æ•°å­—/äº‹å®ï¼›å¦‚æœåŸæ–‡æ²¡æœ‰æ•°å­—ï¼Œå†™æ¸…æ¥šå·®å¼‚åŒ–/ä¿¡å·ç‚¹ï¼Œä½†ä¸è¦ç¼–é€ æ•°å­—
4. å¦‚æœæ–‡ç« åªæ˜¯è¡Œä¸šåˆ†æ/è§‚ç‚¹ï¼Œhas_product è®¾ä¸º false
5. å¦‚æœæ¥æºæ˜¯ YouTube / Xï¼šå¿½ç•¥èµåŠ©å•†ã€å¹¿å‘Šã€æŠ˜æ‰£ç ç­‰æ¨å¹¿å†…å®¹ï¼Œåªæå–ä¸»å†…å®¹çœŸæ­£ä»‹ç»çš„äº§å“

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""


# ============================================
# LLM è°ƒç”¨
# ============================================

def extract_products_with_llm(article: Dict, llm_type: str, llm_client: Any) -> List[Dict]:
    """ä½¿ç”¨ LLM ä»æ–‡ç« ä¸­æå–äº§å“ä¿¡æ¯"""
    
    title = article.get('title', '')
    source = article.get('source', '')
    content = article.get('summary', '')

    if (source or "").lower().strip() in {"youtube", "x"}:
        content = clean_social_content(content)
        if len(content) < 60:
            return []
    
    prompt = EXTRACTION_PROMPT.format(
        title=title,
        source=source,
        content=content
    )
    
    try:
        if llm_type != "perplexity":
            return []

        response = llm_client.analyze(prompt=prompt)
        # analyze è¿”å›è§£æåçš„ JSON æˆ–å­—ç¬¦ä¸²
        if isinstance(response, dict):
            result_text = json.dumps(response)
        elif isinstance(response, list):
            result_text = json.dumps({"has_product": True, "products": response})
        else:
            result_text = str(response)
        
        # è§£æ JSON
        json_match = re.search(r'\{[\s\S]*\}', result_text)
        if json_match:
            result = json.loads(json_match.group())
            if result.get('has_product') and result.get('products'):
                return result['products']
        
        return []
    
    except Exception as e:
        print(f"    âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        return []


# ============================================
# äº§å“éªŒè¯å’Œæ ‡å‡†åŒ–
# ============================================

def clean_social_content(content: str) -> str:
    """Remove common sponsor/boilerplate lines from social snippets (YouTube/X)."""
    if not content:
        return ""

    blocked = re.compile(
        r"(sponsor|sponsors|check\\s+out|sign\\s+up|patreon|discount|promo\\s+code|affiliate|"
        r"subscribe|newsletter|merch|giveaway|support\\s+us|use\\s+code)",
        re.IGNORECASE,
    )

    text = str(content)

    # Common YouTube pattern: sponsor blurb then "ğŸ“ ..." with real context.
    marker_idx = text.find("ğŸ“")
    if marker_idx != -1:
        prefix = text[:marker_idx]
        if blocked.search(prefix):
            text = text[marker_idx:]

    lines = [ln.strip() for ln in text.splitlines() if ln and ln.strip()]
    kept = [ln for ln in lines if not blocked.search(ln)]
    if not kept:
        return ""
    cleaned = " ".join(kept)

    # Strip URLs to reduce chance of extracting sponsors/track links.
    cleaned = re.sub(r"https?://\\S+", "", cleaned)
    cleaned = re.sub(r"\\s+", " ", cleaned).strip()
    return cleaned

def search_website(name: str, category: str, llm_client: Any) -> str:
    """æœç´¢äº§å“å®˜ç½‘"""
    if not llm_client:
        return ""
    
    try:
        query = f"{name} {category} official website"
        results = llm_client.search(query=query, max_results=3)
        
        if results:
            # ä¼˜å…ˆé€‰æ‹©äº§å“å®˜ç½‘
            for r in results:
                url = r.url.lower()
                name_clean = name.lower().replace(' ', '').replace('-', '')
                if name_clean[:4] in url or any(domain in url for domain in ['.com', '.ai', '.io']):
                    return r.url
            return results[0].url
        return ""
    except Exception:
        return ""


def validate_product(
    product: Dict,
    article: Dict,
    llm_client: Any = None,
    *,
    featured_index: Optional[Dict[str, Dict[str, Any]]] = None,
    featured_name_index: Optional[Dict[str, Dict[str, Any]]] = None,
    enrich_featured: bool = True,
) -> Optional[Dict]:
    """éªŒè¯å’Œæ ‡å‡†åŒ–äº§å“æ•°æ®"""
    from utils.website_resolver import extract_official_website_from_source, is_placeholder_url
    
    name = product.get('name', '').strip()
    if not name or len(name) < 2:
        return None
    
    # æ’é™¤çº¯å…¬å¸å (ä¸æ˜¯å…·ä½“äº§å“)
    if name.lower() in EXCLUDE_TERMS:
        return None
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    score = product.get('dark_horse_score', 0)
    if score < 2:
        return None  # è¯„åˆ†å¤ªä½ï¼Œä¸æ”¶å½•
    
    why_matters = product.get('why_matters', '')
    if not why_matters or len(why_matters) < 10:
        return None  # æ²¡æœ‰è¯´æ˜ä¸ºä»€ä¹ˆé‡è¦

    description = (product.get("description", "") or "").strip()
    if len(description) < 20:
        return None  # æè¿°å¤ªçŸ­ï¼Œä¸åˆ©äºåˆ¤æ–­

    # why_matters è´¨é‡ï¼šå¿…é¡»â€œå…·ä½“â€ï¼ˆæ•°å­—æˆ–æ˜ç¡®å·®å¼‚åŒ–/èƒŒä¹¦/é‡Œç¨‹ç¢‘ï¼‰
    generic_phrases = [
        "å¾ˆæœ‰æ½œåŠ›", "å€¼å¾—å…³æ³¨", "èèµ„æƒ…å†µè‰¯å¥½", "å›¢é˜ŸèƒŒæ™¯ä¸é”™", "å‰æ™¯å¹¿é˜”",
        "promising", "worth watching", "interesting", "potential", "good product",
    ]
    why_lower = why_matters.lower()
    if any(p.lower() in why_lower for p in generic_phrases) and len(why_matters) < 60:
        return None

    has_number = bool(re.search(r"[\$Â¥â‚¬]\d+|arr|\d+[mbkä¸‡äº¿]|\d+%|\d{1,3}[,.]?\d{0,3}", why_lower))
    has_specific = any(kw.lower() in why_lower for kw in [
        "é¢†æŠ•", "èèµ„", "ä¼°å€¼", "ç”¨æˆ·", "å¢é•¿", "arr", "é¦–åˆ›", "é¦–ä¸ª",
        "å‰openai", "å‰google", "å‰meta", "yc", "a16z", "sequoia",
        "open source", "å¼€æº", "crowdfunding", "ä¼—ç­¹", "å·²å‘è´§", "é¢„å”®", "no subscription", "æ— è®¢é˜…",
    ])
    if not has_number and not has_specific:
        return None

    # è·å–ç½‘ç«™ (ä¼˜å…ˆä» source_url è§£æï¼Œé¿å…æ¨¡å‹çŒœå®˜ç½‘)
    website = (product.get('website', '') or '').strip()
    if website and is_placeholder_url(website):
        website = ""

    article_link = (article.get('link') or '').strip()
    if (not website or website.lower() == "unknown") and article_link:
        resolved = extract_official_website_from_source(article_link, name)
        if resolved:
            website = resolved
            product['website_source'] = "source_url"

    if not website and llm_client:
        website = search_website(name, product.get('category', ''), llm_client)

    if not website or website.lower() == "unknown":
        return None

    # Industry leaders should not enter candidates, but can still be enriched if they already exist in featured.
    domain_key = normalize_domain(website) if website and website.lower() != "unknown" else ""
    name_key = normalize_name_key(name)
    featured_hit = bool(
        enrich_featured and (
            (featured_index and domain_key and domain_key in featured_index)
            or (featured_name_index and name_key and name_key in featured_name_index)
        )
    )
    if (not featured_hit) and is_industry_leader(name, website):
        return None

    # æ£€æµ‹å¹¶æ’é™¤å¤§å…¬å¸äº§å“ (Focus: é»‘é©¬å’Œåˆ›ä¸šå…¬å¸)
    name_lower = name.lower()
    website_lower = website.lower() if website else ""
    
    # æ£€æŸ¥äº§å“åæ˜¯å¦åŒ…å«å¤§å…¬å¸å…³é”®è¯
    for keyword in BIG_COMPANY_KEYWORDS.keys():
        if keyword in name_lower:
            return None  # æ’é™¤å¤§å…¬å¸äº§å“
    
    # æ£€æŸ¥ç½‘ç«™æ˜¯å¦å±äºå¤§å…¬å¸
    big_company_domains = [
        "openai.com", "anthropic.com", "claude.ai", "claude.com",
        "google.com", "labs.google", "deepmind.google",
        "microsoft.com", "meta.com", "apple.com", "amazon.com",
        "nvidia.com", "alibaba.com", "tencent.com", "baidu.com", "bytedance.com"
    ]
    for domain in big_company_domains:
        if domain in website_lower:
            return None  # æ’é™¤å¤§å…¬å¸äº§å“
    
    is_big_company = False
    parent_company = ""

    dark_index = min(5, max(1, int(score)))
    category = product.get('category', 'other')
    categories = [category] if category else ['other']
    is_hardware = bool(product.get('is_hardware')) or category == 'hardware'
    if is_hardware and 'hardware' not in categories:
        categories.insert(0, 'hardware')

    published_at = (article.get('published_at') or '').strip()
    if not published_at:
        published_at = datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

    # æ ‡å‡†åŒ–æ•°æ®
    standardized = {
        "name": name,
        "slug": name.lower().replace(' ', '-').replace('.', '-'),
        "website": website,
        "logo_url": "",
        "description": description[:200],
        "category": category,
        "categories": categories,
        "is_hardware": is_hardware,
        "hardware_category": product.get('hardware_category', ''),
        "is_big_company": is_big_company,  # æ ‡è®°å¤§å…¬å¸äº§å“
        "parent_company": parent_company,  # æ¯å…¬å¸åç§°
        "funding_total": product.get('funding_total', ''),
        "funding_stage": product.get('funding_stage', ''),
        "founded_date": product.get('founded_date', ''),
        "region": product.get('region', 'ğŸ‡ºğŸ‡¸'),
        "dark_horse_index": dark_index,
        "why_matters": why_matters[:300],
        "criteria_met": [product.get('score_reason', '')],
        "discovered_at": datetime.now().strftime('%Y-%m-%d'),
        "source": article.get('source', ''),
        "source_url": article_link,
        "source_title": article.get('title', ''),
        "published_at": published_at,
        "first_seen": to_iso(datetime.now(timezone.utc)),
        "final_score": dark_index * 20,
        "trending_score": dark_index * 18,
        "hot_score": dark_index * 20,
        "top_score": dark_index * 20,
    }

    if product.get('website_source'):
        standardized['website_source'] = product['website_source']

    return standardized


def is_duplicate(product: Dict, existing_products: List[Dict]) -> bool:
    """æ£€æŸ¥äº§å“æ˜¯å¦é‡å¤"""
    name = product.get('name', '').lower().replace(' ', '')
    website = product.get('website', '').lower()
    
    for existing in existing_products:
        existing_name = existing.get('name', '').lower().replace(' ', '')
        existing_website = existing.get('website', '').lower()
        
        if name == existing_name:
            return True
        if website and existing_website and website in existing_website:
            return True
    
    return False


# ============================================
# ä¸»æµç¨‹
# ============================================

def load_existing_products() -> List[Dict]:
    """åŠ è½½å·²æœ‰äº§å“ (ç”¨äºå»é‡)"""
    featured = safe_load_json(PRODUCTS_FEATURED_FILE, []) or []
    pending = safe_load_json(PENDING_REVIEW_FILE, []) or []
    existing: List[Dict[str, Any]] = []
    if isinstance(featured, list):
        existing.extend(featured)
    if isinstance(pending, list):
        existing.extend(pending)
    return existing


def filter_articles(
    articles: List[Dict[str, Any]],
    sources: Optional[Set[str]] = None,
    source_contains: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """ç­›é€‰åŒ…å«äº§å“ä¿¡æ¯çš„æ–‡ç« ï¼ˆæ”¯æŒ sources ç²¾ç¡®è¿‡æ»¤ + å…³é”®è¯åˆç­›ï¼‰"""
    filtered: List[Dict[str, Any]] = []

    sources_norm = {s.lower().strip() for s in (sources or set()) if s and s.strip()}
    source_contains_norm = (source_contains or "").lower().strip()
    try:
        allowed_year = int(os.getenv("CONTENT_YEAR", str(datetime.now(timezone.utc).year)))
    except Exception:
        allowed_year = datetime.now(timezone.utc).year

    for article in articles:
        src = (article.get("source") or "").lower().strip()

        if sources_norm and src not in sources_norm:
            continue
        if (not sources_norm) and source_contains_norm and source_contains_norm not in src:
            continue

        # Keep only the allowed year (default: current year).
        published_at = (article.get("published_at") or "").strip()
        published_dt = parse_date(published_at)
        if not published_dt or published_dt.year != allowed_year:
            continue

        text = f"{article.get('title', '')} {article.get('summary', '')}".lower()
        has_keyword = any(kw.lower() in text for kw in PRODUCT_KEYWORDS)

        # Social signals (youtube/x) are already pre-filtered by spiders; keep them even if keyword misses.
        if has_keyword or article.get("has_product_mention") or src in {"youtube", "x"}:
            filtered.append(article)

    return filtered


def load_processed_cache(cache_file: str) -> Set[str]:
    data = safe_load_json(cache_file, [])
    if isinstance(data, list):
        return {str(x) for x in data if str(x)}
    return set()


def save_processed_cache(cache_file: str, keys: Set[str]) -> None:
    safe_save_json(cache_file, sorted(keys))


def build_featured_index(featured: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    index: Dict[str, Dict[str, Any]] = {}
    for p in featured:
        domain = normalize_domain(p.get("website", ""))
        if domain and domain not in index:
            index[domain] = p
    return index


def build_featured_name_index(featured: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    index: Dict[str, Dict[str, Any]] = {}
    for p in featured:
        key = normalize_name_key(p.get("name", ""))
        if key and key not in index:
            index[key] = p
    return index


def _bump_score_fields(product: Dict[str, Any], delta: int = 2, cap: int = 100) -> None:
    for field in ("trending_score", "hot_score", "final_score", "top_score"):
        try:
            value = int(product.get(field) or 0)
        except Exception:
            continue
        if value <= 0:
            continue
        product[field] = min(cap, value + delta)


def add_signal_to_product(product: Dict[str, Any], signal: Dict[str, Any], max_items: int = 5) -> bool:
    extra = product.get("extra") or {}
    if not isinstance(extra, dict):
        extra = {}

    signals = extra.get("signals") or []
    if not isinstance(signals, list):
        signals = []

    url = (signal.get("url") or "").strip()
    if url and any((s or {}).get("url") == url for s in signals if isinstance(s, dict)):
        return False

    signals.insert(0, signal)
    extra["signals"] = signals[:max_items]
    product["extra"] = extra
    return True


def enrich_featured_product(featured_product: Dict[str, Any], signal: Dict[str, Any], extracted: Dict[str, Any]) -> bool:
    changed = add_signal_to_product(featured_product, signal, max_items=5)
    if not changed:
        return False

    platform = (signal.get("platform") or "").lower()
    platform_label = "YouTube" if platform == "youtube" else ("X" if platform == "x" else platform.upper())

    # Update latest_news (no link)
    published_at = signal.get("published_at") or ""
    date_prefix = published_at[:10] if published_at else datetime.now().strftime("%Y-%m-%d")
    one_liner = (
        (extracted.get("why_matters") or "").strip() or
        (extracted.get("description") or "").strip() or
        (signal.get("title") or "").strip()
    )
    if len(one_liner) > 120:
        one_liner = one_liner[:117] + "..."

    featured_product["latest_news"] = f"{date_prefix}: æ¥è‡ª {platform_label} çš„ä¸€æ‰‹æåŠï¼š{one_liner}"

    # This drives dark horse freshness via backend product_sorting.get_effective_date()
    featured_product["news_updated_at"] = published_at or to_iso(datetime.now(timezone.utc))

    # Optional: small ranking bump (cap 100)
    _bump_score_fields(featured_product, delta=2, cap=100)
    return True


def merge_pending_candidates(existing: List[Dict[str, Any]], new_items: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], int]:
    by_key = {}
    for p in existing:
        key = normalize_name(p.get("name", "")) or normalize_domain(p.get("website", ""))
        if key and key not in by_key:
            by_key[key] = p

    added = 0
    for p in new_items:
        key = normalize_name(p.get("name", "")) or normalize_domain(p.get("website", ""))
        if not key or key in by_key:
            continue
        by_key[key] = p
        existing.append(p)
        added += 1

    existing.sort(key=lambda x: x.get("final_score", x.get("trending_score", 0)), reverse=True)
    return existing, added


def process_articles(
    articles: List[Dict[str, Any]],
    llm_type: str,
    llm_client: Any,
    existing_products: List[Dict[str, Any]],
    featured_index: Dict[str, Dict[str, Any]],
    featured_name_index: Optional[Dict[str, Dict[str, Any]]] = None,
    enrich_featured: bool = True,
    processed_cache: Optional[Set[str]] = None,
    dry_run: bool = False,
) -> Dict[str, Any]:
    """å¤„ç†æ–‡ç« ï¼Œæå–äº§å“ â†’ enrich featured æˆ–å†™å…¥å€™é€‰æ± """
    new_candidates: List[Dict[str, Any]] = []
    enriched_count = 0
    processed_count = 0
    skipped_cache = 0

    for i, article in enumerate(articles):
        key = article_key(article)
        if processed_cache is not None and key in processed_cache:
            skipped_cache += 1
            continue

        title = (article.get("title") or "")[:80]
        source = article.get("source", "")

        print(f"\n[{processed_count + 1}/{len(articles)}] {source}")
        print(f"  ğŸ“° {title}...")
        processed_count += 1

        products = extract_products_with_llm(article, llm_type, llm_client)
        if not products:
            print("  â­ï¸ æ— äº§å“ä¿¡æ¯")
            if processed_cache is not None:
                processed_cache.add(key)
            continue

        signal = build_signal(article)

        for product in products:
            validated = validate_product(
                product,
                article,
                llm_client,
                featured_index=featured_index,
                featured_name_index=featured_name_index,
                enrich_featured=enrich_featured,
            )
            if not validated:
                print(f"  â­ï¸ {product.get('name', '?')} - éªŒè¯æœªé€šè¿‡")
                continue

            domain = normalize_domain(validated.get("website", ""))
            target_featured = None
            if enrich_featured and domain and domain in featured_index:
                target_featured = featured_index[domain]
            elif enrich_featured and featured_name_index:
                name_key = normalize_name_key(validated.get("name", ""))
                if name_key and name_key in featured_name_index:
                    target_featured = featured_name_index[name_key]

            if target_featured:
                if dry_run:
                    matched = normalize_domain(target_featured.get("website", "")) or "name-match"
                    print(f"  ğŸ§ª [DRY RUN] Enrich featured: {validated.get('name')} ({matched})")
                    enriched_count += 1
                else:
                    if enrich_featured_product(target_featured, signal, validated):
                        enriched_count += 1
                        print(f"  ğŸ”— Enriched featured: {target_featured.get('name')} â† {signal.get('platform')}")
                continue

            if is_duplicate(validated, existing_products + new_candidates):
                print(f"  â­ï¸ {validated.get('name', '?')} - å·²å­˜åœ¨")
                continue

            # Otherwise: save to candidates/pending_review.json for human review
            candidate = dict(validated)
            platform = (signal.get("platform") or "").lower()
            platform_label = "YouTube" if platform == "youtube" else ("X" if platform == "x" else platform)
            candidate["_candidate_reason"] = f"æ¥è‡ª {platform_label} ä¿¡å·"

            # Attach signal evidence to candidate.extra.signals
            add_signal_to_product(candidate, signal, max_items=5)

            score = candidate.get("dark_horse_index", 0)
            why = (candidate.get("why_matters") or "")[:40]
            print(f"  âœ… Candidate: {candidate.get('name')} ({score}åˆ†) - {why}...")
            new_candidates.append(candidate)

        if processed_cache is not None:
            processed_cache.add(key)

        time.sleep(1)

    return {
        "new_candidates": new_candidates,
        "enriched_count": enriched_count,
        "processed_count": processed_count,
        "skipped_cache": skipped_cache,
    }


# ============================================
# CLI
# ============================================

def main():
    parser = argparse.ArgumentParser(description="RSS æ–°é—» â†’ äº§å“æ•°æ®è½¬æ¢")
    parser.add_argument("--limit", type=int, default=50, help="å¤„ç†æ–‡ç« æ•°é‡ä¸Šé™")
    parser.add_argument("--source", type=str, help="åªå¤„ç†æŒ‡å®šæ¥æºçš„æ–‡ç«  (deprecated; use --sources)")
    parser.add_argument("--sources", type=str, default="", help="åªå¤„ç†æŒ‡å®šæ¥æº (é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ youtube,x)")
    parser.add_argument("--dry-run", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼Œä¸ä¿å­˜")
    parser.add_argument("--input", type=str, help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--cache-file", type=str, default=DEFAULT_CACHE_FILE, help="ç¼“å­˜å·²å¤„ç†æ–‡ç«  key çš„æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--enrich-featured", action="store_true", default=True, help="åŒ¹é…åˆ° featured äº§å“åˆ™å†™å…¥ä¿¡å·/æ›´æ–° latest_news")
    parser.add_argument("--no-enrich-featured", action="store_false", dest="enrich_featured", help="ç¦ç”¨ featured enrich")
    
    args = parser.parse_args()
    
    print("ğŸ”„ RSS æ–°é—» â†’ äº§å“æ•°æ®è½¬æ¢")
    print("=" * 50)
    
    # è¯»å–æ–°é—»
    input_file = args.input or BLOGS_NEWS_FILE
    if not os.path.exists(input_file):
        print(f"âŒ æ–°é—»æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print("è¯·å…ˆè¿è¡Œ: python tools/rss_feeds.py")
        return
    
    with open(input_file, 'r') as f:
        articles = json.load(f)
    
    if not isinstance(articles, list):
        print(f"âŒ è¾“å…¥æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ˆæœŸæœ› JSON æ•°ç»„ï¼‰: {input_file}")
        return

    normalized_articles = [normalize_article(a) for a in articles if isinstance(a, dict)]
    print(f"ğŸ“° è¯»å– {len(normalized_articles)} ç¯‡æ–°é—»")
    
    # ç­›é€‰æ–‡ç« 
    sources = {s.strip().lower() for s in (args.sources or "").split(",") if s.strip()}
    filtered = filter_articles(normalized_articles, sources=sources or None, source_contains=args.source)
    print(f"ğŸ” ç­›é€‰å‡º {len(filtered)} ç¯‡åŒ…å«äº§å“ä¿¡æ¯çš„æ–‡ç« ")
    
    if not filtered:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ…å«äº§å“ä¿¡æ¯çš„æ–‡ç« ")
        return

    processed_cache = load_processed_cache(args.cache_file) if args.cache_file else set()
    if processed_cache:
        before = len(filtered)
        filtered = [a for a in filtered if article_key(a) not in processed_cache]
        skipped = before - len(filtered)
        if skipped:
            print(f"â­ï¸ è·³è¿‡å·²å¤„ç†: {skipped} ç¯‡")
    
    # é™åˆ¶æ•°é‡
    filtered = filtered[:args.limit]
    
    # è·å– LLM å®¢æˆ·ç«¯
    print("\nğŸ¤– åˆå§‹åŒ– LLM...")
    llm_type, llm_client = get_llm_client()
    
    if not llm_client:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ LLM å®¢æˆ·ç«¯")
        print("è¯·é…ç½® PERPLEXITY_API_KEY")
        return
    
    print(f"  âœ… ä½¿ç”¨ {llm_type}")
    
    # åŠ è½½å·²æœ‰äº§å“
    featured_products = safe_load_json(PRODUCTS_FEATURED_FILE, []) or []
    if not isinstance(featured_products, list):
        featured_products = []
    featured_index = build_featured_index(featured_products)
    featured_name_index = build_featured_name_index(featured_products)

    pending_candidates = safe_load_json(PENDING_REVIEW_FILE, []) or []
    if not isinstance(pending_candidates, list):
        pending_candidates = []

    existing = featured_products + pending_candidates
    print(f"ğŸ“¦ Featured: {len(featured_products)} | Pending: {len(pending_candidates)}")
    
    # å¤„ç†æ–‡ç« 
    print(f"\nğŸ”„ å¼€å§‹å¤„ç† {len(filtered)} ç¯‡æ–‡ç« ...")
    result = process_articles(
        filtered,
        llm_type,
        llm_client,
        existing,
        featured_index=featured_index,
        featured_name_index=featured_name_index,
        enrich_featured=args.enrich_featured,
        processed_cache=processed_cache,
        dry_run=args.dry_run,
    )

    new_candidates = result.get("new_candidates") or []
    enriched_count = int(result.get("enriched_count") or 0)
    processed_count = int(result.get("processed_count") or 0)

    if not args.dry_run:
        if args.enrich_featured and enriched_count > 0:
            safe_save_json(PRODUCTS_FEATURED_FILE, featured_products)
            print(f"\nâœ… å·²æ›´æ–° featured: +{enriched_count} æ¡ä¿¡å· (latest_news/news_updated_at)")

        if new_candidates:
            pending_candidates, added = merge_pending_candidates(pending_candidates, new_candidates)
            if added:
                safe_save_json(PENDING_REVIEW_FILE, pending_candidates)
            print(f"\nâœ… å·²å†™å…¥å€™é€‰æ± : +{added} ä¸ª â†’ candidates/pending_review.json")

        if args.cache_file:
            save_processed_cache(args.cache_file, processed_cache)
    
    # ç»Ÿè®¡
    print("\nğŸ“Š ç»Ÿè®¡:")
    print(f"  - å¤„ç†æ–‡ç« : {processed_count}")
    print(f"  - enrich featured: {enriched_count}")
    print(f"  - æ–°å€™é€‰: {len(new_candidates)}")

    if new_candidates:
        scores = [p.get('dark_horse_index', 0) for p in new_candidates]
        print(f"  - 5åˆ†äº§å“: {scores.count(5)}")
        print(f"  - 4åˆ†äº§å“: {scores.count(4)}")
        print(f"  - 3åˆ†äº§å“: {scores.count(3)}")
        print(f"  - 2åˆ†äº§å“: {scores.count(2)}")


if __name__ == "__main__":
    main()
