#!/usr/bin/env python3
"""
Perplexity API Client v2.0

é‡‡ç”¨ Search API + Sonar ä¸¤æ­¥èµ°æ–¹æ¡ˆï¼š
1. Search API: è·å–ç»“æ„åŒ–æœç´¢ç»“æœ (title, url, snippet, date)
2. Sonar Model: åˆ†ææœç´¢ç»“æœï¼Œæå–äº§å“ä¿¡æ¯

ä¼˜ç‚¹ï¼š
- æœç´¢ç»“æœå¯æ§ï¼Œå¯ä»¥è¿‡æ»¤ä½è´¨é‡æ¥æº
- å®˜ç½‘æå–æ›´å‡†ç¡®
- æˆæœ¬å¯æ§ï¼ˆSearch API æ¯” Sonar ä¾¿å®œï¼‰

Usage:
    from utils.perplexity_client import PerplexityClient
    
    client = PerplexityClient(api_key="your-key")
    
    # æ–¹å¼1: çº¯æœç´¢
    results = client.search("AI startup funding 2026")
    
    # æ–¹å¼2: æœç´¢ + åˆ†æ
    products = client.search_and_extract(
        query="AI startup funding 2026",
        analysis_prompt="Extract AI products...",
        region="us"
    )
"""

import os
import json
import time
import re
from typing import Optional, Union
from dataclasses import dataclass, field
import requests

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å…¨å±€é…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PERPLEXITY_API_KEY = os.environ.get('PERPLEXITY_API_KEY', '')
PERPLEXITY_MODEL = os.environ.get('PERPLEXITY_MODEL', 'sonar')  # sonar / sonar-pro
API_RATE_LIMIT_DELAY = float(os.environ.get('API_RATE_LIMIT_DELAY', '2'))

# API ç«¯ç‚¹
SEARCH_API_URL = "https://api.perplexity.ai/search"
CHAT_API_URL = "https://api.perplexity.ai/chat/completions"

# åœ°åŒºé…ç½®
REGION_CONFIG = {
    "us": {"country": "US", "languages": ["en"], "recency": "week"},
    "cn": {"country": "CN", "languages": ["zh"], "recency": "week"},
    "eu": {"country": "GB", "languages": ["en", "de", "fr"], "recency": "week"},
    "jp": {"country": "JP", "languages": ["ja", "en"], "recency": "week"},
    "kr": {"country": "KR", "languages": ["ko", "en"], "recency": "week"},
    "sea": {"country": "SG", "languages": ["en"], "recency": "week"},
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•°æ®ç±»
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    title: str
    url: str
    snippet: str
    date: Optional[str] = None
    last_updated: Optional[str] = None
    
    def to_dict(self) -> dict:
        return {
            "title": self.title,
            "url": self.url,
            "content": self.snippet,
            "date": self.date,
            "last_updated": self.last_updated
        }
    
    def format_for_prompt(self) -> str:
        """æ ¼å¼åŒ–ä¸º Prompt æ–‡æœ¬"""
        lines = [f"### {self.title}"]
        lines.append(f"Source URL: {self.url}")
        if self.date:
            lines.append(f"Date: {self.date}")
        lines.append(f"Content: {self.snippet}")
        return "\n".join(lines)


@dataclass
class ExtractionResult:
    """æå–ç»“æœ"""
    products: list = field(default_factory=list)
    sources: list = field(default_factory=list)
    raw_response: str = ""
    search_count: int = 0
    extract_count: int = 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Perplexity å®¢æˆ·ç«¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PerplexityClient:
    """
    Perplexity API å®¢æˆ·ç«¯
    
    æ”¯æŒï¼š
    - Search API: å®æ—¶ Web æœç´¢
    - Chat Completions API: Sonar æ¨¡å‹åˆ†æ
    - ç»„åˆæ–¹æ³•: æœç´¢ + åˆ†æä¸€ä½“åŒ–
    """
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or PERPLEXITY_API_KEY
        self.model = PERPLEXITY_MODEL
        self._session = requests.Session()
        
        if self.api_key:
            self._session.headers.update({
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            })
        else:
            print("âš ï¸ PERPLEXITY_API_KEY not set")
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨"""
        return bool(self.api_key)
    
    def _rate_limit(self):
        """API é™æµ"""
        time.sleep(API_RATE_LIMIT_DELAY)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Search API (ç¬¬ä¸€æ­¥ï¼šè·å–æœç´¢ç»“æœ)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def search(
        self,
        query: str,
        max_results: int = 10,
        country: Optional[str] = None,
        language_filter: Optional[list] = None,
        domain_filter: Optional[list] = None,
        recency_filter: Optional[str] = None,
        max_tokens_per_page: int = 2048
    ) -> list[SearchResult]:
        """
        ä½¿ç”¨ Search API è¿›è¡Œ Web æœç´¢
        
        API ç«¯ç‚¹: POST https://api.perplexity.ai/search
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: ç»“æœæ•°é‡ (1-20)
            country: å›½å®¶ä»£ç  (US/CN/GB/JP ç­‰)
            language_filter: è¯­è¨€è¿‡æ»¤ (["en", "zh"] ç­‰ï¼Œæœ€å¤š10ä¸ª)
            domain_filter: åŸŸåè¿‡æ»¤ (["techcrunch.com", "-reddit.com"] ç­‰ï¼Œæœ€å¤š20ä¸ª)
            recency_filter: æ—¶æ•ˆæ€§ ("day"/"week"/"month"/"year")
            max_tokens_per_page: æ¯é¡µæœ€å¤§ token æ•°
            
        Returns:
            SearchResult åˆ—è¡¨
        """
        if not self.api_key:
            return []
        
        print(f"  ğŸ” Perplexity Search: {query[:50]}...")
        
        payload = {
            "query": query,
            "max_results": min(max_results, 20),
            "max_tokens_per_page": max_tokens_per_page,
            "max_tokens": 25000
        }
        
        if country:
            payload["country"] = country
        if language_filter:
            payload["search_language_filter"] = language_filter[:10]
        if domain_filter:
            payload["search_domain_filter"] = domain_filter[:20]
        if recency_filter and recency_filter in ["day", "week", "month", "year"]:
            payload["search_recency_filter"] = recency_filter
        
        try:
            response = self._session.post(SEARCH_API_URL, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for item in data.get("results", []):
                results.append(SearchResult(
                    title=item.get("title", ""),
                    url=item.get("url", ""),
                    snippet=item.get("snippet", ""),
                    date=item.get("date"),
                    last_updated=item.get("last_updated")
                ))
            
            print(f"  âœ… Found {len(results)} results")
            return results
            
        except requests.exceptions.RequestException as e:
            print(f"  âŒ Search Error: {e}")
            return []
        finally:
            self._rate_limit()
    
    def search_by_region(
        self,
        query: str,
        region: str,
        max_results: int = 10,
        **kwargs
    ) -> list[SearchResult]:
        """
        æŒ‰åœ°åŒºæœç´¢ï¼ˆè‡ªåŠ¨è®¾ç½®å›½å®¶/è¯­è¨€/æ—¶æ•ˆæ€§ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            region: åœ°åŒºä»£ç  (us/cn/eu/jp/kr/sea)
            max_results: ç»“æœæ•°é‡
        """
        config = REGION_CONFIG.get(region, REGION_CONFIG["us"])
        return self.search(
            query,
            max_results=max_results,
            country=config.get("country"),
            language_filter=config.get("languages"),
            recency_filter=config.get("recency", "week"),
            **kwargs
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Chat Completions API (ç¬¬äºŒæ­¥ï¼šåˆ†æå†…å®¹)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def analyze(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 0.3,
        max_tokens: int = 4096
    ) -> Union[dict, list, str]:
        """
        ä½¿ç”¨ Sonar æ¨¡å‹åˆ†æå†…å®¹
        
        API ç«¯ç‚¹: POST https://api.perplexity.ai/chat/completions
        
        Args:
            prompt: å®Œæ•´ prompt (åŒ…å«æœç´¢ç»“æœå’ŒæŒ‡ä»¤)
            model: æ¨¡å‹ (sonar/sonar-pro)
            temperature: æ¸©åº¦ (0-2ï¼Œæ¨è 0.3 ä»¥è·å¾—ç¨³å®šè¾“å‡º)
            max_tokens: æœ€å¤§ token
            
        Returns:
            è§£æåçš„ JSON æˆ–åŸå§‹æ–‡æœ¬
        """
        if not self.api_key:
            return {}
        
        model = model or self.model
        
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        
        try:
            response = self._session.post(CHAT_API_URL, json=payload, timeout=60)
            response.raise_for_status()
            data = response.json()
            
            result_text = data['choices'][0]['message']['content']
            return self._extract_json(result_text)
            
        except requests.exceptions.RequestException as e:
            print(f"  âŒ Analyze Error: {e}")
            return {}
        finally:
            self._rate_limit()
    
    def _extract_json(self, text: str) -> Union[dict, list, str]:
        """ä»æ–‡æœ¬ä¸­æå– JSON.

        Returns parsed JSON (list or dict) on success.
        Returns [] on parse failure so callers never receive raw text
        that masquerades as valid data.
        """
        if not text:
            return []

        # å°è¯• ```json ... ``` å—
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', text)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass

        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # å°è¯•æ‰¾åˆ° JSON æ•°ç»„
        array_match = re.search(r'\[\s*\{[\s\S]*\}\s*\]', text)
        if array_match:
            try:
                return json.loads(array_match.group())
            except json.JSONDecodeError:
                pass

        # All parsing attempts failed â€” log and return empty list
        snippet = text[:200].replace('\n', ' ')
        print(f"  âš  _extract_json: could not parse response (first 200 chars): {snippet}")
        return []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç»„åˆæ–¹æ³•ï¼šæœç´¢ + åˆ†æ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def search_and_extract(
        self,
        query: str,
        analysis_prompt: str,
        region: str = "us",
        max_results: int = 10,
        **search_kwargs
    ) -> ExtractionResult:
        """
        æœç´¢å¹¶æå–äº§å“ä¿¡æ¯ï¼ˆæ¨èæ–¹æ³•ï¼‰
        
        å·¥ä½œæµç¨‹:
        1. Search API è·å–æœç´¢ç»“æœ
        2. æ ¼å¼åŒ–æœç´¢ç»“æœ
        3. Sonar åˆ†æå¹¶æå–äº§å“
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            analysis_prompt: åˆ†æ Promptï¼Œéœ€åŒ…å« {search_results} å ä½ç¬¦
            region: åœ°åŒºä»£ç 
            max_results: æœç´¢ç»“æœæ•°é‡
            
        Returns:
            ExtractionResult åŒ…å«äº§å“åˆ—è¡¨å’Œæ¥æº
        """
        result = ExtractionResult()
        
        # Step 1: æœç´¢
        search_results = self.search_by_region(query, region, max_results, **search_kwargs)
        result.search_count = len(search_results)
        result.sources = [r.url for r in search_results]
        
        if not search_results:
            return result
        
        # Step 2: æ ¼å¼åŒ–æœç´¢ç»“æœ
        formatted_results = "\n\n".join([r.format_for_prompt() for r in search_results])
        
        # Step 3: æ„å»º prompt å¹¶åˆ†æ
        full_prompt = analysis_prompt.replace("{search_results}", formatted_results)
        
        print(f"    ğŸ“Š Analyzing with Sonar...")
        analysis = self.analyze(full_prompt)
        
        if isinstance(analysis, list):
            result.products = analysis
            result.extract_count = len(analysis)
        elif isinstance(analysis, dict):
            result.products = analysis.get("products", [analysis])
            result.extract_count = len(result.products)
        else:
            result.raw_response = str(analysis)
        
        print(f"    âœ… Extracted {result.extract_count} products")
        return result
    
    def format_results_for_prompt(self, results: list[SearchResult]) -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœä¸º Prompt æ–‡æœ¬"""
        return "\n\n".join([r.format_for_prompt() for r in results])
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¾¿æ·æ–¹æ³•
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_status(self) -> dict:
        """è·å–å®¢æˆ·ç«¯çŠ¶æ€"""
        return {
            "available": self.is_available(),
            "api_key_set": bool(self.api_key),
            "model": self.model,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾¿æ·å‡½æ•°ï¼ˆå…¼å®¹æ—§ APIï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_default_client: Optional[PerplexityClient] = None


def get_client() -> PerplexityClient:
    """è·å–é»˜è®¤å®¢æˆ·ç«¯ï¼ˆå•ä¾‹ï¼‰"""
    global _default_client
    if _default_client is None:
        _default_client = PerplexityClient()
    return _default_client


def perplexity_search(query: str, max_results: int = 10, region: str = None, **kwargs) -> list[dict]:
    """
    å¿«é€Ÿæœç´¢
    
    Returns:
        [{"title": "", "url": "", "content": ""}, ...]
    """
    client = get_client()
    if region:
        results = client.search_by_region(query, region, max_results, **kwargs)
    else:
        results = client.search(query, max_results=max_results, **kwargs)
    return [r.to_dict() for r in results]


def perplexity_analyze(prompt: str, **kwargs) -> Union[dict, list]:
    """å¿«é€Ÿåˆ†æ"""
    client = get_client()
    return client.analyze(prompt, **kwargs)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æµ‹è¯•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_client():
    """æµ‹è¯•å®¢æˆ·ç«¯"""
    print("\n" + "="*60)
    print("  ğŸ§ª Testing Perplexity Client")
    print("="*60)
    
    client = PerplexityClient()
    print(f"\n  Status: {client.get_status()}")
    
    if not client.is_available():
        print("\n  âŒ API key not set. Set PERPLEXITY_API_KEY env var.")
        return False
    
    # Test 1: åŸºç¡€æœç´¢
    print("\n  ğŸ“ Test 1: Basic Search")
    results = client.search("AI startup funding January 2026", max_results=3)
    if results:
        for i, r in enumerate(results, 1):
            print(f"    {i}. {r.title[:50]}...")
            print(f"       URL: {r.url}")
            print(f"       Date: {r.date}")
    else:
        print("    âŒ No results")
        return False
    
    # Test 2: åœ°åŒºæœç´¢
    print("\n  ğŸ“ Test 2: Regional Search (US)")
    results = client.search_by_region("AI startup", "us", max_results=3)
    if results:
        for i, r in enumerate(results, 1):
            print(f"    {i}. {r.title[:50]}...")
    else:
        print("    âŒ No results")
    
    # Test 3: åˆ†æ
    print("\n  ğŸ“ Test 3: Sonar Analysis")
    test_prompt = """Based on this search result, extract the company name and website:

### Linker Vision Raises US$35 Million Series-A
Source URL: https://www.prnewswire.com/news/linker-vision
Content: SANTA CLARA, Calif., Jan. 5, 2026 -- Linker Vision, a leading AI software platform company, announced Series A funding.

Return JSON format: {"name": "...", "website": "..."}"""
    
    analysis = client.analyze(test_prompt, temperature=0.1)
    print(f"    Result: {analysis}")
    
    print("\n  âœ… All tests passed!")
    return True


if __name__ == "__main__":
    test_client()
